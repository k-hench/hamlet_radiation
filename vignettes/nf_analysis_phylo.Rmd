---
output: html_document
editor_options:
  chunk_output_type: console
css: highlight.css
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(source = function(x, options) {
  if (!is.null(options$hilang)) {
      code_open <- "\n\n<div class=\"sourceCode\">\n<pre class=\"sourceCode\">\n<code class=\"sourceCode\">"
      code_close <- "\n</code>\n</pre>\n</div>\n"
      code_body <- highr::hi_andre(x, language = options$hilang, format = "html")
    stringr::str_c(
      code_open,
      knitr:::indent_block(paste(code_body, collapse = '\n'), ""),
      code_close
    )
  } else {
    stringr::str_c("\n\n```", tolower(options$engine), "\n",
                   paste(x, collapse = '\n'), "\n```\n\n")

  }
})
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = '../')
```

```{r, include=FALSE}
source('R/draw_workflow.R')
prod_phylo <- tibbler(c('twisst_output','fasttree_mito_output', 'fasttree_whg_output','fasttree_output'))
```

# Analysis III (phylogeny & topology weighting)

## Summary

The whole genome phylogeny and the topology weighting are prepared within the [**nextflow**](https://www.nextflow.io/) script `analysis_fasttree_twisst.nf` (located under `$BASE_DIR/nf/analysis_fasttree_twisst/`), which runs on the _SNPs only_ data set.
Below is an overview of the steps involved in the process.
(The <span style="color:#4DAF4A">green dot</span> indicates the genotype input, <span style="color:#E41A1C">red arrows</span> depict output that is exported for further use.)

<div style="max-width:800px; margin:auto;">
```{r, echo = FALSE, warning = FALSE, message = FALSE}
girafe( ggobj = dot_plot(file = 'docs/analysis_phylo.dot',
                         git = 5, point_types = prod_phylo),
        width_svg = 14, height_svg = 14)

```
</div>

## Details of `analysis_fasttree_twisst.nf`

### Setup

The nextflow script starts by opening the genotype data and feeding it into two different streams (one for the phylogeny and one for topology weighting).

<div class="kclass">
```{r , eval = FALSE, hilang = 'nf'}
#!/usr/bin/env nextflow
// git 5.1
// open genotype data
Channel
	.fromFilePairs("../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}")
	.into{ vcf_fasttree_whg; vcf_locations }
```

### Whole genome phylogeny

During data exploration, various subsets of the data set were investigated, including the different sampling locations, _whole genome_ vs. _non-outlier regions_ and all samples vs. _hamlets only_.
(Now, most of the oftions have been muted by commenting most options out.)

Here, we set up a channel managing the subset by location.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.2
// setting the sampling location
// (the script is set up to run on diffferent subsets of samples)
Channel
	.from( "all" ) //, "bel", "hon", "pan" )
	.set{ locations4_ch }
```

This channel toggles the inclusion of <i>F<sub>ST</sub></i> outlier regions.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.3
// setting loci restrictions
// (keep vs remove outlier regions)
Channel
	.from( "whg" ) //, "no_musks" )
	.set{ whg_modes }
```

This channel toggles the inclusion of the non-hamlet outgroup.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.4
// setting the sampling mode
// (the script is set up to run on diffferent subsets of samples)
Channel
	.from( "no_outgroups" ) //, "all" )
	.into{ sample_modes }
```

We combine the different selector channels to create all possible combinations of the settings.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.5
// compile the config settings and add data file
locations4_ch
	.combine( vcf_fasttree_whg )
	.combine( whg_modes )
	.combine( sample_modes )
	.set{ vcf_fasttree_whg_location_combo }
```

To prepare the input for the phylogeny, the fine tuned selection is applied to subset the genotypes.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.6
// apply sample filter, subset and convert genotypes
process subset_vcf_by_location_whg {
	label "L_28g5h_subset_vcf_whg"

	input:
	set val( loc ), vcfId, file( vcf ), val( mode ), val( sample_mode ) from vcf_fasttree_whg_location_combo

	output:
	set val( mode ), val( loc ), val( sample_mode ), file( "${loc}.${mode}.${sample_mode}.whg.geno.gz" ) into snp_geno_tree_whg

	script:
	"""
	DROP_CHRS=" "

	# check if samples need to be dropped based on location
	if [ "${loc}" == "all" ];then
		vcfsamplenames ${vcf[0]} > prep.pop
	else
		vcfsamplenames ${vcf[0]} | \
			grep ${loc}  > prep.pop
	fi

	# check if outgroups need to be dropped
	if [ "${sample_mode}" == "all" ];then
		mv prep.pop ${loc}.pop
	else
		cat prep.pop | \
			grep -v tor | \
			grep -v tab > ${loc}.pop
	fi

	# check if diverged LGs need to be dropped
	if [ "${mode}" == "no_musks" ];then
		DROP_CHRS="--not-chr LG04 --not-chr LG07 --not-chr LG08 --not-chr LG09 --not-chr LG12 --not-chr LG17 --not-chr LG23"
	fi

	vcftools --gzvcf ${vcf[0]} \
		--keep ${loc}.pop \
		\$DROP_CHRS \
		--mac 3 \
		--recode \
		--stdout | gzip > ${loc}.${mode}.${sample_mode}.vcf.gz

	python \$SFTWR/genomics_general/VCF_processing/parseVCF.py \
		-i  ${loc}.${mode}.${sample_mode}.vcf.gz | gzip > ${loc}.${mode}.${sample_mode}.whg.geno.gz
	"""
}
```

The subset is converted to `fasta` format, creating two whole genome pseudo haplotypes per sample.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.7
// convert genotypes to fasta
process fasttree_whg_prep {
	label 'L_190g4h_fasttree_whg_prep'
	tag "${mode} - ${loc} - ${sample_mode}"

	input:
	set val( mode ), val( loc ), val( sample_mode ), file( geno ) from snp_geno_tree_whg

	output:
	set val( mode ), val( loc ), val( sample_mode ), file( "all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa" ) into ( fasttree_whg_prep_ch )

	script:
	"""
	python \$SFTWR/genomics_general/genoToSeq.py -g ${geno} \
		-s  all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa \
		-f fasta \
		--splitPhased
	"""
}
```

Then, the phylogeny is reconstucted based on the converted geneotypes.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.8
// create phylogeny
process fasttree_whg_run {
	label 'L_300g30h_fasttree_run'
	tag "${mode} - ${loc} - ${sample_mode}"
	publishDir "../../2_analysis/fasttree/", mode: 'copy'

	input:
	set val( mode ), val( loc ), val( sample_mode ), file( fa ) from fasttree_whg_prep_ch

	output:
	file( "${sample_mode}.${loc}.${mode}.SNP.tree" ) into ( fasttree_whg_output )

	script:
	"""
	fasttree -nt ${fa} > ${sample_mode}.${loc}.${mode}.SNP.tree
	"""
}
```

### Topology weighting

The complexety of topology weighting increases non-linearely with the number of included populations as the number of possible unrooted topologies skyrockets.
The maximum number of possible populations allowed within `twisst` is eight, so we are running the topology weighting for Belize and Honduras independently (for the three species at Panama only one unrooted topology is possible, so we don't run `twisst` here).

We start by setting up a channel for the sampling location.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.9
// initialize the locations for topology weighting
Channel
	.from( "bel", "hon" )
	.set{ locations_ch }
```

Then, we attach the genotypes to the location.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.10
locations_ch
	.combine( vcf_locations )
	.set{ vcf_location_combo }
```

The anlysis is split by linkage group, so we need to initialize the LGs.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.11
// initialize LGs
Channel
	.from( ('01'..'09') + ('10'..'19') + ('20'..'24') )
	.map{ "LG" + it }
	.set{ lg_twisst }
```

The data is subset to include only the samples of the respective location.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.12
// subset the genotypes by location
process subset_vcf_by_location {
	label "L_20g2h_subset_vcf"

	input:
	set val( loc ), vcfId, file( vcf ) from vcf_location_combo

	output:
	set val( loc ), file( "${loc}.vcf.gz" ), file( "${loc}.pop" ) into ( vcf_loc_twisst )

	script:
	"""
	vcfsamplenames ${vcf[0]} | \
		grep ${loc} | \
		grep -v tor | \
		grep -v tab > ${loc}.pop

	vcftools --gzvcf ${vcf[0]} \
		--keep ${loc}.pop \
		--mac 3 \
		--recode \
		--stdout | gzip > ${loc}.vcf.gz
	"""
}
```

> While runing `twisst`, we ran into an issues regarding incompatibilities of the used scheduling system used on our computing cluster and the threading within the python scripts used in the preparation of `twisst`.
Unfortunately, this prevented us running the preparation _in place_ (as part of the `nextflow` script).
Instead, we rand the preparation separately on a local computer and clumsitly plugged the results into the `nextflow` process.

> Below we include the originally intended workflow which we muted so that the script is runnable using the plugg in approach.
Still, the the original workflow describes the steps excuted locally and convays the intermediate steps more clearly.
Also, on a different computer cluster, the original script should work allright.

The next step is to attach the LGs to the genotype subsets.


```{r , eval = FALSE, hilang = 'nf'}
// ---------------------------------------------------------------
// Unfortunately the twisst preparation did not work on the cluster
// ('in place'), so I had to setup the files locally and then plug
// them into this workflow.
// Below is the originally intended clean workflow (commented out),
// while the plugin vesion picks up at git 5.19.
// ---------------------------------------------------------------

/* MUTE:
// git 5.13
// add the lg channel to the genotype subset
vcf_loc_twisst
	.combine( lg_twisst )
	.set{ vcf_loc_lg_twisst }
*/
```

Based on the LGs the genotypes are split.

```{r , eval = FALSE, hilang = 'nf'}
/* MUTE: python thread conflict - run locally and feed into ressources/plugin
// git 5.14
// subset genotypes by LG
process vcf2geno_loc {
	label 'L_20g15h_vcf2geno'

	input:
	set val( loc ), file( vcf ), file( pop ), val( lg ) from vcf_loc_lg_twisst

	output:
	set val( loc ), val( lg ), file( "${loc}.${lg}.geno.gz" ), file( pop ) into snp_geno_twisst

	script:
	"""
	vcftools \
		--gzvcf ${vcf} \
		--chr ${lg} \
		--recode \
		--stdout |
		gzip > intermediate.vcf.gz

	python \$SFTWR/genomics_general/VCF_processing/parseVCF.py \
	  -i intermediate.vcf.gz | gzip > ${loc}.${lg}.geno.gz
	"""
}
*/
```

We initialize the window size size (as SNPs) used for the topology weighting...

```{r , eval = FALSE, hilang = 'nf'}
/* MUTE: python thread conflict - run locally and feed into ressources/plugin
// git 5.15
// initialize SNP window size
Channel.from( 50, 200 ).set{ twisst_window_types }
*/

```

...and attach them to the genotypes.

```{r , eval = FALSE, hilang = 'nf'}
/* MUTE: python thread conflict - run locally and feed into ressources/plugin
// git 5.16
// add the SNP window size to the genotype subset
snp_geno_twisst.combine( twisst_window_types ).set{ twisst_input_ch }
*/
```

To conduct topology weighting, we need some undelying phylogenies, so we run `PhyML` along the sliding window.

```{r , eval = FALSE, hilang = 'nf'}
/* MUTE: python thread conflict - run locally and feed into ressources/plugin
// git 5.17
// create the phylogenies along the sliding window
process twisst_prep {
  label 'L_G120g40h_prep_twisst'
  	publishDir "../../2_analysis/twisst/positions/${loc}/", mode: 'copy'

  input:
  set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ) from twisst_input_ch.filter { it[0] != 'pan' }

	output:
	set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( "*.trees.gz" ), file( "*.data.tsv" ) into twisst_prep_ch

  script:
   """
	module load intel17.0.4 intelmpi17.0.4

   mpirun \$NQSII_MPIOPTS -np 1 \
	python \$SFTWR/genomics_general/phylo/phyml_sliding_windows.py \
      -g ${geno} \
      --windType sites \
      -w ${twisst_w} \
      --prefix ${loc}.${lg}.w${twisst_w}.phyml_bionj \
      --model HKY85 \
      --optimise n \
		--threads 1
	 """
}
*/
```

The last step then is to run `twisst` prepared phylogenies.

```{r , eval = FALSE, hilang = 'nf'}
/* MUTE: python thread conflict - run locally and feed into ressources/plugin
// git 5.18
// run the topology weighting on the  phylogenies
process twisst_run {
	label 'L_G120g40h_run_twisst'
	publishDir "../../2_analysis/twisst/weights/", mode: 'copy'

	input:
	set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( tree ), file( data ) from twisst_prep_ch

	output:
	set val( loc ), val( lg ), val( twisst_w ), file( "*.weights.tsv.gz" ), file( "*.data.tsv" ) into ( twisst_output )

	script:
	"""
	module load intel17.0.4 intelmpi17.0.4

	awk '{print \$1"\\t"\$1}' ${pop} | \
	sed 's/\\(...\\)\\(...\\)\$/\\t\\1\\t\\2/g' | \
	cut -f 1,3 | \
	awk '{print \$1"_A\\t"\$2"\\n"\$1"_B\\t"\$2}' > ${loc}.${lg}.twisst_pop.txt

	TWISST_POPS=\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d',' | sed 's/,/ -g /g; s/^/-g /' )

	mpirun \$NQSII_MPIOPTS -np 1 \
	python \$SFTWR/twisst/twisst.py \
	  --method complete \
	  -t ${tree} \
	  -T 1 \
	  \$TWISST_POPS \
	  --groupsFile ${loc}.${lg}.twisst_pop.txt | \
	  gzip > ${loc}.${lg}.w${twisst_w}.phyml_bionj.weights.tsv.gz
	"""
}
*/
```

> Here, the plug in approach picks up.
At this point we have run `PhyML` locally and deposited the results under `$BASE_DIR/ressources/plugin/trees`.

To restart, we need to emulate the settings needed for the `twisst` process.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.19
// emmulate setting
Channel
	.from(50, 200)
	.combine( vcf_loc_twisst )
	.combine( lg_twisst )
	.set{ twisst_modes }
```

Then, we feed the phylogenies from an external directory into `twisst`.

```{r , eval = FALSE, hilang = 'nf'}
// git 5.20
// run the topology weighting on the  phylogenies
process twisst_plugin {
	label 'L_G120g40h_twisst_plugin'
	publishDir "../../2_analysis/twisst/weights/", mode: 'copy'
	tag "${loc}-${lg}-${mode}"

	input:
	set val( mode ), val( loc ), file( vcf ), file( pop ), val( lg ) from twisst_modes

	output:
	set val( loc ), val( lg ), file( "*.weights.tsv.gz" ) into ( twisst_output )

	script:
	"""
	module load intel17.0.4 intelmpi17.0.4

	awk '{print \$1"\\t"\$1}' ${pop} | \
	sed 's/\\(...\\)\\(...\\)\$/\\t\\1\\t\\2/g' | \
	cut -f 1,3 | \
	awk '{print \$1"_A\\t"\$2"\\n"\$1"_B\\t"\$2}' > ${loc}.${lg}.twisst_pop.txt

	TWISST_POPS=\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d',' | sed 's/,/ -g /g; s/^/-g /' )

	mpirun \$NQSII_MPIOPTS -np 1 \
	python \$SFTWR/twisst/twisst.py \
	  --method complete \
	  -t \$BASE_DIR/ressources/plugin/trees/${loc}/${loc}.${lg}.w${mode}.phyml_bionj.trees.gz \
	  \$TWISST_POPS \
	  --groupsFile ${loc}.${lg}.twisst_pop.txt | \
	  gzip > ${loc}.${lg}.w${mode}.phyml_bionj.weights.tsv.gz
	"""
}
```
</div>

At this step we are done with phlyogeny and the topology weighting.

---
