[
["index.html", "Script repository (Hench et al. supplement) 1 Intro 1.1 Analysis 1.2 Prerequesites 1.3 Figures 1.4 R setup", " Script repository (Hench et al. supplement) Kosmas Hench and Martin Helmkampf 2021-11-26 1 Intro Disclaimer: We are currently still in the process of updating the documentation after the last round of revisions. Some sections are thus still not up to date. This repository contains the complete workflow used in the paper “Rapid radiation in a highly diverse marine environment.” The individual chapters of this documentation follow the separate main steps of the workflow. Each of the chapters thus refers to an individual prefix in the git x.x references of the papers method section. The individual steps partly depend on each other - especially git 1 - git 3 should be executed in order and before the other steps. 1.1 Analysis A documentation of the data preparation and the data analysis (git 1.x - 19.x) can be found at: git 1.x: Genotyping git 2.x: Genotyping all base pairs git 3.x: Analysis (FST &amp; GxP) git 4.x: Analysis (dXY &amp; \\(\\pi\\)) git 5.x: Analysis (topolgy weighting) git 6.x: Analysis (\\(\\rho\\)) git 7.x: Analysis (PCA) git 8.x: Analysis (demographic history) git 9.x: Analysis (hybridization) git 10.x: Analysis (admixture) git 11.x: Analysis (allele age) git 12.x: Analysis (FST permutation) git 13.x: Analysis (whg phylogeny) git 14.x: Analysis (outlier region phylogeny) git 15.x: Analysis (\\(\\pi\\) with/without outlier regions) git 16.x: Analysis (IBD) git 17.x: Analysis (dstats) git 18.x: Genotyping all base pairs (mtDNA and unplaced Contigs) git 19.x: Analysis (Serraninae phyologeny) 1.2 Prerequesites All scripts assume two variables to be set within the bash environment: $BASE_DIR is assumed to point to the base folder of this repository $SFTWR is a folder that contains all the software dependencies that are used within the scripts The analysis is controlled using the workflow manager nextflow and uses slightly different configurations across the individual pipelines. The exact commands used to execute the analysis during the development of the publication are stored within the aliases set within sh/nextflow_alias.sh. Furthermore, external dependencies need to be downloaded and deployed at the expected places (s. README.md at the ressources folder). 1.3 Figures The creation of the figures is bundled in a single script (git 20) which can be executed once all nextflow scripts have successfully run. cd $BASE_DIR bash sh/create_figures.sh This is basically just a wrapper script that will run all scripts located under $BASE_DIR/R/fig. Under this location, you will find one R script per figure (and suppl. figure). So if you are only interested in a single figure - that is the place to start looking. Furthermore, a more detailed documentation exists for all the figure scripts used for the manuscript: F1, F2, F3, F4 F5 and F6 as well as for all the supplementary figures: SF1, SF2, SF3, SF4, SF5, SF6, SF7, SF8, SF9, SF10, SF11, SF12, SF13, SF14, SF15, SF16, SF17, SF18, SF19, SF20 and SF21. 1.4 R setup There is an additional R package needed to run the plotting scripts for the figures ({GenomicOriginsScripts}). This depends on several non-CRAN R-packages, so to be able to install the package successfully, the following packages will also need to be installed: # installing non-CRAN dependencies install.packages(&quot;remotes&quot;) remotes::install_bioc(&quot;rtracklayer&quot;) remotes::install_github(&quot;YuLab-SMU/ggtree&quot;) remotes::install_github(&quot;k-hench/hypogen&quot;) remotes::install_github(&quot;k-hench/hypoimg&quot;) # installing GenomicOriginsScripts remotes::install_github(&quot;k-hench/GenomicOriginsScripts&quot;) Once these non-CRAN packages are installed, it should be possible to re-create the used R environment using the {renv} package. After opening the RStudio project (hamlet_radiation.Rproj), call: # restoring R environment install.packages(&quot;renv&quot;) renv::restore() Apart from the specific R packages that can be retrieved via {renv} from the renv.lock file, the used R setup at the at time of compilation is as follows: sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 20.04.3 LTS ## ## Matrix products: default ## BLAS: /usr/local/lib/R/lib/libRblas.so ## LAPACK: /usr/local/lib/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_4.0.3 magrittr_2.0.1 bookdown_0.19 htmltools_0.5.1.1 ## [5] tools_4.0.3 yaml_2.2.1 stringi_1.5.3 rmarkdown_2.7.6 ## [9] knitr_1.31 stringr_1.4.0 digest_0.6.27 xfun_0.22 ## [13] rlang_0.4.10 evaluate_0.14 "],
["git-1-genotyping-i-snps-only.html", "2 (git 1) Genotyping I (SNPs only) 2.1 Summary 2.2 Details of genotyping.nf", " 2 (git 1) Genotyping I (SNPs only) This pipeline can be executed as follows: cd $BASE_DIR/nf/01_genotyping source ../../sh/nextflow_alias.sh nf_run_gatk 2.1 Summary The genotyping procedure is controlled by the nextflow script genotyping.nf (located under $BASE_DIR/nf/01_genotyping/). It takes the analysis from the raw sequencing data to the genotyped and phased SNPs. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the raw data input, red arrows depict output that is exported for further use.) 2.2 Details of genotyping.nf 2.2.1 Data preparation The nextflow script starts with a small header and then opens the analysis by reading a table with meta data about the samples. The table is parsed and the values are stored in nextflow variables. #!/usr/bin/env nextflow /* =============================================================== Disclaimer: This pipeline needs a lot of time &amp; memory to run: All in all we used roughly 10 TB and ran for about 1 Month (mainly due to limited bandwidth on the cluster durint the &quot;receive_tuple step) =============================================================== */ // git 1.1 /* open the pipeline based on the metadata spread sheet that includes all information necessary to assign read groups to the sequencing data, split the spread sheet by row and feed it into a channel */ Channel .fromPath(&#39;../../metadata/file_info.txt&#39;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ id:row.id, label:row.label, file_fwd:row.file_fwd, file_rev:row.file_rev, flowcell_id_fwd:row.flowcell_id_fwd, lane_fwd:row.lane_fwd, company:row.company] } .set { samples_ch } Below is a little preview of the table containing the sample meta data: id label spec geo date coord_N coord_W company .. 16_21-30 16_21-30nigpan nig pan 2016 NA NA duke … 16_21-30 16_21-30nigpan nig pan 2016 NA NA duke … 16_31-40 16_31-40unipan uni pan 2016 NA NA duke … 16_31-40 16_31-40unipan uni pan 2016 NA NA duke … 17996 17996indbel ind bel 2004-07-27 16.801 -88.079 novogene … 17997 17997indbel ind bel 2004-07-27 16.801 -88.079 novogene … … … … … … … … … The first step to prepare the data for the GATK best practices, is to convert the sample sequences from *.fq to *.bam format to assign read groups: // git 1.2 /* for every sequencing file, convert into ubam format and assign read groups */ process split_samples { label &#39;L_20g2h_split_samples&#39; input: val x from samples_ch output: set val( &quot;${x.label}.${x.lane_fwd}&quot; ), file( &quot;${x.label}.${x.lane_fwd}.ubam.bam&quot; ) into ubams_mark, ubams_merge script: &quot;&quot;&quot; echo -e &quot;---------------------------------&quot; echo -e &quot;Label:\\t\\t${x.label}\\nFwd:\\t\\t${x.file_fwd}\\nRev:\\t\\t${x.file_rev}&quot; echo -e &quot;Flowcell:\\t${x.flowcell_id_fwd}\\nLane:\\t\\t${x.lane_fwd}&quot; echo -e &quot;Read group:\\t${x.flowcell_id_fwd}.${x.lane_fwd}\\nCompany:\\t${x.company}&quot; mkdir -p \\$BASE_DIR/temp_files gatk --java-options &quot;-Xmx20G&quot; \\ FastqToSam \\ -SM=${x.label} \\ -F1=\\$BASE_DIR/data/seqdata/${x.file_fwd} \\ -F2=\\$BASE_DIR/data/seqdata/${x.file_rev} \\ -O=${x.label}.${x.lane_fwd}.ubam.bam \\ -RG=${x.label}.${x.lane_fwd} \\ -LB=${x.label}&quot;.lib1&quot; \\ -PU=${x.flowcell_id_fwd}.${x.lane_fwd} \\ -PL=Illumina \\ -CN=${x.company} \\ --TMP_DIR=\\$BASE_DIR/temp_files; &quot;&quot;&quot; } The second step is marking the Illumina adapters. // git 1.3 /* for every ubam file, mark Illumina adapters */ process mark_adapters { label &#39;L_20g2h_mark_adapters&#39; tag &quot;${sample}&quot; input: set val( sample ), file( input ) from ubams_mark output: set val( sample ), file( &quot;*.adapter.bam&quot;) into adapter_bams file &quot;*.adapter.metrics.txt&quot; into adapter_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx18G&quot; \\ MarkIlluminaAdapters \\ -I=${input} \\ -O=${sample}.adapter.bam \\ -M=${sample}.adapter.metrics.txt \\ -TMP_DIR=\\$BASE_DIR/temp_files; &quot;&quot;&quot; } We need to pass on the unaligned .bam file and the file containing the adapter information together, so the output of the first two processes are matched by the combined sample and sequencing lane information. // git 1.4 adapter_bams .combine(ubams_merge, by:0) .set {merge_input} For the actual mapping, the sequences are transformed back into .fq format, aligned using bwa and merged back with their original read group information. // git 1.5 /* this step includes a 3 step pipeline: * - re-transformatikon into fq format * - mapping aginst the reference genome_file * - merging with the basuch ubams to include read group information */ process map_and_merge { label &#39;L_75g24h8t_map_and_merge&#39; tag &quot;${sample}&quot; input: set val( sample ), file( adapter_bam_input ), file( ubam_input ) from merge_input output: set val( sample ), file( &quot;*.mapped.bam&quot; ) into mapped_bams script: &quot;&quot;&quot; set -o pipefail gatk --java-options &quot;-Xmx68G&quot; \\ SamToFastq \\ -I=${adapter_bam_input} \\ -FASTQ=/dev/stdout \\ -INTERLEAVE=true \\ -NON_PF=true \\ -TMP_DIR=\\$BASE_DIR/temp_files | \\ bwa mem -M -t 8 -p \\$BASE_DIR/ressources/HP_genome_unmasked_01.fa /dev/stdin | gatk --java-options &quot;-Xmx68G&quot; \\ MergeBamAlignment \\ --VALIDATION_STRINGENCY SILENT \\ --EXPECTED_ORIENTATIONS FR \\ --ATTRIBUTES_TO_RETAIN X0 \\ -ALIGNED_BAM=/dev/stdin \\ -UNMAPPED_BAM=${ubam_input} \\ -OUTPUT=${sample}.mapped.bam \\ --REFERENCE_SEQUENCE=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa.gz \\ -PAIRED_RUN true \\ --SORT_ORDER &quot;unsorted&quot; \\ --IS_BISULFITE_SEQUENCE false \\ --ALIGNED_READS_ONLY false \\ --CLIP_ADAPTERS false \\ --MAX_RECORDS_IN_RAM 2000000 \\ --ADD_MATE_CIGAR true \\ --MAX_INSERTIONS_OR_DELETIONS -1 \\ --PRIMARY_ALIGNMENT_STRATEGY MostDistant \\ --UNMAPPED_READ_STRATEGY COPY_TO_TAG \\ --ALIGNER_PROPER_PAIR_FLAGS true \\ --UNMAP_CONTAMINANT_READS true \\ -TMP_DIR=\\$BASE_DIR/temp_files &quot;&quot;&quot; } Next, the duplicates are being marked. // git 1.6 /* for every mapped sample,sort and mark duplicates * (intermediate step is required to create .bai file) */ process mark_duplicates { label &#39;L_32g30h_mark_duplicates&#39; publishDir &quot;../../1_genotyping/0_sorted_bams/&quot;, mode: &#39;symlink&#39; tag &quot;${sample}&quot; input: set val( sample ), file( input ) from mapped_bams output: set val { sample - ~/\\.(\\d+)/ }, val( sample ), file( &quot;*.dedup.bam&quot;) into dedup_bams file &quot;*.dedup.metrics.txt&quot; into dedup_metrics script: &quot;&quot;&quot; set -o pipefail gatk --java-options &quot;-Xmx30G&quot; \\ SortSam \\ -I=${input} \\ -O=/dev/stdout \\ --SORT_ORDER=&quot;coordinate&quot; \\ --CREATE_INDEX=false \\ --CREATE_MD5_FILE=false \\ -TMP_DIR=\\$BASE_DIR/temp_files \\ | \\ gatk --java-options &quot;-Xmx30G&quot; \\ SetNmAndUqTags \\ --INPUT=/dev/stdin \\ --OUTPUT=intermediate.bam \\ --CREATE_INDEX=true \\ --CREATE_MD5_FILE=true \\ -TMP_DIR=\\$BASE_DIR/temp_files \\ --REFERENCE_SEQUENCE=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa.gz gatk --java-options &quot;-Xmx30G&quot; \\ MarkDuplicates \\ -I=intermediate.bam \\ -O=${sample}.dedup.bam \\ -M=${sample}.dedup.metrics.txt \\ -MAX_FILE_HANDLES=1000 \\ -TMP_DIR=\\$BASE_DIR/temp_files rm intermediate* &quot;&quot;&quot; } As a preparation for the actual genotyping, the .bam files are being indexed. // git 1.7 /* index al bam files */ process index_bam { label &#39;L_32g1h_index_bam&#39; tag &quot;${sample}&quot; input: set val( sample ), val( sample_lane ), file( input ) from dedup_bams output: set val( sample ), val( sample_lane ), file( input ), file( &quot;*.bai&quot;) into ( indexed_bams, pir_bams ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx30G&quot; \\ BuildBamIndex \\ -INPUT=${input} &quot;&quot;&quot; } At this point the preparation of the sequencing is done and we can start with the genotyping. (The output of the data preparation is split and one copy is later also used to prepare the read aware phasing in the process called extractPirs.) 2.2.2 Genotying Since some of our samples were split over several lanes, we now need to collect all .bam files for each sample. // git 1.8 /* collect all bam files for each sample */ indexed_bams .groupTuple() .set {tubbled} Now, we can create the genotype likelihoods for each individual sample. // git 1.9 /* create one *.g.vcf file per sample */ process receive_tuple { label &#39;L_36g47h_receive_tuple&#39; publishDir &quot;../../1_genotyping/1_gvcfs/&quot;, mode: &#39;symlink&#39; tag &quot;${sample}&quot; input: set sample, sample_lane, bam, bai from tubbled output: file( &quot;*.g.vcf.gz&quot;) into gvcfs file( &quot;*.vcf.gz.tbi&quot;) into tbis script: &quot;&quot;&quot; INPUT=\\$(echo ${bam} | sed &#39;s/\\\\[/-I /g; s/\\\\]//g; s/,/ -I/g&#39;) gatk --java-options &quot;-Xmx35g&quot; HaplotypeCaller \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ \\$INPUT \\ -O ${sample}.g.vcf.gz \\ -ERC GVCF &quot;&quot;&quot; } The individual genotype likelihoods are collected and combined for the entire data set. // git 1.10 /* collect and combine all *.g.vcf files */ process gather_gvcfs { label &#39;L_O88g90h_gather_gvcfs&#39; publishDir &quot;../../1_genotyping/1_gvcfs/&quot;, mode: &#39;symlink&#39; echo true input: file( gvcf ) from gvcfs.collect() file( tbi ) from tbis.collect() output: set file( &quot;cohort.g.vcf.gz&quot; ), file( &quot;cohort.g.vcf.gz.tbi&quot; ) into ( gcvf_snps, gvcf_acs, gvcf_indel ) script: &quot;&quot;&quot; GVCF=\\$(echo &quot; ${gvcf}&quot; | sed &#39;s/ /-V /g; s/vcf.gz/vcf.gz /g&#39;) gatk --java-options &quot;-Xmx85g&quot; \\ CombineGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ \\$GVCF \\ -O cohort.g.vcf.gz &quot;&quot;&quot; } All samples are jointly genotyped. // git 1.11 /* actual genotyping step (varinat sites only) */ process joint_genotype_snps { label &#39;L_O88g90h_joint_genotype&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;symlink&#39; input: set file( vcf ), file( tbi ) from gcvf_snps output: set file( &quot;raw_var_sites.vcf.gz&quot; ), file( &quot;raw_var_sites.vcf.gz.tbi&quot; ) into ( raw_var_sites, raw_var_sites_to_metrics ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=${vcf} \\ -O=intermediate.vcf.gz gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-include=SNP \\ -O=raw_var_sites.vcf.gz rm intermediate.* &quot;&quot;&quot; } The output of this process is split and used to collect the genotype metrics to inform the hard filtering of SNPs and to pass on the genotypes to the process called filterSNPs. At this point we create a channel containing all 24 hamlet linkage groups (LGs). This is used later (in the process called extractPirs) since all LGs are phased separately and only located at this part of the script for historical reasons (sorry :/). // git 1.12 /* generate a LG channel */ Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .into{ LG_ids1; LG_ids2 } The metrics of the raw genotypes are collected. // git 1.13 /* produce metrics table to determine filtering thresholds - ups forgot to extract SNPS first*/ process joint_genotype_metrics { label &#39;L_28g5h_genotype_metrics&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;move&#39; input: set file( vcf ), file( tbi ) from raw_var_sites_to_metrics output: file( &quot;${vcf}.table.txt&quot; ) into raw_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx25G&quot; \\ VariantsToTable \\ --variant=${vcf} \\ --output=${vcf}.table.txt \\ -F=CHROM -F=POS -F=MQ \\ -F=QD -F=FS -F=MQRankSum -F=ReadPosRankSum \\ --show-filtered &quot;&quot;&quot; } Based on the thresholds derived from the genotype metrics, the genotypes are first tagged and then filtered. After this, the data is filtered for missingness and only bi-allelic SNPs are selected. // git 1.14 /* filter snps basaed on locus annotations, missingness and type (bi-allelic only) */ process filterSNPs { label &#39;L_78g10h_filter_Snps&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;symlink&#39; input: set file( vcf ), file( tbi ) from raw_var_sites output: set file( &quot;filterd_bi-allelic.vcf.gz&quot; ), file( &quot;filterd_bi-allelic.vcf.gz.tbi&quot; ) into filtered_snps script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ -O=intermediate.filterd.vcf.gz \\ --exclude-filtered vcftools \\ --gzvcf intermediate.filterd.vcf.gz \\ --max-missing-count 17 \\ --max-alleles 2 \\ --stdout \\ --recode | \\ bgzip &gt; filterd_bi-allelic.vcf.gz tabix -p vcf filterd_bi-allelic.vcf.gz rm intermediate.* &quot;&quot;&quot; } At this point, the genotying is done. 2.2.3 Phasing To get from genotypes to haplotypes, we apply read-aware phasing using Shapeit. This takes the original sequencing reads into account, so in the first step the reads are screened for Phase Informative Reads (i.e. reads containing more than a single SNP). This needs to be done for each LG independently, so we first need to split the genotypes before running extractPIRs. // git 1.15 // extract phase informative reads from // alignments and SNPs process extractPirs { label &#39;L_78g10h_extract_pirs&#39; input: val( lg ) from LG_ids2 set val( sample ), val( sample_lane ), file( input ), file( index ) from pir_bams.collect() set file( vcf ), file( tbi ) from filtered_snps output: set val( lg ), file( &quot;filterd_bi-allelic.LG${lg}.vcf.gz&quot; ), file( &quot;filterd_bi-allelic.LG${lg}.vcf.gz.tbi&quot; ), file( &quot;PIRsList-LG${lg}.txt&quot; ) into pirs_lg script: &quot;&quot;&quot; LG=&quot;LG${lg}&quot; awk -v OFS=&#39;\\t&#39; -v dir=\\$PWD -v lg=\\$LG &#39;{print \\$1,dir&quot;/&quot;\\$2,lg}&#39; \\$BASE_DIR/metadata/bamlist_proto.txt &gt; bamlist.txt vcftools \\ --gzvcf ${vcf} \\ --chr \\$LG \\ --stdout \\ --recode | \\ bgzip &gt; filterd_bi-allelic.LG${lg}.vcf.gz tabix -p vcf filterd_bi-allelic.LG${lg}.vcf.gz extractPIRs \\ --bam bamlist.txt \\ --vcf filterd_bi-allelic.LG${lg}.vcf.gz \\ --out PIRsList-LG${lg}.txt \\ --base-quality 20 \\ --read-quality 15 &quot;&quot;&quot; } Using those PIRs, we can then proceed with the actual phasing. The resulting haplotypes are converted back into .vcf format. // git 1.16 // run the actual phasing process run_shapeit { label &#39;L_75g24h8t_run_shapeit&#39; input: set val( lg ), file( vcf ), file( tbi ), file( pirs ) from pirs_lg output: file( &quot;phased-LG${lg}.vcf.gz&quot; ) into phased_lgs script: &quot;&quot;&quot; LG=&quot;LG${lg}&quot; shapeit \\ -assemble \\ --input-vcf ${vcf} \\ --input-pir ${pirs} \\ --thread 8 \\ -O phased-LG${lg} shapeit \\ -convert \\ --input-hap phased-LG${lg} \\ --output-vcf phased-LG${lg}.vcf bgzip phased-LG${lg}.vcf &quot;&quot;&quot; } After the phasing, we merge the LGs back together to get a single data set. We export a comple data set as well as one that was filtered for a minor allele count of at least two. // git 1.17 // merge the phased LGs back together. // the resulting vcf file represents // the &#39;SNPs only&#39; data set process merge_phased { label &#39;L_28g5h_merge_phased_vcf&#39; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &#39;move&#39; input: file( vcf ) from phased_lgs.collect() output: set file( &quot;phased.vcf.gz&quot; ), file( &quot;phased.vcf.gz.tbi&quot; ) into phased_vcf set file( &quot;phased_mac2.vcf.gz&quot; ), file( &quot;phased_mac2.vcf.gz.tbi&quot; ) into phased_mac2_vcf script: &quot;&quot;&quot; vcf-concat \\ phased-LG* | \\ grep -v ^\\$ | \\ tee phased.vcf | \\ vcftools --vcf - --mac 2 --recode --stdout | \\ bgzip &gt; phased_mac2.vcf.gz bgzip phased.vcf tabix -p vcf phased.vcf.gz tabix -p vcf phased_mac2.vcf.gz &quot;&quot;&quot; } Finally, we are done with the entire genotyping procedure for the SNPs olny data set. 2.2.4 Indel masks The genotyping.nf workflow contains an appendix that makes use of the genotyping likelihoods created in step git 1.10 to create an indel mask that is later used in the inference of the hamlet demographic history (git 8.x). (This part is excluded from the initial visualization of this script) We restart by reopening the joint-sample genotype likelyhoods file and calling the indels from it. /* ========================================= */ /* appendix: generate indel masks for msmc: */ // git 1.18 // reopen the gvcf file to also genotype indels process joint_genotype_indel { label &#39;L_O88g90h_genotype_indel&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from gvcf_indel output: set file( &quot;raw_var_indel.vcf.gz&quot; ), file( &quot;raw_var_indel.vcf.gz.tbi&quot; ) into ( raw_indel, raw_indel_to_metrics ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$REF_GENOME \\ -V=${vcf} \\ -O=intermediate.vcf.gz gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$REF_GENOME \\ -V=intermediate.vcf.gz \\ --select-type-to-include=INDEL \\ -O=raw_var_indel.vcf.gz rm intermediate.* &quot;&quot;&quot; } We export the the indel genotype metrics to determine cutoff values for the hard filtering step. // git 1.19 // export indel metrics for filtering process indel_metrics { label &#39;L_28g5h_genotype_metrics&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from raw_indel_to_metrics output: file( &quot;${vcf}.table.txt&quot; ) into raw_indel_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx25G&quot; \\ VariantsToTable \\ --variant=${vcf} \\ --output=${vcf}.table.txt \\ -F=CHROM -F=POS -F=MQ \\ -F=QD -F=FS -F=MQRankSum -F=ReadPosRankSum \\ --show-filtered &quot;&quot;&quot; } Based on the exported metrics the genotypes are being filtered. // git 1.20 // hard filter indels and create mask process filterIndels { label &#39;L_78g10h_filter_indels&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from raw_indel output: set file( &quot;filterd.indel.vcf.gz&quot; ), file( &quot;filterd.indel.vcf.gz.tbi&quot; ) into filtered_indel file( &quot;indel_mask.bed.gz&quot; ) into indel_mask_ch /* FILTER THRESHOLDS NEED TO BE UPDATED */ script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$REF_GENOME \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;SOR &gt; 3.0&quot; \\ --filter-name &quot;filter_SOR&quot; \\ --filter-expression &quot;InbreedingCoeff &lt; -0.25&quot; \\ --filter-name &quot;filter_InbreedingCoeff&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$REF_GENOME \\ -V=intermediate.vcf.gz \\ -O=filterd.indel.vcf.gz \\ --exclude-filtered zcat filterd.indel.vcf.gz | \\ awk &#39;! /\\\\#/&#39; | \\ awk &#39;{if(length(\\$4) &gt; length(\\$5)) print \\$1&quot;\\\\t&quot;(\\$2-6)&quot;\\\\t&quot;(\\$2+length(\\$4)+4); else print \\$1&quot;\\\\t&quot;(\\$2-6)&quot;\\\\t&quot;(\\$2+length(\\$5)+4)}&#39; | \\ gzip -c &gt; indel_mask.bed.gz rm intermediate.* &quot;&quot;&quot; } Since we need one indel mask per linkage group, we create a channel of LGs. // git 1.21 /* create channel of linkage groups */ Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .into{ lg_ch } The linkage group channel is combined with the filtered indels. // git 1.22 // attach linkage groups to indel masks lg_ch.combine( filtered_indel ).set{ filtered_indel_lg } Finally, one mask per linkage group is created from the indel positions. // git 1.23 // split indel mask by linkage group process split_indel_mask { label &#39;L_loc_split_indel_mask&#39; publishDir &quot;../../ressources/indel_masks/&quot;, mode: &#39;copy&#39; input: set val( lg ), file( bed ) from filtered_indel_lg output: set val( lg ), file( &quot;indel_mask.${lg}.bed.gz &quot; ) into lg_indel_mask script: &quot;&quot;&quot; gzip -cd ${bed} | \\ grep ${lg} | \\ gzip -c &gt; indel_mask.${lg}.bed.gz &quot;&quot;&quot; } All indel masks are exported to the resources folder within the root directory. "],
["git-2-genotyping-ii-all-callable-sites.html", "3 (git 2) Genotyping II (all callable sites) 3.1 Summary 3.2 Details of genotyping_all_basepairs.nf", " 3 (git 2) Genotyping II (all callable sites) This pipeline can be executed as follows: cd $BASE_DIR/nf/02_genotyping_all_basepairs source ../../sh/nextflow_alias.sh nf_run_allbp 3.1 Summary The genotyping procedure is controlled by the nextflow script genotyping_all_basepairs.nf (located under $BASE_DIR/nf/02_genotyping_all_basepairs/). Based on an intermediate step from genotyping.nf (git 1.10), this script produces a data set that includes all callable sites - that is SNPs as well a invariant sites that are covered by sequence. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the data input, red arrows depict output that is exported for further use.) 3.2 Details of genotyping_all_basepairs.nf 3.2.1 Data preparation The nextflow script starts with a small header and then imports the joint genotyping likelihoods for all samples produced by genotyping.nf. #!/usr/bin/env nextflow // git 2.1 // open genotype likelyhoods Channel .fromFilePairs(&quot;../../1_genotyping/1_gvcfs/cohort.g.vcf.{gz,gz.tbi}&quot;) .set{ vcf_cohort } The genotyping of the different linkage groups is going to happen in parallel, so we need to initialize a channel for the 24 LGs. // git 2.2 // initialize LG channel Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .set{ ch_LG_ids } The genotyping likelihoods are combined, effectively linking the data set to the 24 parallel LGs. // git 2.3 // combine genotypes and LGs ch_LG_ids.combine( vcf_cohort ).set{ vcf_lg_combo } The samples are jointly genotyped, independently for each LG and including invariant sites. // git 2.4 // actual genotyping step (including invariant sites) process joint_genotype_snps { label &quot;L_O88g90h_LGs_genotype&quot; input: set val( lg ), vcfId, file( vcf ) from vcf_lg_combo output: set val( &#39;all&#39; ), val( lg ), file( &quot;all_site*.vcf.gz&quot; ), file( &quot;all_site*.vcf.gz.tbi&quot; ) into all_bp_by_location script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -L=LG${lg} \\ -V=${vcf[0]} \\ -O=intermediate.vcf.gz \\ --include-non-variant-sites=true gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-exclude=INDEL \\ -O=all_sites.LG${lg}.vcf.gz rm intermediate.* &quot;&quot;&quot; } The genotypes of the different LGs are merged. // git 2.5 // merge all LGs process merge_genotypes { label &#39;L_78g5h_merge_genotypes&#39; echo true input: set val( dummy ), val( lg ), file( vcf ), file( tbi ) from all_bp_by_location.groupTuple() output: file( &quot;all_sites.vcf.gz&quot; ) into all_bp_merged script: &quot;&quot;&quot; INPUT=\\$(ls -1 *vcf.gz | sed &#39;s/^/ -I /g&#39; | cat \\$( echo )) gatk --java-options &quot;-Xmx85g&quot; \\ GatherVcfs \\ \\$INPUT \\ -O=all_sites.vcf.gz &quot;&quot;&quot; } The genotypes are hard filtered based on various genotyping scores. // git 2.6 // quality based filtering process filterSNP_first { label &#39;L_105g30h_filter_gt1&#39; input: file( vcf ) from all_bp_merged output: set file( &quot;intermediate.filterd.vcf.gz&quot; ), file( &quot;intermediate.filterd.vcf.gz.tbi&quot; ) into filtered_snps_first script: &quot;&quot;&quot; module load openssl1.0.2 tabix -p vcf ${vcf} gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; \\ --QUIET true &amp;&gt; var_filt.log gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ -O=intermediate.filterd.vcf.gz \\ --exclude-filtered \\ --QUIET true \\ --verbosity ERROR &amp;&gt; var_select.log &quot;&quot;&quot; } A second filtering is based on the missingness of samples. // git 2.7 // missingness based filtering // the resulting vcf file represents // the &#39;all BP&#39; data set process filterSNP_second { label &#39;L_105g30h_filter_gt2&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from filtered_snps_first output: file( &quot;filterd.allBP.vcf.gz&quot; ) into filtered_snps script: &quot;&quot;&quot; module load openssl1.0.2 vcftools \\ --gzvcf ${vcf} \\ --max-missing-count 17 \\ --stdout \\ --recode | \\ bgzip &gt; filterd.allBP.vcf.gz &quot;&quot;&quot; } Finally, we are done with the second version of genotyping. "],
["git-3-analysis-i-fst-gxp.html", "4 (git 3) Analysis I (FST &amp; GxP) 4.1 Summary 4.2 Details of analysis_fst_gxp.nf", " 4 (git 3) Analysis I (FST &amp; GxP) This pipeline can be executed as follows: cd $BASE_DIR/nf/03_analysis_fst_gxp source ../../sh/nextflow_alias.sh nf_run_basic 4.1 Summary The genetic differentiation, as well as the genotype x phenotype association, are computed within the nextflow script analysis_fst_gxp.nf (located under $BASE_DIR/nf/03_analysis_fst_gxp/). It takes the SNPs only data set and computes \\(F_{ST}\\) and the GxP association. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 4.2 Details of analysis_fst_gxp.nf 4.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into three different streams. #!/usr/bin/env nextflow // git 3.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_locations; vcf_filter; vcf_gxp; vcf_adapt } Since we are going to work on the three sampling locations independently, we create channel for the locations. // git 3.2 // initialize location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } Then we attach the genotypes to the locations. // git 3.3 // attach genotypes to location locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } Next, we define the species sets sampled at the individual locations. // git 3.4 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } For each location, the data is subset to include only local hamlets. // git 3.5 // subset data to local hamlets process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.pop&quot; ) into ( vcf_loc_pair1, vcf_loc_pair2, vcf_loc_pair3 ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.vcf.gz &quot;&quot;&quot; } As we also want to compute global statistics, we create another subset including all sampled hamlets but excluding the outgroups. // git 3.6 // subset the global data set to hamlets only process subset_vcf_hamlets_only { label &quot;L_20g15h_filter_hamlets_only&quot; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.vcf.gz&quot; //module &quot;R3.5.2&quot; input: set vcfId, file( vcf ) from vcf_filter output: file( &quot;hamlets_only.vcf.gz*&quot; ) into vcf_hamlets_only set file( &quot;hamlets_only.vcf.gz*&quot; ), file( &quot;hamlets_only.pop.txt&quot; ) into vcf_multi_fst script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni&quot; &gt; outgroup.pop vcfsamplenames ${vcf[0]} | \\ grep &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; hamlets_only.pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --remove outgroup.pop \\ --recode \\ --stdout | gzip &gt; hamlets_only.vcf.gz &quot;&quot;&quot; } 4.2.2 FST Using this subsets, we compute the global differentiation among all hamlet populations. // ----------- Fst section ----------- // git 3.7 // compute global fst process fst_multi { label &#39;L_20g15h_fst_multi&#39; publishDir &quot;../../2_analysis/fst/50k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.50k.tsv.gz&quot; publishDir &quot;../../2_analysis/fst/10k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.10k.tsv.gz&quot; publishDir &quot;../../2_analysis/fst/logs/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.log&quot; publishDir &quot;../../2_analysis/summaries&quot;, mode: &#39;copy&#39; , pattern: &quot;fst_outliers_998.tsv&quot; //conda &quot;$HOME/miniconda2/envs/py3&quot; //module &quot;R3.5.2&quot; input: set file( vcf ), file( pop ) from vcf_multi_fst output: file( &quot;multi_fst*&quot; ) into multi_fst_output file( &quot;fst_outliers_998.tsv&quot; ) into fst_outlier_output script: &quot;&quot;&quot; awk &#39;{print \\$1&quot;\\\\t&quot;\\$2\\$3}&#39; ${pop} &gt; pop.txt for k in abehon gumhon indbel maybel nigbel nighon nigpan puebel puehon puepan ranhon unibel unihon unipan; do grep \\$k pop.txt | cut -f 1 &gt; pop.\\$k.txt done POP=&quot;--weir-fst-pop pop.abehon.txt \\ --weir-fst-pop pop.gumhon.txt \\ --weir-fst-pop pop.indbel.txt \\ --weir-fst-pop pop.maybel.txt \\ --weir-fst-pop pop.nigbel.txt \\ --weir-fst-pop pop.nighon.txt \\ --weir-fst-pop pop.nigpan.txt \\ --weir-fst-pop pop.puebel.txt \\ --weir-fst-pop pop.puehon.txt \\ --weir-fst-pop pop.puepan.txt \\ --weir-fst-pop pop.ranhon.txt \\ --weir-fst-pop pop.unibel.txt \\ --weir-fst-pop pop.unihon.txt \\ --weir-fst-pop pop.unipan.txt&quot; # fst by SNP # ---------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --stdout 2&gt; multi_fst_snp.log | \\ gzip &gt; multi_fst.tsv.gz # fst 50kb window # --------------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --fst-window-step 5000 \\ --fst-window-size 50000 \\ --stdout 2&gt; multi_fst.50k.log | \\ gzip &gt; multi_fst.50k.tsv.gz # fst 10kb window # --------------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --fst-window-step 1000 \\ --fst-window-size 10000 \\ --stdout 2&gt; multi_fst.10k.log | \\ gzip &gt; multi_fst_snp.tsv.gz Rscript --vanilla \\$BASE_DIR/R/table_fst_outliers.R multi_fst.50k.tsv.gz &quot;&quot;&quot; } Then, we set up the pair wise species comparisons… // git 3.8 // prepare pairwise fsts // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ // ------------------------------ /* channel content after joinig: set [0:val(loc), 1:file(vcf), 2:file(pop), 3:val(spec1), 4:val(spec2)]*/ // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .join( vcf_loc_pair1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .join( vcf_loc_pair2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .join( vcf_loc_pair3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } … and run them. // git 3.9 // compute pairwise fsts process fst_run { label &#39;L_32g4h_fst_run&#39; publishDir &quot;../../2_analysis/fst/50k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.50k.windowed.weir.fst.gz&quot; publishDir &quot;../../2_analysis/fst/10k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.10k.windowed.weir.fst.gz&quot; publishDir &quot;../../2_analysis/fst/logs/&quot;, mode: &#39;copy&#39; , pattern: &quot;${loc}-${spec1}-${spec2}.log&quot; input: set val( loc ), file( vcf ), file( pop ), val( spec1 ), val( spec2 ) from all_fst_pairs_ch output: set val( loc ), file( &quot;*.50k.windowed.weir.fst.gz&quot; ), file( &quot;${loc}-${spec1}-${spec2}.log&quot; ) into fst_50k file( &quot;*.10k.windowed.weir.fst.gz&quot; ) into fst_10k_output file( &quot;${loc}-${spec1}-${spec2}.log&quot; ) into fst_logs script: &quot;&quot;&quot; grep ${spec1} ${pop} &gt; pop1.txt grep ${spec2} ${pop} &gt; pop2.txt vcftools --gzvcf ${vcf} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-step 5000 \\ --fst-window-size 50000 \\ --out ${loc}-${spec1}-${spec2}.50k 2&gt; ${loc}-${spec1}-${spec2}.log vcftools --gzvcf ${vcf} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-size 10000 \\ --fst-window-step 1000 \\ --out ${loc}-${spec1}-${spec2}.10k gzip *.windowed.weir.fst &quot;&quot;&quot; } The genome wide summaries are compiled from the log files of VCFtools. // git 3.10 /* collect the VCFtools logs to crate a table with the genome wide fst values */ process fst_globals { label &#39;L_loc_fst_globals&#39; publishDir &quot;../../2_analysis/summaries&quot;, mode: &#39;copy&#39; , pattern: &quot;fst_globals.txt&quot; //module &quot;R3.5.2&quot; input: file( log ) from fst_logs.collect() output: file( &quot;fst_globals.txt&quot; ) into fst_glob script: &quot;&quot;&quot; cat *.log | \\ grep -E &#39;Weir and Cockerham|--out&#39; | \\ grep -A 3 50k | \\ sed &#39;/^--/d; s/^.*--out //g; s/.50k//g; /^Output/d; s/Weir and Cockerham //g; s/ Fst estimate: /\\t/g&#39; | \\ paste - - - | \\ cut -f 1,3,5 | \\ sed &#39;s/^\\\\(...\\\\)-/\\\\1\\\\t/g&#39; &gt; fst_globals.txt &quot;&quot;&quot; } 4.2.3 GxP The software used for the GxP association (gemma) uses genotypes in plink file format, so the first step was to convert the input. // ----------- G x P section ----------- // git 3.11 // reformat genotypes (1) process plink12 { label &#39;L_20g2h_plink12&#39; input: set vcfId, file( vcf ) from vcf_gxp output: set file( &quot;GxP_plink.map&quot; ), file( &quot;GxP_plink.ped&quot; ) into plink_GxP script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --plink \\ --out GxP_plink plink \\ --file GxP_plink \\ --recode12 \\ --out hapmap &quot;&quot;&quot; } // git 3.12 // reformat genotypes (2) process GxP_run { label &#39;L_20g2h_GxP_binary&#39; input: set file( map ), file( ped ) from plink_GxP output: set file( &quot;*.bed&quot; ), file( &quot;*.bim&quot; ),file( &quot;*.fam&quot; ) into plink_binary script: &quot;&quot;&quot; # convert genotypes into binary format (bed/bim/fam) plink \\ --noweb \\ --file GxP_plink \\ --make-bed \\ --out GxP_plink_binary &quot;&quot;&quot; } Additionally to the genotypes, the phenotypes of the samples are needed. // git 3.13 // import phenotypes Channel .fromPath(&quot;../../metadata/phenotypes.sc&quot;) .set{ phenotypes_raw } As a legacy of the early analysis, we perform a PCA on the phenotypes and report the extended phenotype data including the scoring on PC1 &amp; PC2. // git 3.14 // run PCA on phenotypes process phenotye_pca { label &quot;L_loc_phenotype_pca&quot; publishDir &quot;../../2_analysis/phenotype&quot;, mode: &#39;copy&#39; , pattern: &quot;*.txt&quot; //module &quot;R3.5.2&quot; input: file( sc ) from phenotypes_raw output: file( &quot;phenotypes.txt&quot; ) into phenotype_file file( &quot;phenotype_pca*.pdf&quot; ) into phenotype_pca script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/phenotypes_pca.R ${sc} &quot;&quot;&quot; } We set up all the traits for which we want to perform a GxP association. // git 3.15 // setup GxP traits Channel .from(&quot;Bars&quot;, &quot;Snout&quot;, &quot;Peduncle&quot;) .set{ traits_ch } Then, we combine the reformatted genotypes with the phenotyes and the traits of interest. // git 3.16 // bundle GxP input traits_ch.combine( plink_binary ).combine( phenotype_file ).set{ trait_plink_combo } Having collected all the input, we can now run the GxP. // git 3.17 // actually run the GxP process gemma_run { label &#39;L_32g4h_GxP_run&#39; publishDir &quot;../../2_analysis/GxP/bySNP/&quot;, mode: &#39;copy&#39; //module &quot;R3.5.2&quot; input: set val( pheno ), file( bed ), file( bim ), file( fam ), file( pheno_file ) from trait_plink_combo output: file(&quot;*.GxP.txt.gz&quot;) into gemma_results script: &quot;&quot;&quot; source \\$BASE_DIR/sh/body.sh BASE_NAME=\\$(echo ${fam} | sed &#39;s/.fam//g&#39;) mv ${fam} \\$BASE_NAME-old.fam cp \\${BASE_NAME}-old.fam ${fam} # 1) replace the phenotype values Rscript --vanilla \\$BASE_DIR/R/assign_phenotypes.R ${fam} ${pheno_file} ${pheno} # 2) create relatedness matrix of samples using gemma gemma -bfile \\$BASE_NAME -gk 1 -o ${pheno} # 3) fit linear model using gemma (-lm) gemma -bfile \\$BASE_NAME -lm 4 -miss 0.1 -notsnp -o ${pheno}.lm # 4) fit linear mixed model using gemma (-lmm) gemma -bfile \\$BASE_NAME -k output/${pheno}.cXX.txt -lmm 4 -o ${pheno}.lmm # 5) reformat output sed &#39;s/\\\\trs\\\\t/\\\\tCHROM\\\\tPOS\\\\t/g; s/\\\\([0-2][0-9]\\\\):/\\\\1\\\\t/g&#39; output/${pheno}.lm.assoc.txt | \\ cut -f 2,3,9-14 | body sort -k1,1 -k2,2n | gzip &gt; ${pheno}.lm.GxP.txt.gz sed &#39;s/\\\\trs\\\\t/\\\\tCHROM\\\\tPOS\\\\t/g; s/\\\\([0-2][0-9]\\\\):/\\\\1\\\\t/g&#39; output/${pheno}.lmm.assoc.txt | \\ cut -f 2,3,8-10,13-15 | body sort -k1,1 -k2,2n | gzip &gt; ${pheno}.lmm.GxP.txt.gz &quot;&quot;&quot; } To smooth the GxP results, we initialize two resolutions (50 kb windows with 5 kb increments and 10 kb windows with 1 kb increments). // git 3.18 // setup smoothing levels Channel .from([[50000, 5000], [10000, 1000]]) .set{ gxp_smoothing_levels } The we apply all smoothing levels to the raw GxP output… // git 3.19 // apply all smoothing levels gemma_results.combine( gxp_smoothing_levels ).set{ gxp_smoothing_input } .. and run the smoothing. // git 3.20 // actually run the smoothing process gemma_smooth { label &#39;L_20g2h_GxP_smooth&#39; publishDir &quot;../../2_analysis/GxP/${win}&quot;, mode: &#39;copy&#39; input: set file( lm ), file( lmm ), val( win ), val( step ) from gxp_smoothing_input output: set val( win ), file( &quot;*.lm.*k.txt.gz&quot; ) into gxp_lm_smoothing_output set val( win ), file( &quot;*.lmm.*k.txt.gz&quot; ) into gxp_lmm_smoothing_output script: &quot;&quot;&quot; \\$BASE_DIR/sh/gxp_slider ${lm} ${win} ${step} \\$BASE_DIR/sh/gxp_slider ${lmm} ${win} ${step} &quot;&quot;&quot; } At this step we are done with differentiation and GxP. 4.2.4 FST within species (adaptation scenario) To judge the independence of the individual populations of the hamlets species sampled at several locations, we also compute the \\(F_{ST}\\) among those populations. We start by defining the species-set for which we sampled multiple populations. // Fst within species --------------------------------------------------------- // git 3.21 // define species set Channel .from( &quot;nig&quot;, &quot;pue&quot;, &quot;uni&quot;) .set{ species_ch } Then, we define the sampling locations. // git 3.22 // define location set Channel.from( [[1, &quot;bel&quot;], [2, &quot;hon&quot;], [3, &quot;pan&quot;]]).into{ locations_ch_1;locations_ch_2 } Next, we create all population-pairs and bind it to the genotype file. // git 3.23 // create location pairs locations_ch_1 .combine(locations_ch_2) .filter{ it[0] &lt; it[2] } .map{ it[1,3]} .combine( species_ch ) .combine( vcf_adapt ) .set{ vcf_location_combo_adapt } Last, we compute the pair wise \\(F_{ST}\\) among all populations. // git 3.24 // compute pairwise fsts process fst_run_adapt { label &#39;L_20g4h_fst_run_adapt&#39; publishDir &quot;../../2_analysis/fst/adapt/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.log&quot; input: set val( loc1 ), val( loc2 ), val( spec ), val(vcf_indx), file( vcf ) from vcf_location_combo_adapt output: file( &quot;adapt_${spec}${loc1}-${spec}${loc2}.log&quot; ) into fst_adapt_logs script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | grep ${spec}${loc1} &gt; pop1.txt vcfsamplenames ${vcf[0]} | grep ${spec}${loc2} &gt; pop2.txt vcftools --gzvcf ${vcf[0]} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --out adapt_${spec}${loc1}-${spec}${loc2} 2&gt; adapt_${spec}${loc1}-${spec}${loc2}.log &quot;&quot;&quot; } "],
["git-4-analysis-ii-dxy-pi.html", "5 (git 4) Analysis II (dXY &amp; pi) 5.1 Summary 5.2 Details of analysis_dxy.nf", " 5 (git 4) Analysis II (dXY &amp; pi) This pipeline can be executed as follows: cd $BASE_DIR/nf/04_analysis_dxy source ../../sh/nextflow_alias.sh nf_run_dxy 5.1 Summary The genetic divergence, as well diversity, are computed within the nextflow script analysis_dxy.nf (located under $BASE_DIR/nf/04_analysis_dxy/). It takes the all BP data set and computes \\(d_{XY}\\) and \\(\\pi\\). Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 5.2 Details of analysis_dxy.nf 5.2.1 Data preparation The nextflow script starts by opening the genotype data and feeding it into two different streams (one for dXY and one for \\(\\pi\\)). #!/usr/bin/env nextflow // This pipeline includes the analysis run on the // all callable sites data sheet (dxy). // git 4.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/filterd.allBP.vcf.{gz,gz.tbi}&quot;) .into{ vcf_ch; vcf_pi_ch } The computation of dXY is split by linkage group, so we need to initialize a channel for the LGs. // git 4.2 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .set{ lg_ch } Now, we can subset the data set and convert it to a custom genotype format. // git 4.3 // split by LG and reformat the genotypes process split_allBP { label &#39;L_32g15h_split_allBP&#39; tag &quot;LG${lg}&quot; input: set val( lg ), vcfId, file( vcf ) from lg_ch.combine( vcf_ch ) output: set val( lg ), file( &#39;filterd.allBP.vcf.gz&#39; ), file( &quot;allBP.LG${lg}.geno.gz&quot; ) into geno_ch script: &quot;&quot;&quot; module load openssl1.0.2 vcftools --gzvcf ${vcf[0]} \\ --chr LG${lg} \\ --recode \\ --stdout | bgzip &gt; allBP.LG${lg}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i allBP.LG${lg}.vcf.gz | gzip &gt; allBP.LG${lg}.geno.gz &quot;&quot;&quot; } Since the species composition differs between locations, we need to initialize three separate sets of species. These are going to be used to create the divergence species pairs. // git 4.4 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } For the diversity, we also initialize the full set of populations of the study. // git 4.5 // init all sampled populations (for pi) Channel .from(&#39;indbel&#39;, &#39;maybel&#39;, &#39;nigbel&#39;, &#39;puebel&#39;, &#39;unibel&#39;, &#39;abehon&#39;, &#39;gumhon&#39;, &#39;nighon&#39;, &#39;puehon&#39;, &#39;ranhon&#39;, &#39;unihon&#39;, &#39;nigpan&#39;, &#39;puepan&#39;, &#39;unipan&#39;) .set{spec_dxy} We want to run a sliding window at different resolutions, so we set up a channel for these. // git 4.6 // init slining window resolutions Channel .from( 1, 5 ) .into{ kb_ch; kb_ch2; kb_ch3 } 5.2.2 dXY To prepare all species comparisons used to estimate divergence, we combine each species witch all other species within a location. // git 4.7 // prepare pair wise dxy // ------------------------------ // create all possible species pairs depending on location // and combine with genotype subset (for the respective location) // ------------------------------ // channel content after joining: // set [0:val(loc), 1:file(vcf), 2:file(pop), 3:val(spec1), 4:val(spec2)] // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .combine( bel_spec1_ch ) .combine( bel_spec2_ch ) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .combine( hon_spec1_ch ) .combine(hon_spec2_ch) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .combine( pan_spec1_ch ) .combine(pan_spec2_ch) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} Now we attach the genotypes and the resolution level to the species pairs. // git 4.8 // combine species pair with genotypes (and window size) bel_pairs_ch .concat( hon_pairs_ch, pan_pairs_ch ) .combine( geno_ch ) .combine( kb_ch ) .into { all_dxy_pairs_ch; random_dxy_pairs_ch } At this point, we can calculate dXY. // git 4.9 // compute the dxy values process dxy_lg { label &#39;L_G32g15h_dxy_lg&#39; tag &quot;${spec1}${loc}-${spec2}${loc}_LG${lg}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), val( lg ), file( vcf ), file( geno ), val( kb ) from all_dxy_pairs_ch output: set val( &quot;${spec1}${loc}-${spec2}${loc}-${kb}&quot; ), file( &quot;dxy.${spec1}${loc}-${spec2}${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz&quot; ), val( lg ), val( &quot;${spec1}${loc}&quot; ), val( &quot;${spec2}${loc}&quot; ), val( kb ) into dxy_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 module load intel17.0.4 intelmpi17.0.4 zcat ${geno} | \\ head -n 1 | \\ cut -f 3- | \\ sed &#39;s/\\\\t/\\\\n/g&#39; | \\ awk -v OFS=&#39;\\\\t&#39; &#39;{print \\$1, substr( \\$1, length(\\$1) - 5, 6)}&#39; &gt; pop.txt mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 -s ${kb}000 \\ --popsFile pop.txt \\ -p ${spec1}${loc} -p ${spec2}${loc} \\ -g ${geno} \\ -o dxy.${spec1}${loc}-${spec2}${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } Since the calculation was split across LGs, we now need to collect all LGs of a particular species pair… // git 4.10 // collect all LGs for each species pair dxy_lg_ch .groupTuple() .set{ tubbled_dxy } … and merge the results. // git 4.11 // concatenate all LGs for each species pair process receive_tuple { label &#39;L_20g2h_receive_tuple&#39; publishDir &quot;../../2_analysis/dxy/${kb[0]}0k/&quot;, mode: &#39;copy&#39; tag &quot;${pop1[0]}-${pop2[0]}&quot; input: set val( comp ), file( dxy ), val( lg ), val( pop1 ), val( pop2 ), val( kb ) from tubbled_dxy output: file( &quot;dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv.gz&quot; ) into dxy_output_ch script: &quot;&quot;&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG01.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ head -n 1 &gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; for j in {01..24};do echo &quot;-&gt; LG\\$j&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG\\$j.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ awk &#39;NR&gt;1{print}&#39; &gt;&gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; done gzip dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv &quot;&quot;&quot; } For control, we also create a “random” dXY run, where we take the most diverged species pair and randomize the population assignment of the samples. Therefore, we first pick the most diverged species pair. // git 4.12 // collect a species pair to randomize Channel .from( [[&#39;bel&#39;, &#39;ind&#39;, &#39;may&#39;]] ) .set{ random_run_ch } Then we set the sliding window resolution. // git 4.13 // setup channel content for random channel Channel .from( 1 ) .combine( random_run_ch ) .combine( kb_ch2 ) .filter{ it[4] == 5 } .set{ random_sets_ch } Now, we randomize the population assignment of the samples and calculate differentiation. // git 4.14 // permute the population assignment (the randomization) process randomize_samples { label &#39;L_20g15h_randomize_samples&#39; publishDir &quot;../../2_analysis/fst/${kb}0k/random&quot;, mode: &#39;copy&#39; , pattern: &quot;*_windowed.weir.fst.gz&quot; module &quot;R3.5.2&quot; input: set val( random_set ), val( loc ), val(spec1), val(spec2), val( kb ) from random_sets_ch output: set random_set, file( &quot;random_pop.txt&quot; ) into random_pops_ch file( &quot;*_windowed.weir.fst.gz&quot;) into random_fst_out script: &quot;&quot;&quot; cut -f 2,3 \\$BASE_DIR/metadata/sample_info.txt | \\ grep &quot;${loc}&quot; | \\ grep &quot;${spec1}\\\\|${spec2}&quot; &gt; pop_prep.tsv Rscript --vanilla \\$BASE_DIR/R/randomize_pops.R grep A random_pop.txt | cut -f 1 &gt; pop1.txt grep B random_pop.txt | cut -f 1 &gt; pop2.txt vcftools \\ --gzvcf \\$BASE_DIR/1_genotyping/3_gatk_filtered/filterd_bi-allelic.allBP.vcf.gz \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-step ${kb}0000 \\ --fst-window-size ${kb}0000 \\ --stdout | gzip &gt; ${loc}-aaa-bbb.${kb}0k.random_${spec1}_${spec2}_windowed.weir.fst.gz &quot;&quot;&quot; } We set up another channel to prepare the random dXY… // git 4.15 // pick random pair of interest random_dxy_pairs_ch .filter{ it[0] == &#39;bel&#39; &amp;&amp; it[1] == &#39;ind&#39; &amp;&amp; it[2] == &#39;may&#39; &amp;&amp; it[6] == 5 } .combine( random_pops_ch ) .set{ random_assigned_ch } .. and compute the divergence. // git 4.16 // compute the dxy values process dxy_lg_random { label &#39;L_G32g15h_dxy_lg_random&#39; tag &quot;aaa${loc}-bbb${loc}_LG${lg}&quot; module &quot;R3.5.2&quot; input: set val( loc ), val( spec1 ), val( spec2 ), val( lg ), file( vcf ), file( geno ), val( kb ), val( random_set ), file( pop_file ) from random_assigned_ch output: set val( &quot;aaa${loc}-bbb${loc}-${kb}0kb&quot; ), file( &quot;dxy.aaa${loc}-bbb${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz&quot; ), val( lg ), val( &quot;aaa${loc}&quot; ), val( &quot;bbb${loc}&quot; ), val( kb ) into dxy_random_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 module load intel17.0.4 intelmpi17.0.4 mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 -s ${kb}000 \\ --popsFile ${pop_file} \\ -p A -p B \\ -g ${geno} \\ -o dxy.aaa${loc}-bbb${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } Again, we collect the output of the individual LGs…. // git 4.17 // collect all LGs of random run dxy_random_lg_ch .groupTuple() .set{ tubbled_random_dxy } … and we merge them // git 4.18 // concatinate all LGs of random run process receive_random_tuple { label &#39;L_20g2h_receive_random_tuple&#39; publishDir &quot;../../2_analysis/dxy/random/&quot;, mode: &#39;copy&#39; input: set val( comp ), file( dxy ), val( lg ), val( pop1 ), val( pop2 ), val( kb ) from tubbled_random_dxy output: file( &quot;dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv.gz&quot; ) into dxy_random_output_ch script: &quot;&quot;&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG01.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ head -n 1 &gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; for j in {01..24};do echo &quot;-&gt; LG\\$j&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG\\$j.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ awk &#39;NR&gt;1{print}&#39; &gt;&gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; done gzip dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv &quot;&quot;&quot; } 5.2.3 \\(\\pi\\) Estimating the diversity is quite straight forward: We take the prepared population identifier, the data set and the window resolutions and run VCFtools on each combination. // --------------------------------------------------------------- // The pi part need to be run AFTER the global fst outlier // windows were selected (REMEMBER TO CHECK FST OUTLIER DIRECTORY) // --------------------------------------------------------------- // git 4.19 // calculate pi per species process pi_per_spec { label &#39;L_32g15h_pi&#39; tag &quot;${spec}&quot; publishDir &quot;../../2_analysis/pi/${kb}0k&quot;, mode: &#39;copy&#39; input: set val( spec ), vcfId, file( vcf ), val( kb ) from spec_dxy.combine( vcf_pi_ch ).combine( kb_ch3 ) output: file( &quot;*.${kb}0k.windowed.pi.gz&quot; ) into pi_50k script: &quot;&quot;&quot; module load openssl1.0.2 vcfsamplenames ${vcf[0]} | \\ grep ${spec} &gt; pop.txt vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --window-pi ${kb}0000 \\ --window-pi-step ${kb}000 \\ --out ${spec}.${kb}0k 2&gt; ${spec}.pi.log gzip ${spec}.${kb}0k.windowed.pi tail -n +2 \\$BASE_DIR/2_analysis/summaries/fst_outliers_998.tsv | \\ cut -f 2,3,4 &gt; outlier.bed vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --exclude-bed outlier.bed \\ --window-pi ${kb}0000 \\ --window-pi-step ${kb}000\\ --out ${spec}_no_outlier.${kb}0k 2&gt; ${spec}_${kb}0k_no_outllier.pi.log gzip ${spec}_no_outlier.${kb}0k.windowed.pi &quot;&quot;&quot; } At this step we are done with divergence and diversity. "],
["git-5-analysis-iii-topology-weighting.html", "6 (git 5) Analysis III (topology weighting) 6.1 Summary 6.2 Details of analysis_fasttree_twisst.nf", " 6 (git 5) Analysis III (topology weighting) This pipeline can be executed as follows: cd $BASE_DIR/nf/05_analysis_fasttree_twisst source ../../sh/nextflow_alias.sh nf_run_phylo 6.1 Summary A superseded version of the whole genome phylogeny and the topology weighting are prepared within the nextflow script analysis_fasttree_twisst.nf (located under $BASE_DIR/nf/05_analysis_fasttree_twisst/), which runs on the SNPs only data set. Below is an overview of the steps involved in the process. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 6.2 Details of analysis_fasttree_twisst.nf 6.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into two different streams (one for the phylogeny and one for topology weighting). #!/usr/bin/env nextflow // git 5.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_fasttree_whg; vcf_locations } 6.2.2 Whole genome phylogeny During data exploration, various subsets of the data set were investigated, including the different sampling locations, whole genome vs. non-outlier regions and all samples vs. hamlets only. (Now, most of the options have been muted by commenting these options out.) Here, we set up a channel managing the subset by location. // git 5.2 // setting the sampling location // (the script is set up to run on diffferent subsets of samples) Channel .from( &quot;all&quot; ) //, &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot; ) .set{ locations4_ch } This channel toggles the inclusion of \\(F_{ST}\\) outlier regions. // git 5.3 // setting loci restrictions // (keep vs remove outlier regions) Channel .from( &quot;whg&quot; ) //, &quot;no_outl&quot; ) .set{ whg_modes } This channel toggles the inclusion of the non-hamlet outgroup. // git 5.4 // setting the sampling mode // (the script is set up to run on diffferent subsets of samples) Channel .from( &quot;no_outgroups&quot; ) //, &quot;all&quot; ) .into{ sample_modes } We combine the different selector channels to create all possible combinations of the settings. // git 5.5 // compile the config settings and add data file locations4_ch .combine( vcf_fasttree_whg ) .combine( whg_modes ) .combine( sample_modes ) .set{ vcf_fasttree_whg_location_combo } To prepare the input for the phylogeny, the fine tuned selection is applied to subset the genotypes. // git 5.6 // apply sample filter, subset and convert genotypes process subset_vcf_by_location_whg { label &quot;L_28g5h_subset_vcf_whg&quot; input: set val( loc ), vcfId, file( vcf ), val( mode ), val( sample_mode ) from vcf_fasttree_whg_location_combo output: set val( mode ), val( loc ), val( sample_mode ), file( &quot;${loc}.${mode}.${sample_mode}.whg.geno.gz&quot; ) into snp_geno_tree_whg script: &quot;&quot;&quot; DROP_CHRS=&quot; &quot; # check if samples need to be dropped based on location if [ &quot;${loc}&quot; == &quot;all&quot; ];then vcfsamplenames ${vcf[0]} &gt; prep.pop else vcfsamplenames ${vcf[0]} | \\ grep ${loc} &gt; prep.pop fi # check if outgroups need to be dropped if [ &quot;${sample_mode}&quot; == &quot;all&quot; ];then mv prep.pop ${loc}.pop else cat prep.pop | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop fi # check if diverged LGs need to be dropped if [ &quot;${mode}&quot; == &quot;no_outl&quot; ];then DROP_CHRS=&quot;--not-chr LG04 --not-chr LG07 --not-chr LG08 --not-chr LG09 --not-chr LG12 --not-chr LG17 --not-chr LG23&quot; fi vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ \\$DROP_CHRS \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.${mode}.${sample_mode}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i ${loc}.${mode}.${sample_mode}.vcf.gz | gzip &gt; ${loc}.${mode}.${sample_mode}.whg.geno.gz &quot;&quot;&quot; } The subset is converted to fasta format, creating two whole genome pseudo haplotypes per sample. // git 5.7 // convert genotypes to fasta process fasttree_whg_prep { label &#39;L_190g4h_fasttree_whg_prep&#39; tag &quot;${mode} - ${loc} - ${sample_mode}&quot; input: set val( mode ), val( loc ), val( sample_mode ), file( geno ) from snp_geno_tree_whg output: set val( mode ), val( loc ), val( sample_mode ), file( &quot;all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa&quot; ) into ( fasttree_whg_prep_ch ) script: &quot;&quot;&quot; python \\$SFTWR/genomics_general/genoToSeq.py -g ${geno} \\ -s all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa \\ -f fasta \\ --splitPhased &quot;&quot;&quot; } Then, the phylogeny is reconstructed based on the converted geneotypes. // git 5.8 // create phylogeny process fasttree_whg_run { label &#39;L_300g30h_fasttree_run&#39; tag &quot;${mode} - ${loc} - ${sample_mode}&quot; publishDir &quot;../../2_analysis/fasttree/&quot;, mode: &#39;copy&#39; input: set val( mode ), val( loc ), val( sample_mode ), file( fa ) from fasttree_whg_prep_ch output: file( &quot;${sample_mode}.${loc}.${mode}.SNP.tree&quot; ) into ( fasttree_whg_output ) script: &quot;&quot;&quot; fasttree -nt ${fa} &gt; ${sample_mode}.${loc}.${mode}.SNP.tree &quot;&quot;&quot; } 6.2.3 Topology weighting The complexity of topology weighting increases non-linearely with the number of included populations as the number of possible unrooted topologies skyrockets. The maximum number of possible populations allowed within twisst is eight, so we are running the topology weighting for Belize and Honduras independently (for the three species at Panama only one unrooted topology is possible, so we don’t run twisst here). We start by setting up a channel for the sampling location. // git 5.9 // initialize the locations for topology weighting Channel .from( &quot;bel&quot;, &quot;hon&quot; ) .set{ locations_ch } Then, we attach the genotypes to the location. // git 5.10 locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } The analysis is split by linkage group, so we need to initialize the LGs. // git 5.11 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .set{ lg_twisst } The data is subset to include only the samples of the respective location. // git 5.12 // subset the genotypes by location process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.pop&quot; ) into ( vcf_loc_twisst ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.vcf.gz &quot;&quot;&quot; } While running twisst, we ran into an issues regarding incompatibilities of the used scheduling system used on our computing cluster and the threading within the python scripts used in the preparation of twisst. Unfortunately, this prevented us running the preparation in place (as part of the nextflow script). Instead, we rand the preparation separately on a local computer and clumsily plugged the results into the nextflow process. Below we include the originally intended workflow which we muted so that the script is runnable using the plug-in approach. Still, the the original workflow describes the steps executed locally and conveys the intermediate steps more clearly. Also, on a different computer cluster, the original script should work alright. The next step is to attach the LGs to the genotype subsets. // --------------------------------------------------------------- // Unfortunately the twisst preparation did not work on the cluster // (&#39;in place&#39;), so I had to setup the files locally and then plug // them into this workflow. // Below is the originally intended clean workflow (commented out), // while the plugin version picks up at git 5.19. // --------------------------------------------------------------- /* MUTE: // git 5.13 // add the lg channel to the genotype subset vcf_loc_twisst .combine( lg_twisst ) .set{ vcf_loc_lg_twisst } */ Based on the LGs the genotypes are split. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.14 // subset genotypes by LG process vcf2geno_loc { label &#39;L_20g15h_vcf2geno&#39; input: set val( loc ), file( vcf ), file( pop ), val( lg ) from vcf_loc_lg_twisst output: set val( loc ), val( lg ), file( &quot;${loc}.${lg}.geno.gz&quot; ), file( pop ) into snp_geno_twisst script: &quot;&quot;&quot; vcftools \\ --gzvcf ${vcf} \\ --chr ${lg} \\ --recode \\ --stdout | gzip &gt; intermediate.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i intermediate.vcf.gz | gzip &gt; ${loc}.${lg}.geno.gz &quot;&quot;&quot; } */ /* MUTE: python thread conflict - run locally and feed into ressources/plugin We initialize the window size size (as SNPs) used for the topology weighting… // git 5.15 // initialize SNP window size Channel.from( 50, 200 ).set{ twisst_window_types } */ …and attach them to the genotypes. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.16 // add the SNP window size to the genotype subset snp_geno_twisst.combine( twisst_window_types ).set{ twisst_input_ch } */ To conduct topology weighting, we need some underlying phylogenies, so we run PhyML along the sliding window. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.17 // create the phylogenies along the sliding window process twisst_prep { label &#39;L_G120g40h_prep_twisst&#39; publishDir &quot;../../2_analysis/twisst/positions/${loc}/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ) from twisst_input_ch.filter { it[0] != &#39;pan&#39; } output: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( &quot;*.trees.gz&quot; ), file( &quot;*.data.tsv&quot; ) into twisst_prep_ch script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/phylo/phyml_sliding_windows.py \\ -g ${geno} \\ --windType sites \\ -w ${twisst_w} \\ --prefix ${loc}.${lg}.w${twisst_w}.phyml_bionj \\ --model HKY85 \\ --optimise n \\ --threads 1 &quot;&quot;&quot; } */ The last step then is to run twisst on the prepared phylogenies. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.18 // run the topology weighting on the phylogenies process twisst_run { label &#39;L_G120g40h_run_twisst&#39; publishDir &quot;../../2_analysis/twisst/weights/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( tree ), file( data ) from twisst_prep_ch output: set val( loc ), val( lg ), val( twisst_w ), file( &quot;*.weights.tsv.gz&quot; ), file( &quot;*.data.tsv&quot; ) into ( twisst_output ) script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; ${pop} | \\ sed &#39;s/\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; | \\ cut -f 1,3 | \\ awk &#39;{print \\$1&quot;_A\\\\t&quot;\\$2&quot;\\\\n&quot;\\$1&quot;_B\\\\t&quot;\\$2}&#39; &gt; ${loc}.${lg}.twisst_pop.txt TWISST_POPS=\\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d&#39;,&#39; | sed &#39;s/,/ -g /g; s/^/-g /&#39; ) mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/twisst/twisst.py \\ --method complete \\ -t ${tree} \\ -T 1 \\ \\$TWISST_POPS \\ --groupsFile ${loc}.${lg}.twisst_pop.txt | \\ gzip &gt; ${loc}.${lg}.w${twisst_w}.phyml_bionj.weights.tsv.gz &quot;&quot;&quot; } */ Here, the plug in approach picks up. At this point we have run PhyML locally and deposited the results under $BASE_DIR/ressources/plugin/trees. To restart, we need to emulate the settings needed for the twisst process. // git 5.19 // emulate setting Channel .from(50, 200) .combine( vcf_loc_twisst ) .combine( lg_twisst ) .set{ twisst_modes } Then, we feed the phylogenies from an external directory into twisst. // git 5.20 // run the topology weighting on the phylogenies process twisst_plugin { label &#39;L_G120g40h_twisst_plugin&#39; publishDir &quot;../../2_analysis/twisst/weights/&quot;, mode: &#39;copy&#39; tag &quot;${loc}-${lg}-${mode}&quot; input: set val( mode ), val( loc ), file( vcf ), file( pop ), val( lg ) from twisst_modes output: set val( loc ), val( lg ), file( &quot;*.weights.tsv.gz&quot; ) into ( twisst_output ) script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; ${pop} | \\ sed &#39;s/\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; | \\ cut -f 1,3 | \\ awk &#39;{print \\$1&quot;_A\\\\t&quot;\\$2&quot;\\\\n&quot;\\$1&quot;_B\\\\t&quot;\\$2}&#39; &gt; ${loc}.${lg}.twisst_pop.txt TWISST_POPS=\\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d&#39;,&#39; | sed &#39;s/,/ -g /g; s/^/-g /&#39; ) mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/twisst/twisst.py \\ --method complete \\ -t \\$BASE_DIR/ressources/plugin/trees/${loc}/${loc}.${lg}.w${mode}.phyml_bionj.trees.gz \\ \\$TWISST_POPS \\ --groupsFile ${loc}.${lg}.twisst_pop.txt | \\ gzip &gt; ${loc}.${lg}.w${mode}.phyml_bionj.weights.tsv.gz &quot;&quot;&quot; } At this step we are done with phylogeny and the topology weighting. "],
["git-6-analysis-iv-rho.html", "7 (git 6) Analysis IV (rho) 7.1 Summary 7.2 Details of analysis_recombination.nf", " 7 (git 6) Analysis IV (rho) This pipeline can be executed as follows: cd $BASE_DIR/nf/06_analysis_recombination source ../../sh/nextflow_alias.sh nf_run_recombination 7.1 Summary The population recombination rate is estimated within the nextflow script analysis_recombination.nf (located under $BASE_DIR/nf/06_analysis_recombination/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 7.2 Details of analysis_recombination.nf 7.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // This pipeline includes the recombination anlysis // git 6.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } The estimation of the population recombination rate using FastEPRR happens in three steps. The first step is run independently for all linkage groups, so we set up a channel for the LGs. // git 6.2 // initialize LGs Channel .from( 1..24 ) .map{ it.toString().padLeft(2, &quot;0&quot;) } .set{ lg_ch } To prepare the input, the genotypes are split by LG. // git 6.3 // split genotypes by LG process split_allBP { label &#39;L_20g2h_split_by_lg&#39; tag &quot;LG${lg}&quot; input: set val( lg ), vcfId, file( vcf ) from lg_ch.combine( vcf_ch ) output: set val( lg ), file( &quot;phased_mac2.LG${lg}.vcf.gz&quot; ) into vcf_by_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 vcftools --gzvcf ${vcf[0]} \\ --chr LG${lg} \\ --recode \\ --stdout | bgzip &gt; phased_mac2.LG${lg}.vcf.gz &quot;&quot;&quot; } The prepared data is then fed to the first step of FastEPRR. // git 6.4 // run fasteprr step 1 process fasteprr_s1 { label &#39;L_20g2h_fasteprr_s1&#39; tag &quot;LG${lg}&quot; module &quot;R3.5.2&quot; input: set val( lg ), file( vcf ) from vcf_by_lg_ch output: file( &quot;step1_LG${lg}&quot; ) into step_1_out_ch script: &quot;&quot;&quot; mkdir step1_LG${lg} Rscript --vanilla \\$BASE_DIR/R/fasteprr_step1.R ./${vcf} step1_LG${lg} LG${lg} 50 &quot;&quot;&quot; } Since nextflow manages the results of its processes in a complex file structure, we need to collect all results of step 1 and bundle them before proceeding. // git 6.5 // collect step 1 output process fasteprr_s1_summary { label &#39;L_loc_fasteprr_s1_summmary&#39; input: file( step1 ) from step_1_out_ch.collect() output: file( &quot;step1&quot; ) into ( step1_ch1, step1_ch2 ) script: &quot;&quot;&quot; mkdir step1 cp step1_LG*/* step1/ &quot;&quot;&quot; } The second step of FastEPRR is parallelized over an arbitrary number of sub-processes. Here, we initialize 250 parallel processes and combine the parallelization index with the results from step 1. // git 6.6 // initialize fasteperr subprocesses and attach them to step 1 output Channel .from( 1..250 ) .map{ it.toString().padLeft(3, &quot;0&quot;) } .combine( step1_ch1 ) .set{ step_2_run_ch } Taking this prepared bundle, we now can start the second step of FastEPRR. // git 6.7 // run fasteprr step 2 process fasteprr_s2 { label &#39;L_long_loc_fasteprr_s2&#39; tag &quot;run_${idx}&quot; module &quot;R3.5.2&quot; input: set val( idx ), file( step1 ) from step_2_run_ch output: set val( idx ), file( &quot;step2_run${idx}&quot; ) into ( step_2_indxs, step_2_files ) script: &quot;&quot;&quot; mkdir -p step2_run${idx} Rscript --vanilla \\$BASE_DIR/R/fasteprr_step2.R ${step1} step2_run${idx} ${idx} &quot;&quot;&quot; } We collect both clones of the step 2 results and bundle the results in a single directory. // git 6.8 // collect step 2 output process fasteprr_s2_summary { label &#39;L_loc_fasteprr_s2_summmary&#39; input: val( idx ) from step_2_indxs.map{ it[0] }.collect() file( files ) from step_2_files.map{ it[1] }.collect() output: file( &quot;step2&quot; ) into ( step2_ch ) script: &quot;&quot;&quot; mkdir step2 for k in \\$( echo ${idx} | sed &#39;s/\\\\[//g; s/\\\\]//g; s/,//g&#39;); do cp -r step2_run\\$k/* step2/ done &quot;&quot;&quot; } Then we feed the bundled results into the third step of FastEPRR. // git 6.9 // run fasteprr step 3 process fasteprr_s3 { label &#39;L_32g4h_fasteprr_s3&#39; module &quot;R3.5.2&quot; input: set file( step1 ), file( step2 ) from step1_ch2.combine( step2_ch ) output: file( &quot;step3&quot; ) into step_3_out_ch script: &quot;&quot;&quot; mkdir step3 Rscript --vanilla \\$BASE_DIR/R/fasteprr_step3.R ${step1} ${step2} step3 &quot;&quot;&quot; } To ease the usage of the FastEPRR results downstream, we reformat them and compile a tidy table. // git 6.10 // reformat overall fasteprr output process fasteprr_s3_summary { label &#39;L_loc_fasteprr_s3_summmary&#39; publishDir &quot;../../2_analysis/fasteprr&quot;, mode: &#39;copy&#39; input: file( step3 ) from step_3_out_ch output: file( &quot;step4/fasteprr.all.rho.txt.gz&quot; ) into ( step3_ch ) script: &quot;&quot;&quot; mkdir step4 # ------ rewriting the fasteprr output into tidy format -------- for k in {01..24};do j=&quot;LG&quot;\\$k; echo \\$j; \\$BASE_DIR/sh/fasteprr_trans.sh step3/chr_\\$j \\$j step4/fasteprr.\\$j done # --------- combining all LGs into a single data set ----------- cd step4 head -n 1 fasteprr.LG01.rho.txt &gt; fasteprr.all.rho.txt for k in {01..24}; do echo &quot;LG&quot;\\$k awk &#39;NR&gt;1{print \\$0}&#39; fasteprr.LG\\$k.rho.txt &gt;&gt; fasteprr.all.rho.txt done gzip fasteprr.all.rho.txt cd .. &quot;&quot;&quot; } Finally, we are done with preparing the recombination rate for plotting. "],
["git-7-analysis-v-principal-component-analysis.html", "8 (git 7) Analysis V (Principal Component Analysis) 8.1 Summary 8.2 Details of analysis_pca.nf", " 8 (git 7) Analysis V (Principal Component Analysis) This pipeline can be executed as follows: cd $BASE_DIR/nf/07_analysis_pca source ../../sh/nextflow_alias.sh nf_run_pca 8.1 Summary The PCAs are produced within the nextflow script analysis_pca.nf (located under $BASE_DIR/nf/07_analysis_pca/). It takes the SNPs only data set and runs the PCAs (for all samples and within locations) both for the whole genome and for the highly differentiated regions excluded. Below is an overview of the steps involved in the analysis. (The green dots indicate the input files, red dots depict output that is exported for further use.) 8.2 Details of analysis_pca.nf 8.2.1 Setup The nextflow script starts by setting the channel for data subset options. #!/usr/bin/env nextflow // git 7.1 // prepare subset modes (whole genome vs non-diverged regions) Channel .from( &quot;whg&quot;, &quot;subset_non_diverged&quot;) .set{ subset_type_ch } Also, the file with the genomic coordinates of the outlier regions is loaded. // git 7.2 // load table with differentiation outlier regions Channel .fromPath( &quot;../../2_analysis/summaries/fst_outliers_998.tsv&quot; ) .set{ outlier_tab } Then it combines the genotype file with the subset type and the outlier location file. // git 7.3 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .combine( outlier_tab ) .combine( subset_type_ch ) .set{ vcf_ch } Depending on subset type, the outlier regions are excluded from the genotypes. // git 7.4 // depending on subset mode, subset vcf process subset_vcf_divergence_based { label &quot;L_20g2h_subset_divergence&quot; input: set vcfId, file( vcf ), file( outlier_tab ), val( subset_type ) from vcf_ch output: set file( &quot;${subset_type}.vcf.gz&quot; ), file( &quot;${subset_type}.vcf.gz.tbi&quot; ), val( subset_type ) into ( vcf_locations, vcf_all_samples_pca ) script: &quot;&quot;&quot; if [ &quot;${subset_type}&quot; == &quot;subset_non_diverged&quot; ];then awk -v OFS=&quot;\\\\t&quot; &#39;{print \\$2,\\$3,\\$4}&#39; ${outlier_tab} &gt; diverged_regions.bed SUBSET=&quot;--exclude-bed diverged_regions.bed&quot; else SUBSET=&quot;&quot; fi vcftools --gzvcf ${vcf[0]} \\ \\$SUBSET \\ --recode \\ --stdout | bgzip &gt; ${subset_type}.vcf.gz tabix ${subset_type}.vcf.gz &quot;&quot;&quot; } The locations channel is set… // git 7.5 // prepare location channel for separate pcas Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } …and combined with the genotypes. // git 7.6 // attach genotypes to location channel locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } Then, the genotypes are subset by location. // git 7.7 // subset vcf by location process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), file( vcf ), file( vcfidx ), val( subset_type ) from vcf_location_combo output: set val( loc ), file( &quot;*.vcf.gz&quot; ), file( &quot;*.pop&quot; ), val( subset_type ) into ( vcf_loc_pca ) script: &quot;&quot;&quot; vcfsamplenames ${vcf} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.${subset_type}.pop vcftools --gzvcf ${vcf} \\ --keep ${loc}.${subset_type}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.${subset_type}.vcf.gz &quot;&quot;&quot; } Finally, the PCAs are run within the different locations… // PCA section // ----------- // git 7.8 // run pca by location process pca_location { label &quot;L_20g15h_pca_location&quot; publishDir &quot;../../figures/pca&quot;, mode: &#39;copy&#39; , pattern: &quot;*.pdf&quot; publishDir &quot;../../2_analysis/pca&quot;, mode: &#39;copy&#39; , pattern: &quot;*.gz&quot; input: set val( loc ), file( vcf ), file( pop ), val( subset_type ) from vcf_loc_pca output: set file( &quot;*.prime_pca.pdf&quot; ), file( &quot;*.pca.pdf&quot; ), file( &quot;*.exp_var.txt.gz&quot; ), file( &quot;*.scores.txt.gz&quot; ) into pca_loc_out script: &quot;&quot;&quot; awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; ${loc}.${subset_type}.pop | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; ${loc}.${subset_type}.pop.txt Rscript --vanilla \\$BASE_DIR/R/vcf2pca.R ${vcf} ${loc}.${subset_type}.pop.txt 6 &quot;&quot;&quot; } …and for the entire data set. // git 7.9 // run pca for global data set process pca_all { label &quot;L_20g15h_pca_all&quot; publishDir &quot;../../figures/pca&quot;, mode: &#39;copy&#39; , pattern: &quot;*.pdf&quot; publishDir &quot;../../2_analysis/pca&quot;, mode: &#39;copy&#39; , pattern: &quot;*.txt.gz&quot; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.vcf.gz&quot; input: set file( vcf ), file( vcfidx ), val( subset_type ) from vcf_all_samples_pca output: set file( &quot;*.prime_pca.pdf&quot; ), file( &quot;*.pca.pdf&quot; ), file( &quot;*.exp_var.txt.gz&quot; ), file( &quot;*.scores.txt.gz&quot; ) into pca_all_out file( &quot;hamlets_only.${subset_type}.vcf.gz&quot; ) into vcf_hamlets_only set file( &quot;hamlets_only.${subset_type}.vcf.gz&quot; ), file( &quot;hamlets_only.${subset_type}.pop.txt&quot; ) into vcf_multi_fst script: &quot;&quot;&quot; # complete PCA, all samples ------------ vcfsamplenames ${vcf} | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; all.${subset_type}.pop.txt Rscript --vanilla \\$BASE_DIR/R/vcf2pca.R ${vcf} all.${subset_type}.pop.txt 6 # PCA without outgroups --------------- vcfsamplenames ${vcf} | \\ grep -v &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni\\\\|flo&quot; &gt; outgroup.${subset_type}.pop vcfsamplenames ${vcf} | \\ grep &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni\\\\|flo&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; hamlets_only.${subset_type}.pop.txt vcftools \\ --gzvcf ${vcf} \\ --remove outgroup.${subset_type}.pop \\ --recode \\ --stdout | gzip &gt; hamlets_only.${subset_type}.vcf.gz Rscript --vanilla \\$BASE_DIR/R/vcf2pca.R hamlets_only.${subset_type}.vcf.gz hamlets_only.${subset_type}.pop.txt 6 &quot;&quot;&quot; } "],
["git-8-analysis-vi-demographic-history.html", "9 (git 8) Analysis VI (Demographic History) 9.1 Summary 9.2 Details of analysis_msmc.nf", " 9 (git 8) Analysis VI (Demographic History) This pipeline can be executed as follows: cd $BASE_DIR/nf/08_analysis_msmc source ../../sh/nextflow_alias.sh nf_run_msmc 9.1 Summary The demographic history rate is inferred within the nextflow script analysis_msmc.nf (located under $BASE_DIR/nf/08_analysis_msmc/), which runs on the SNPs only data set. Below is an overview of the steps involved in the inference. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 9.2 Details of analysis_msmc.nf 9.2.1 Data preparation The first part of the scripts includes a large block of preparation work. In this initial block, the data masks are being generated based on the samples coverage statistics combined with the locations of idels and the reference genomes mappability. The whole script is opened by a creating a channel for the linkage groups since the coverage statistics are being created on a linkage group basis. #!/usr/bin/env nextflow // git 8.1 // create channel of linkage groups Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .into{ lg_ch1; lg_ch2; lg_ch3 } Then the phased genotype data is opened (for later use in msmc). // git 8.2 // open phased genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_msmc } To extract the sequencing depth for each individual, the unphased genotypes are opened as well (as this information is lost during phasing). // git 8.3 // open unphased genotype data to extract depth information Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/filterd_bi-allelic.vcf.{gz,gz.tbi}&quot;) .set{ vcf_depth } The outgroups are removed from the data set and the depth is reported for each individual. // git 8.4 // gather depth per individual process gather_depth { label &#39;L_20g2h_split_by_sample&#39; publishDir &quot;../../metadata&quot;, mode: &#39;copy&#39; input: set vcfID, file( vcf ) from vcf_depth output: file( &quot;depth_by_sample.txt&quot; ) into depth_ch script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --depth \\ --stdout &gt; depth_by_sample.txt &quot;&quot;&quot; } The depth information is fed into a channel so that the information is accessible for nextflow. // git 8.5 // create channel out of sequencing depth table depth_ch .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ id:row.INDV, sites:row.N_SITES, depth:row.MEAN_DEPTH] } .map{ [it.id, it.sites, it.depth] } .set { depth_by_sample_ch } Next, a channel is created from all the original .bam files from the mapped sequences (git 1.6). // git 8.6 // create channel from bam files and add sample id Channel .fromPath( &#39;../../1_genotyping/0_dedup_bams/*.bam&#39; ) .map{ file -&gt; def key = file.name.toString().tokenize(&#39;.&#39;).get(0) return tuple(key, file)} .set{ sample_bams } The previously created depth information is attached to the bam channel… // git 8.7 // combine sample bams and sequencing depth sample_bams .join( depth_by_sample_ch ) .set{ sample_bam_and_depth } … and the genotype data and individual linkage groups are added. // git 8.8 // multiply the sample channel by the linkage groups sample_bam_and_depth .combine( vcf_msmc ) .combine( lg_ch1 ) .set{ samples_msmc } Now, the data is split by individual and all the additional information is passed on to the masking (git 8.10) as well as to the production of the the individuals segregating sites (git 8.11). // git 8.9 // split vcf by individual process split_vcf_by_individual { label &#39;L_20g15m_split_by_vcf&#39; input: set val( id ), file( bam ), val( sites ), val( depth ), val( vcf_id ), file( vcf ), val( lg ) from samples_msmc output: set val( id ), val( lg ), file( bam ), val( depth ), file( &quot;phased_mac2.${id}.${lg}.vcf.gz&quot; ) into ( sample_vcf, sample_vcf2 ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx10G&quot; \\ SelectVariants \\ -R \\$REF_GENOME \\ -V ${vcf[0]} \\ -sn ${id} \\ -L ${lg}\\ -O phased_mac2.${id}.${lg}.vcf.gz &quot;&quot;&quot; } The individual coverage statistics are then being queried using the individuals average depth to create the coverage mask for each individual. // git 8.10 // create coverage mask from original mapped sequences process bam_caller { label &#39;L_36g47h_bam_caller&#39; publishDir &quot;../../ressources/coverage_masks&quot;, mode: &#39;copy&#39; , pattern: &quot;*.coverage_mask.bed.gz&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: set val( id ), val( lg ), file( bam ), val( depth ), file( vcf ) from sample_vcf output: set val( id ), val( lg ), file( &quot;*.bam_caller.vcf.gz&quot; ), file( &quot;*.coverage_mask.bed.gz&quot; ) into coverage_by_sample_lg script: &quot;&quot;&quot; module load openssl1.0.2 samtools index ${bam} samtools mpileup -q 25 -Q 20 -C 50 -u -r ${lg} -f \\$REF_GENOME ${bam} | \\ bcftools call -c -V indels | \\ \\$BASE_DIR/py/bamHamletCaller.py ${depth} ${id}.${lg}.coverage_mask.bed.gz | \\ gzip -c &gt; ${id}.${lg}.bam_caller.vcf.gz &quot;&quot;&quot; } For each individual, the segregating sites are created. // git 8.11 // create segsites file process generate_segsites { label &quot;L_20g15m_msmc_generate_segsites&quot; publishDir &quot;../../2_analysis/msmc/segsites&quot;, mode: &#39;copy&#39; , pattern: &quot;*.segsites.vcf.gz&quot; input: set val( id ), val( lg ), file( bam ), val( depth ), file( vcf ) from sample_vcf2 output: set val( id ), val( lg ), file( &quot;*.segsites.vcf.gz&quot; ), file( &quot;*.covered_sites.bed.txt.gz&quot; ) into segsites_by_sample_lg script: &quot;&quot;&quot; zcat ${vcf} | \\ vcfAllSiteParser.py ${id} ${id}.${lg}.covered_sites.bed.txt.gz | \\ gzip -c &gt; ${id}.${lg}.segsites.vcf.gz &quot;&quot;&quot; } 9.2.2 Grouping of individuals At this point, the data masks are prepared and the samples can be assigned to their respective groups for their demographic history and and cross-coalescence rate inference. // git 8.12 // assign samples randomly across MSMC and cross coalescence runs process msmc_sample_grouping { label &quot;L_loc_msmc_grouping&quot; publishDir &quot;../../2_analysis/msmc/setup&quot;, mode: &#39;copy&#39; module &quot;R3.5.2&quot; output: file( &quot;msmc_grouping.txt&quot; ) into msmc_grouping file( &quot;msmc_cc_grouping.txt&quot; ) into cc_grouping script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/sample_assignment_msmc.R \\ \\$BASE_DIR/R/distribute_samples_msmc_and_cc.R \\ \\$BASE_DIR/R/cross_cc.R \\ \\$BASE_DIR/metadata/sample_info.txt \\ msmc &quot;&quot;&quot; } The results of the random assignment are being fed into a channel. // git 8.13 // read grouping into a channel msmc_grouping .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ run:row.msmc_run, spec:row.spec, geo:row.geo, group_nr:row.group_nr, group_size:row.group_size, samples:row.samples ] } .set { msmc_runs } Since this script uses an unorthodox way of making the results of the data preparation available to all following processes by exporting them back to the root folder, the following dummy process is installed to wait for the data preparation to finish before proceeding with the workflow. // git 8.14 // wait for bam_caller and generate_segsites to finish: /*this &#39;.collect&#39; is only meant to wait until the channel is done, files are being redirected via publishDir*/ coverage_by_sample_lg.collect().map{ &quot;coverage done!&quot; }.into{ coverage_done; coverage_cc } segsites_by_sample_lg.collect().map{ &quot;segsites done!&quot; }.into{ segsites_done; segsites_cc } To set up the msmc2 runs, the sample grouping is waiting for the dummy process to finish. // git 8.15 // attach masks to MSMC group assignment lg_ch2 .combine( msmc_runs ) .combine( coverage_done ) .combine( segsites_done ) .map{[it[0], it[1].run, it[1]]} .set{ msmc_grouping_after_segsites } Then, the specific msmc2 input files are compiled from the combined masks of the involved samples (for each linkage group individually). // git 8.16 // generating MSMC input files (4 or 3 inds per species) process generate_multihetsep { label &quot;L_120g40h_msmc_generate_multihetsep&quot; publishDir &quot;../../2_analysis/msmc/input/run_${run}&quot;, mode: &#39;copy&#39; , pattern: &quot;*.multihetsep.txt&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: /* content msmc_gr: val( msmc_run ), val( spec ), val( geo ), val( group_nr ), val( group_size ), val( samples ) */ /*[LG20, [msmc_run:45, spec:uni, geo:pan, group_nr:4, group_size:3, samples:ind1, ind2, ind3], coverage done!, segsites done!]*/ set val( lg ), val( run ), msmc_gr from msmc_grouping_after_segsites output: set val( run ), val( lg ), val( msmc_gr.spec ), val( msmc_gr.geo ), val( msmc_gr.group_size ), file( &quot;msmc_run.*.multihetsep.txt&quot; ) into msmc_input_lg script: &quot;&quot;&quot; COVDIR=&quot;\\$BASE_DIR/ressources/coverage_masks/&quot; SMP=\\$(echo ${msmc_gr.samples} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SEGDIR=&quot;\\$BASE_DIR/2_analysis/msmc/segsites/&quot; SEG=\\$(echo ${msmc_gr.samples} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) generate_multihetsep.py \\ \\$SMP \\ --mask=\\$BASE_DIR/ressources/mappability_masks/${lg}.mapmask.bed.txt.gz \\ --negative_mask=\\$BASE_DIR/ressources/indel_masks/indel_mask.${lg}.bed.gz \\ \\$SEG &gt; msmc_run.${msmc_gr.run}.${msmc_gr.spec}.${msmc_gr.geo}.${lg}.multihetsep.txt &quot;&quot;&quot; } The input files of all linkage group are collected for each sample grouping. // git 8.17 // collect all linkage groups for each run msmc_input_lg .groupTuple() .set {msmc_input} And finally msmc2 is executed to infer the demographic history of the involved samples. // git 8.18 // run msmc process msmc_run { label &quot;L_190g100h_msmc_run&quot; publishDir &quot;../../2_analysis/msmc/output/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.final.txt&quot; publishDir &quot;../../2_analysis/msmc/loops/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.loop.txt&quot; input: set msmc_run, lg , spec, geo, group_size, file( hetsep ) from msmc_input output: file(&quot;*.msmc2.*.txt&quot;) into msmc_output script: &quot;&quot;&quot; NHAP=\\$(echo \\$(seq 0 \\$((${group_size[0]}*2-1))) | sed &#39;s/ /,/g&#39; ) INFILES=\\$( echo ${hetsep} ) msmc2 \\ -m 0.00254966 -t 8 \\ -p 1*2+25*1+1*2+1*3 \\ -o run${msmc_run}.${spec[0]}.${geo[0]}.msmc2 \\ -I \\${NHAP} \\ \\${INFILES} &quot;&quot;&quot; } The process cross-coalescence rate is similar (with git 8.19 being the equivalent of git 8.13). So, again the grouping information in fed into a channel. // git 8.19 // generate MSMC cross coalescence input files (2 inds x 2 species) cc_grouping .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ run:row.run_nr, geo:row.geo, spec_1:row.spec_1, spec_2:row.spec_2, contrast_nr:row.contrast_nr, samples_1:row.samples_1, samples_2:row.samples_2 ] } .set { cc_runs } The groups wait for the data preparation to finish. // git 8.20 // attach masks to cross coalescence group assignment lg_ch3 .combine( cc_runs ) .combine( coverage_cc ) .combine( segsites_cc ) .map{[it[0], it[1].run, it[1]]} .set{ cc_grouping_after_segsites } The msmc2 input files are being compiled based on the involved samples (for each linkage group). // git 8.21 // create multihetsep files (combination off all 4 individuals) process generate_multihetsep_cc { label &quot;L_105g30h_cc_generate_multihetsep&quot; publishDir &quot;../../2_analysis/cross_coalescence/input/run_${run}&quot;, mode: &#39;copy&#39; , pattern: &quot;*.multihetsep.txt&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: /* content cc_gr: val( run_nr ), val( geo ), val( spec_1 ), val( spec_2 ), val( contrast_nr ), val( samples_1 ), val( samples_2 ) */ set val( lg ), val( run ), cc_gr from cc_grouping_after_segsites output: set val( cc_gr.run ), val( lg ), val( cc_gr.spec_1 ), val( cc_gr.spec_2 ), val( cc_gr.geo ), val( cc_gr.contrast_nr ), val( cc_gr.samples_1 ), val( cc_gr.samples_2 ), file( &quot;cc_run.*.multihetsep.txt&quot; ) into cc_input_lg script: &quot;&quot;&quot; COVDIR=&quot;\\$BASE_DIR/ressources/coverage_masks/&quot; SMP1=\\$(echo ${cc_gr.samples_1} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SMP2=\\$(echo ${cc_gr.samples_2} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SEGDIR=&quot;\\$BASE_DIR/2_analysis/msmc/segsites/&quot; SEG1=\\$(echo ${cc_gr.samples_1} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) SEG2=\\$(echo ${cc_gr.samples_2} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) generate_multihetsep.py \\ \\${SMP1} \\ \\${SMP2} \\ --mask=\\$BASE_DIR/ressources/mappability_masks/${lg}.mapmask.bed.txt.gz \\ --negative_mask=\\$BASE_DIR/ressources/indel_masks/indel_mask.${lg}.bed.gz \\ \\${SEG1} \\ \\${SEG2} \\ &gt; cc_run.${run}.${cc_gr.spec_1}-${cc_gr.spec_2}.${cc_gr.contrast_nr}.${cc_gr.geo}.${lg}.multihetsep.txt &quot;&quot;&quot; } The input files of all linkage group are collected for each sample grouping. // git 8.22 // collect all linkage groups for each run cc_input_lg .groupTuple() .set {cc_input} And msmc2 is executed to infer the cross-coalescence rate of the involved samples. // git 8.23 // run cross coalescence process cc_run { label &quot;L_190g10ht24_cc_run&quot; publishDir &quot;../../2_analysis/cross_coalescence/output/&quot;, mode: &#39;copy&#39; tag &quot;${cc_run}-${geo[0]}:${spec1[0]}/${spec2[0]}&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: set cc_run, lg , spec1, spec2, geo, contr_nr, samples_1, samples_2, file( hetsep ) from cc_input output: file(&quot;cc_run.*.final.txt.gz&quot;) into cc_output script: &quot;&quot;&quot; INFILES=\\$( echo ${hetsep} ) POP1=\\$( echo &quot;${samples_1}&quot; | sed &#39;s/\\\\[//g; s/, /,/g; s/\\\\]//g&#39; ) POP2=\\$( echo &quot;${samples_2}&quot; | sed &#39;s/\\\\[//g; s/, /,/g; s/\\\\]//g&#39; ) msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.${spec1[0]}.msmc \\ -I 0,1,2,3 \\ \\${INFILES} msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.${spec2[0]}.msmc \\ -I 4,5,6,7 \\ \\${INFILES} msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.cross.msmc \\ -I 0,1,2,3,4,5,6,7 \\ -P 0,0,0,0,1,1,1,1 \\ \\${INFILES} combineCrossCoal.py \\ cc_run.${cc_run}.cross.msmc.final.txt \\ cc_run.${cc_run}.${spec1[0]}.msmc.final.txt \\ cc_run.${cc_run}.${spec2[0]}.msmc.final.txt | \\ gzip &gt; cc_run.${cc_run}.final.txt.gz &quot;&quot;&quot; } Finally, we are done with the inference of the demographic history. "],
["git-9-analysis-vii-hybridization.html", "10 (git 9) Analysis VII (hybridization) 10.1 Summary 10.2 Details of analysis_hybridization.nf", " 10 (git 9) Analysis VII (hybridization) This pipeline can be executed as follows: cd $BASE_DIR/nf/09_analysis_hybridization source ../../sh/nextflow_alias.sh nf_run_hybrid 10.1 Summary The hybridization classes are assigned within the nextflow script analysis_hybridization.nf (located under $BASE_DIR/nf/09_analysis_hybridization/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 10.2 Details of analysis_hybridization.nf 10.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 9.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_loc1; vcf_loc2; vcf_loc3 } Since we are going to work on the three sampling locations independently, we create channel for the locations. // git 9.2 // initialize location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } Next, we define the species sets sampled at the individual locations. // git 9.3 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } For each location, we create all possible species pairs and then merge the channels of the different locations. // git 9.4 // prepare pairwise new_hybrids // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .combine( vcf_loc1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .combine( vcf_loc2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .combine( vcf_loc3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } We are going to run newhybrids only on a subset of the most diverged SNPs for each species pair. For this we first need to compute the \\(F_{ST}\\) on a SNP level for each species pair. // git 9.5 // comute pairwise fsts for SNP filtering process fst_run { label &#39;L_20g45m_fst_run&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( vcfidx ), file( vcf ), val( spec1 ), val( spec2 ) from all_fst_pairs_ch output: set val( loc ), val( spec1 ), val( spec2 ), file( &quot;${vcf[0]}&quot; ), file( &quot;*.fst.tsv.gz&quot; ), file( &quot;${spec1}${loc}.pop&quot;), file( &quot;${spec2}${loc}.pop&quot;) into fst_SNPS script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | grep ${spec1}${loc} &gt; ${spec1}${loc}.pop vcfsamplenames ${vcf[0]} | grep ${spec2}${loc} &gt; ${spec2}${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --weir-fst-pop ${spec1}${loc}.pop \\ --weir-fst-pop ${spec2}${loc}.pop \\ --stdout | gzip &gt; ${spec1}${loc}-${spec2}${loc}.fst.tsv.gz &quot;&quot;&quot; } For each species pair, the 800 most diverged SNPs for this particular pair are selected. // git 9.6 // select the 800 most differentiated SNPs for each population pair process filter_fst { label &#39;L_8g15m_filter_fst&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( fst ), file( pop1 ), file( pop2 ) from fst_SNPS output: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( pop1 ), file( pop2 ), file( &quot;*SNPs.snps&quot; ) into filter_SNPs script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/filter_snps.R ${fst} 800 ${spec1}${loc}-${spec2}${loc} &quot;&quot;&quot; } This pre-selection is then filtered to guarantee a minimum distance of 5 kb between all SNPs. Of this filtered SNP set, 80 SNPs are randomly sampled and formateted for the newhybrid analysis. // git 9.7 // filter the SNP set by min distance (5kb), than randomly pick 80 SNPs // then reformat newhybrid input process prep_nh_input { label &#39;L_8g15m_prep_nh&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( pop1 ), file( pop2 ), file( snps ) from filter_SNPs output: set val( loc ), val( spec1 ), val( spec2 ), file( &quot;*_individuals.txt&quot; ), file( &quot;*.80SNPs.txt&quot;) into newhybrids_input script: &quot;&quot;&quot; vcftools \\ --gzvcf ${vcf} \\ --keep ${pop1} \\ --keep ${pop2} \\ --thin 5000 \\ --out newHyb.${spec1}${loc}-${spec2}${loc} \\ --positions ${snps} \\ --recode grep &#39;#&#39; newHyb.${spec1}${loc}-${spec2}${loc}.recode.vcf &gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf grep -v &#39;#&#39; newHyb.${spec1}${loc}-${spec2}${loc}.recode.vcf | \\ shuf -n 80 | \\ sort -k 1 -k2 &gt;&gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf grep &#39;#CHROM&#39; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf | \\ cut -f 10- | \\ sed &#39;s/\\\\t/\\\\n/g&#39; &gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs_individuals.txt /usr/bin/java -Xmx1024m -Xms512M \\ -jar \\$SFTWR/PGDSpider/PGDSpider2-cli.jar \\ -inputfile newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf \\ -inputformat VCF \\ -outputfile newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.txt \\ -outputformat NEWHYBRIDS \\ -spid \\$BASE_DIR/ressources/vcf2nh.spid &quot;&quot;&quot; } Using a prepared R sript we then can run newhybrids on the SNP selection. // git 9.8 // Run new hybrids // (copy of nh_input is needed because nh can&#39;t read links) process run_nh { label &#39;L_20g15h4x_run_nh&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; publishDir &quot;../../2_analysis/newhyb/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( spec1 ), val( spec2 ), file( inds ), file( snps ) from newhybrids_input output: set file( &quot;nh_input/NH.Results/newHyb.*/*_individuals.txt&quot; ), file( &quot;nh_input/NH.Results/newHyb.*/*_PofZ.txt&quot; ) into newhybrids_output script: &quot;&quot;&quot; mkdir -p nh_input cp ${snps} nh_input/${snps} cp ${inds} nh_input/${inds} Rscript --vanilla \\$BASE_DIR/R/run_newhybrids.R &quot;&quot;&quot; } Finally, we are done with the hybridization analysis. "],
["git-10-analysis-viii-admixture.html", "11 (git 10) Analysis VIII (admixture) 11.1 Summary 11.2 Details of analysis_admixture.nf", " 11 (git 10) Analysis VIII (admixture) This pipeline can be executed as follows: cd $BASE_DIR/nf/10_analysis_admixture source ../../sh/nextflow_alias.sh nf_run_admixture 11.1 Summary The degree of admixture is estimated within the nextflow script analysis_admixture.nf (located under $BASE_DIR/nf/10_analysis_admixture/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 11.2 Details of analysis_admixture.nf 11.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 10.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } Next, we select the range k values that we we want to explore within the admixture analysis. // git 10.2 // Set different k values for the admixture analysis Channel .from( 2..15 ) .set{ k_ch } Then, we open the \\(F_{ST}\\) outlier regions within which we want to run the admixture analysis. // git 10.3 // load Fst outlier regions Channel .fromPath(&quot;../../ressources/plugin/poptrees/outlier.bed&quot;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ chrom:row.chrom, start:row.start, end:row.end, gid:row.gid ] } .combine( vcf_ch ) .set{ vcf_admx } Then, we subset the genotypes to each outlier regions of interest respectively and reformat them to the plink genotype format. // git 10.4 // subset genotypes to the outlier region and reformat process plink12 { label &#39;L_20g2h_plink12&#39; tag &quot;${grouping.gid}&quot; input: set val( grouping ), val( vcfidx ), file( vcf ) from vcf_admx output: set val( grouping ), file( &quot;hapmap.*.ped&quot; ), file( &quot;hapmap.*.map&quot; ), file( &quot;hapmap.*.nosex&quot; ), file( &quot;pop.txt&quot; ) into admx_plink script: &quot;&quot;&quot; echo -e &quot;CHROM\\\\tSTART\\\\tEND&quot; &gt; outl.bed echo -e &quot;${grouping.chrom}\\\\t${grouping.start}\\\\t${grouping.end}&quot; &gt;&gt; outl.bed vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --bed outl.bed \\ --plink \\ --out admx_plink plink \\ --file admx_plink \\ --recode12 \\ --out hapmap.${grouping.gid} &quot;&quot;&quot; } Then, we combine the reformatted genotypes with the k values… // git 10.5 // combine genoutype subsets with k values admx_prep = k_ch.combine( admx_plink ) … and run admixture. // git 10.6 // run admixture process admixture_all { label &#39;L_20g4h_admixture_all&#39; publishDir &quot;../../2_analysis/admixture/&quot;, mode: &#39;copy&#39; tag &quot;${grouping.gid}.${k}&quot; input: set val( k ), val( grouping ), file( ped ), file( map ), file( nosex ), file( pop ) from admx_prep output: set val( &quot;dummy&quot; ), file( &quot;*.out&quot; ), file( &quot;*.Q&quot; ), file( &quot;*.txt&quot; ) into admx_log script: &quot;&quot;&quot; mv ${pop} pop.${grouping.gid}.${k}.txt admixture --cv ${ped} ${k} | tee log.${grouping.gid}.${k}.out &quot;&quot;&quot; } "],
["git-11-analysis-ix-allele-age.html", "12 (git 11) Analysis IX (Allele Age) 12.1 Summary 12.2 Details of analysis_allele_age.nf", " 12 (git 11) Analysis IX (Allele Age) This pipeline can be executed as follows: cd $BASE_DIR/nf/11_analysis_allele_age source ../../sh/nextflow_alias.sh nf_run_aa 12.1 Summary The allele age is estimated within the nextflow script analysis_allele_age.nf (located under $BASE_DIR/nf/11_analysis_allele_age/) which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 12.2 Details of analysis_allele_age.nf 12.2.1 Setup The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 11.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } As the data is going to be split by linkage group, we create a channel for the individual LGs. // git 11.2 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{&quot;LG&quot; + it} .combine( vcf_ch ) .set{ lg_ch } Several things are happening in the next step: First the genotypes are split by LG and a new info field is added to the vcf file to store information about the ancestral state of each SNP. The second step checks for each SNP if it is invariant across all Serranid outgroup samples (in that case the outgroup allele is considered ancestral). Then, allele frequencies are computed as a fallback clue - if a SNP is variant in the outgroup, the major allele is then set as ancestral allele. Lastly, the ancestral state information is added to the genotype file. // git 11.3 // subset the genotypes by LG // and add ancestral allele annotation process prepare_vcf { label &quot;L_20g2h_prepare_vcf&quot; input: set val( lg ), val( vcfidx ), file( vcf ) from lg_ch output: set val( lg ), file( &quot;${lg}_integer.vcf&quot; ) into ( vcf_prep_ch ) script: &quot;&quot;&quot; # subset by LG and add AC info field vcftools \\ --gzvcf ${vcf[0]} \\ --chr ${lg} \\ --recode \\ --stdout | \\ sed &#39;s/\\\\(##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=&quot;Phased Genotype&quot;&gt;\\\\)/\\\\1\\\\n##INFO=&lt;ID=AC,Number=A,Type=Integer,Description=&quot;Allele count in genotypes, for each ALT allele, in the same order as listed&quot;&gt;/&#39; | \\ bgzip &gt; ${lg}.vcf.gz # determine ancestral state based on invariant sites in outgoup zcat ${lg}.vcf.gz | \\ grep -v &quot;^#&quot; | \\ awk -v OFS=&quot;\\\\t&quot; \\ &#39;BEGIN{print &quot;#CHROM&quot;,&quot;FROM&quot;,&quot;TO&quot;,&quot;AA&quot;} {o1=substr(\\$107, 1, 1); o2=substr(\\$107, 3, 3); o3=substr(\\$167, 1, 1); o4=substr(\\$167, 3, 3); o5=substr(\\$179, 1, 1); o6=substr(\\$179, 3, 3); if (o1 == o2 &amp;&amp; o3 == o4 &amp;&amp; o5 == o6 &amp;&amp; o1 == o3 &amp;&amp; o1 == o5){ aa = \\$(4+o1)} else {aa = &quot;.&quot;}; print \\$1,\\$2,\\$2,aa}&#39; &gt; ${lg}_annotations.bed # determine allele frquencies vcftools \\ --gzvcf ${lg}.vcf.gz \\ --freq \\ --stdout | \\ sed &#39;s/{ALLELE:FREQ}/ALLELE1\\\\tALLELE2/&#39; &gt; ${lg}_allele_counts.tsv # determine ancestral state for variant sites in outgoup based on allele freq Rscript --vanilla \\$BASE_DIR/R/major_allele.R ${lg}_allele_counts.tsv ${lg}_annotations.bed bgzip ${lg}_annotations_maj.bed tabix -s 1 -b 2 -e 3 ${lg}_annotations_maj.bed.gz # add ancestral state annotation zcat ${lg}.vcf.gz | \\ vcf-annotate -a ${lg}_annotations_maj.bed.gz \\ -d key=INFO,ID=AA,Number=1,Type=String,Description=&#39;Ancestral Allele&#39; \\ -c CHROM,FROM,TO,INFO/AA | \\ sed &#39;s/LG//g&#39; \\ &gt; ${lg}_integer.vcf &quot;&quot;&quot; } Based on the ancestral state information, the genotypes are recoded such that the ancestral allele is set as the new reference allele of the vcf file. // git 11.4 // re-write ancestral state in vcf process set_ancestral_states { label &#39;L_2g15m_ancestral_states&#39; publishDir &quot;../../1_genotyping/5_ancestral_allele&quot;, mode: &#39;copy&#39; input: set val( lg ), file( vcf ) from ( vcf_prep_ch ) output: set val( lg ), file( &quot;${lg}_aa.vcf.gz&quot; ) into ( vcf_aa_ch ) script: &quot;&quot;&quot; java -jar \\$SFTWR/jvarkit/dist/vcffilterjdk.jar \\ -f \\$BASE_DIR/js/script.js ${vcf} | \\ bgzip &gt; ${lg}_aa.vcf.gz &quot;&quot;&quot; } After this, the outgroup samples are removed from the data and the remaining data set is filtered to remove sites that are invariant within the hamlets. // git 11.5 // filter vcf to remove invariant sites in hamlets process create_positions { label &#39;L_20g2h_create_positions&#39; publishDir &quot;../../2_analysis/sliding_phylo/&quot;, mode: &#39;copy&#39; input: set val( lg ), file( vcf ) from ( vcf_aa_ch ) output: set val( lg ), file( &quot;${lg}_aa_h_variant.vcf.gz&quot; ), file( &quot;${lg}_positions.txt&quot; ) into ( positions_ch ) script: &quot;&quot;&quot; echo -e &quot;20478tabhon\\\\n28393torpan\\\\ns_tort_3torpan&quot; &gt; outgr.pop # keeping only sites that are variant within hamlets vcftools \\ --gzvcf ${vcf} \\ --remove outgr.pop \\ --recode \\ --stdout | \\ vcftools \\ --gzvcf - \\ --mac 1 \\ --recode \\ --stdout | \\ bgzip &gt; ${lg}_aa_no_outgroup.vcf.gz zcat ${lg}_aa_no_outgroup.vcf.gz | \\ grep -v &quot;^#&quot; | \\ cut -f 1,2 | \\ head -n -1 &gt; ${lg}_positions_prep.txt vcftools \\ --gzvcf ${vcf} \\ --positions ${lg}_positions_prep.txt \\ --recode \\ --stdout | \\ bgzip &gt; ${lg}_aa_h_variant.vcf.gz cut -f 2 ${lg}_positions_prep.txt &gt; ${lg}_positions.txt &quot;&quot;&quot; } Since a single GEVA run can take quite some time, the data is split further into chunks of 25k SNPs // git 11.5 // prepare the age estimation by splitting the vcf // (all in one takes too long...) process pre_split { label &#39;L_2g2h_pre_split&#39; publishDir &quot;../../2_analysis/geva/&quot;, mode: &#39;copy&#39; input: set val( lg ), file( vcf ), file( pos ) from ( positions_ch ) output: set val( lg ), file( &quot;pre_positions/pre_*&quot; ), file( &quot;*.bin&quot; ), file( &quot;*.marker.txt&quot; ), file( &quot;*.sample.txt&quot; ) into ( geva_setup_ch ) set val( lg ), file( &quot;inner_pos.txt&quot; ), file( vcf ) into ( ccf_vcf_ch ) script: &quot;&quot;&quot; mkdir -p pre_positions head -n -1 ${pos} | \\ tail -n +2 &gt; inner_pos.txt split inner_pos.txt -a 4 -l 25000 -d pre_positions/pre_ r=\\$(awk -v k=${lg} &#39;\\$1 == k {print \\$4}&#39; \\$BASE_DIR/ressources/avg_rho_by_LG.tsv) geva_v1beta \\ --vcf ${vcf} --rec \\$r --out ${lg} &quot;&quot;&quot; } Then GEVA is run on those genotype subsets to actually estimate the allele ages. // git 11.6 // run geva on vcf subsets process run_geva { label &#39;L_30g15h6x_run_geva&#39; input: set val( lg ), file( pos ), file( bin ), file( marker ), file( sample ) from geva_setup_ch.transpose() output: set val( lg ), file( &quot;*.sites.txt.gz&quot; ), file( &quot;*.pairs.txt.gz&quot; ) into ( output_split_ch ) script: &quot;&quot;&quot; pref=\\$(echo &quot;${pos}&quot; | sed &#39;s=^.*/A==; s=pre_positions/pre_==&#39;) mkdir -p sub_positions sub_results split ${pos} -a 4 -l 250 -d sub_positions/sub_pos_\\${pref}_ r=\\$(awk -v k=${lg} &#39;\\$1 == k {print \\$4}&#39; \\$BASE_DIR/ressources/avg_rho_by_LG.tsv) for sp in \\$(ls sub_positions/sub_pos_\\${pref}_*); do run_id=\\$(echo \\$sp | sed &quot;s=sub_positions/sub_pos_\\${pref}_==&quot;) geva_v1beta \\ -t 6 \\ -i ${bin} \\ -o sub_results/${lg}_\\${pref}_\\${run_id}\\ --positions \\$sp \\ --Ne 30000 \\ --mut 3.7e-08 \\ --hmm \\$SFTWR/geva/hmm/hmm_initial_probs.txt \\$SFTWR/geva/hmm/hmm_emission_probs.txt tail -n +2 sub_results/${lg}_\\${pref}_\\${run_id}.sites.txt &gt;&gt; ${lg}_\\${pref}.sites.txt tail -n +2 sub_results/${lg}_\\${pref}_\\${run_id}.pairs.txt &gt;&gt; ${lg}_\\${pref}.pairs.txt done gzip ${lg}_\\${pref}.sites.txt gzip ${lg}_\\${pref}.pairs.txt &quot;&quot;&quot; } And finally, the results of the separate chunks are gathered and compiled into a single output file. // git 11.7 // collect results by lg process collect_by_lg { label &#39;L_2g2h_collect&#39; publishDir &quot;../../2_analysis/geva/&quot;, mode: &#39;copy&#39; input: set val( lg ), file( sites ), file( pairs ) from output_split_ch.groupTuple() output: set val( lg ), file( &quot;*.sites.txt.gz&quot; ), file( &quot;*.pairs.txt.gz&quot; ) into ( output_lg_ch ) script: &quot;&quot;&quot; echo &quot;MarkerID Clock Filtered N_Concordant N_Discordant PostMean PostMode PostMedian&quot; &gt; ${lg}.sites.txt echo &quot;MarkerID Clock SampleID0 Chr0 SampleID1 Chr1 Shared Pass SegmentLHS SegmentRHS Shape Rate&quot; &gt; ${lg}.pairs.txt zcat ${sites} &gt;&gt; ${lg}.sites.txt zcat ${pairs} &gt;&gt; ${lg}.pairs.txt gzip ${lg}.sites.txt gzip ${lg}.pairs.txt &quot;&quot;&quot; } "],
["git-12-analysis-x-fst-permutation-test.html", "13 (git 12) Analysis X (FST Permutation Test) 13.1 Summary 13.2 Details of analysis_fst_sign.nf", " 13 (git 12) Analysis X (FST Permutation Test) This pipeline can be executed as follows: cd $BASE_DIR/nf/12_analysis_fst_signif source ../../sh/nextflow_alias.sh nf_run_fstsig 13.1 Summary \\(F_{ST}\\) significance is assesed within the nextflow script analysis_fst_sign.nf (located under $BASE_DIR/nf/12_analysis_fst_signif/). Below is an overview of the steps involved in the analysis. (The green dots indicates the input files, red dots depict output that is exported for further use.) 13.2 Details of analysis_fst_sign.nf 13.2.1 Setup The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 12.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_locations; vcf_adapt } As population pairs of different locations are going to be tested independently, we prepare a location channel. // git 12.2 // prepare location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } The permutation test is going to be run once for the full genotype set and once for a subset excluding the highly differentiated regions of the genome. For this, we create a channel to toggle between the subset types. // git 12.3 // prepare subset modes (whole genome vs non-diverged regions) Channel .from( &quot;whg&quot;, &quot;subset_non_diverged&quot;) .into{ subset_type_ch; subset_type_ch2 } We also load a reference table with the genomic coordinates of the highly differentiated regions. // git 12.4 // load table with differentiation outlier regions Channel .fromPath( &quot;../../2_analysis/summaries/fst_outliers_998.tsv&quot; ) .into{ outlier_tab; outlier_tab2 } Then we combine the information about the focal location with the genotype data and the outlier file. // git 12.5 // attach genotypes to location locations_ch .combine( vcf_locations ) .combine( outlier_tab ) .set{ vcf_location_combo } At this point, the genotypes are subset by location. // git 12.6 // subset vcf by location process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ), file( outlier_tab ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.vcf.gz.tbi&quot; ), file( &quot;${loc}.pop&quot; ), file( outlier_tab ) into ( vcf_loc_pair1, vcf_loc_pair2, vcf_loc_pair3 ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | bgzip &gt; ${loc}.vcf.gz tabix ${loc}.vcf.gz &quot;&quot;&quot; } Now, for each location, we respectively list the specific set… // git 12.7 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } …and create all possible species pairs. // git 12.8 // prepare pairwise fsts // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ // ------------------------------ /* channel content after joinig: set [0:val(loc), 1:file(vcf), 2:file( vcfidx ), 3:file(pop), 4:file( outlier_tab ), 5:val(spec1), 6:val(spec2)]*/ // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .join( vcf_loc_pair1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[5] &lt; it[7] } .map{ it[0,1,2,3,4,6,8]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .join( vcf_loc_pair2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[5] &lt; it[7] } .map{ it[0,1,2,3,4,6,8]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .join( vcf_loc_pair3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[5] &lt; it[7] } .map{ it[0,1,2,3,4,6,8]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } Before the actual permutations, a little preparation is necessary: first, the differentiated regions are excluded from the data if needed then, population assignment files are created (pop1.txt and pop2.txt) next, differentiation for the actual population assignment are computed the resulting genome wide FST is used to start populating a results table *_random_fst_a00.tsv finally files that are not needed anymore are cleaned up and the list of involved samples is exported // git 12.9 // run fst on actual populations process fst_run { label &#39;L_32g1h_fst_run&#39; input: set val( loc ), file( vcf ), file( vcfidx ), file( pop ), file( outlier_tab ), val( spec1 ), val( spec2 ), val( subset_type ) from all_fst_pairs_ch.combine( subset_type_ch ) output: set val( &quot;${spec1}${loc}-${spec2}${loc}_${subset_type}&quot; ), file( &quot;*_random_fst_a00.tsv&quot; ) into rand_header_ch set val( &quot;${spec1}${loc}-${spec2}${loc}_${subset_type}&quot; ), val( loc ), val( spec1 ), val( spec2 ), file( &quot;${loc}.${subset_type}.vcf.gz&quot; ), file( &quot;col1.pop&quot; ), file( &quot;prep.pop&quot; ) into rand_body_ch script: &quot;&quot;&quot; if [ &quot;${subset_type}&quot; == &quot;subset_non_diverged&quot; ];then awk -v OFS=&quot;\\\\t&quot; &#39;{print \\$2,\\$3,\\$4}&#39; ${outlier_tab} &gt; diverged_regions.bed SUBSET=&quot;--exclude-bed diverged_regions.bed&quot; else SUBSET=&quot;&quot; fi vcftools --gzvcf ${vcf[0]} \\ \\$SUBSET \\ --recode \\ --stdout | bgzip &gt; ${loc}.${subset_type}.vcf.gz tabix ${loc}.${subset_type}.vcf.gz echo -e &quot;0000\\treal_pop&quot; &gt; idx.txt vcfsamplenames ${loc}.${subset_type}.vcf.gz | \\ awk &#39;{print \\$1&quot;\\\\t&quot;substr(\\$1, length(\\$1)-5, length(\\$1))}&#39; &gt; prep.pop grep ${spec1} ${pop} &gt; pop1.txt grep ${spec2} ${pop} &gt; pop2.txt vcftools --gzvcf ${loc}.${subset_type}.vcf.gz \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --stdout 2&gt; fst.log 1&gt; tmp.txt grep &quot;^Weir&quot; fst.log | sed &#39;s/.* //&#39; | paste - - &gt; fst.tsv echo -e &quot;idx\\\\ttype\\\\tmean_fst\\\\tweighted_fst&quot; &gt; ${spec1}${loc}-${spec2}${loc}_${subset_type}_random_fst_a00.tsv paste idx.txt fst.tsv &gt;&gt; ${spec1}${loc}-${spec2}${loc}_${subset_type}_random_fst_a00.tsv rm fst.tsv fst.log pop1.txt pop2.txt tmp.txt idx.txt awk &#39;{print \\$1}&#39; prep.pop &gt; col1.pop &quot;&quot;&quot; } We are going to perform 10,000 permutations for each population pair. Theses are going to be split over 100 batches of 100 permutations each. For this we now create a channel with the indices for those batches ranging from 00 - 99. // git 12.10 // create indexes for permutation itteration Channel .from( (&#39;0&#39;..&#39;9&#39;)) .into{ singles_ch; tens_ch } singles_ch .combine(tens_ch) .map{ it[0]+it[1] } .toSortedList() .flatten() .into{ sub_pre_ch; sub_pre_ch2 } Then within each batch, 100 permutations of the population assignments are conducted, the genome wide FST is computed and the results are exported. // git 12.11 // for each itteration run fst on 100 // permutations of population assignment process random_bodies { label &#39;L_32g6h_fst_run&#39; input: set val( run ), val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( col1 ), file( prepop ), val( pre ) from rand_body_ch.combine(sub_pre_ch) output: set val( run ), file(&quot;*_random_fst_b${pre}.tsv&quot;) into rand_body_out_ch script: &quot;&quot;&quot; for k in {00..99}; do echo &quot;Iteration_&quot;\\$k echo -e &quot;${pre}\\$k\\trandom&quot; &gt; idx.txt awk &#39;{print \\$2}&#39; ${prepop} | shuf &gt; col2.pop # premutation happens here paste ${col1} col2.pop &gt; rand.pop grep &quot;${spec1}${loc}\\$&quot; rand.pop &gt; r_pop1.pop grep &quot;${spec2}${loc}\\$&quot; rand.pop &gt; r_pop2.pop vcftools --gzvcf ${vcf} \\ --weir-fst-pop r_pop1.pop \\ --weir-fst-pop r_pop2.pop \\ --stdout 2&gt; fst.log 1&gt; tmp.txt grep &quot;^Weir&quot; fst.log | sed &#39;s/.* //&#39; | paste - - &gt; fst.tsv paste idx.txt fst.tsv &gt;&gt; ${run}_random_fst_b${pre}.tsv rm fst.tsv fst.log rand.pop col2.pop r_pop1.pop r_pop2.pop tmp.txt done &quot;&quot;&quot; } Finally, the results of all batches are collected and a single output file is compiled per species pair. // git 12.12 // collect all itterations and compile // output for each population pair process compile_random_results { label &#39;L_20g2h_compile_rand&#39; publishDir &quot;../../2_analysis/fst_signif/random&quot;, mode: &#39;copy&#39; input: set val( run ), file( body ), file( head ) from rand_body_out_ch.groupTuple().join(rand_header_ch, remainder: true) output: file(&quot;${run}_random_fst.tsv.gz&quot;) into random_lists_result script: &quot;&quot;&quot; cat ${head} &gt; ${run}_random_fst.tsv cat ${body} &gt;&gt; ${run}_random_fst.tsv gzip ${run}_random_fst.tsv &quot;&quot;&quot; } The same general approach is used for the permutation test for the allopatric populations of the H. nigricans, H. puella and H. unicolor. First, a channel for the three species is created. // ----------------------------------------- // repeat the same procedure for adaptation // (permuting location within species) // git 12.13 // prepare species channel Channel .from( &quot;nig&quot;, &quot;pue&quot;, &quot;uni&quot;) .set{ species_ch } Then two instances of a location channel are created… // git 12.14 // define location set Channel.from( [[1, &quot;bel&quot;], [2, &quot;hon&quot;], [3, &quot;pan&quot;]]).into{ locations_ch_1;locations_ch_2 } …to produce every possible location combination. // git 12.15 // create location pairs locations_ch_1 .combine(locations_ch_2) .filter{ it[0] &lt; it[2] } .map{ it[1,3]} .combine( species_ch ) .combine( vcf_adapt ) .combine( outlier_tab2 ) .combine( subset_type_ch2 ) .set{ vcf_location_combo_adapt } Then, as a preparation of the permutation, genome wide FST for the actual sample configuration is computed (s. git 12.9). // git 12.16 // collapsed analog to git 12.6 &amp; 9 // subset vcf by species and // run fst on actual populations process fst_run_adapt { label &#39;L_32g1h_fst_run&#39; input: set val( loc1 ), val( loc2 ), val( spec ), val( vcf_indx) , file( vcf ), file( outlier_tab ), val( subset_type ) from vcf_location_combo_adapt output: set val( &quot;${spec}${loc1}-${spec}${loc2}_${subset_type}&quot; ), file( &quot;*_random_fst_a00.tsv&quot; ) into rand_header_adapt_ch set val( &quot;${spec}${loc1}-${spec}${loc2}_${subset_type}&quot; ), val( spec ), val( loc1 ), val( loc2 ), file( &quot;${spec}.${subset_type}.vcf.gz&quot; ), file( &quot;col1.pop&quot; ), file( &quot;prep.pop&quot; ) into rand_body_adapt_ch script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${spec} &gt; ${spec}.pop if [ &quot;${subset_type}&quot; == &quot;subset_non_diverged&quot; ];then awk -v OFS=&quot;\\\\t&quot; &#39;{print \\$2,\\$3,\\$4}&#39; ${outlier_tab} &gt; diverged_regions.bed SUBSET=&quot;--exclude-bed diverged_regions.bed&quot; else SUBSET=&quot;&quot; fi vcftools --gzvcf ${vcf[0]} \\ \\$SUBSET \\ --keep ${spec}.pop \\ --mac 3 \\ --recode \\ --stdout | bgzip &gt; ${spec}.${subset_type}.vcf.gz tabix ${spec}.${subset_type}.vcf.gz echo -e &quot;0000\\treal_pop&quot; &gt; idx.txt vcfsamplenames ${spec}.${subset_type}.vcf.gz | \\ awk &#39;{print \\$1&quot;\\\\t&quot;substr(\\$1, length(\\$1)-5, length(\\$1))}&#39; &gt; prep.pop grep ${loc1} ${spec}.pop &gt; pop1.txt grep ${loc2} ${spec}.pop &gt; pop2.txt vcftools --gzvcf ${spec}.${subset_type}.vcf.gz \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --stdout 2&gt; fst.log 1&gt; tmp.txt grep &quot;^Weir&quot; fst.log | sed &#39;s/.* //&#39; | paste - - &gt; fst.tsv echo -e &quot;idx\\\\ttype\\\\tmean_fst\\\\tweighted_fst&quot; &gt; ${spec}${loc1}-${spec}${loc2}_${subset_type}_random_fst_a00.tsv paste idx.txt fst.tsv &gt;&gt; ${spec}${loc1}-${spec}${loc2}_${subset_type}_random_fst_a00.tsv rm fst.tsv fst.log pop1.txt pop2.txt tmp.txt idx.txt awk &#39;{print \\$1}&#39; prep.pop &gt; col1.pop &quot;&quot;&quot; } Now, 100 batches of 100 permutations each are run for every location pair (s. git 12.11). // git 12.17 // for each itteration run fst on 100 // permutations of location assignment process random_bodies_adapt { label &#39;L_32g6h_fst_run&#39; input: set val( run ), val( spec ), val( loc1 ), val( loc2 ), file( vcf ), file( col1 ), file( prepop ), val( pre ) from rand_body_adapt_ch.combine(sub_pre_ch2) output: set val( run ), file(&quot;*_random_fst_b${pre}.tsv&quot;) into rand_body_out_adapt_ch script: &quot;&quot;&quot; for k in {00..99}; do echo &quot;Iteration_&quot;\\$k echo -e &quot;${pre}\\$k\\trandom&quot; &gt; idx.txt awk &#39;{print \\$2}&#39; ${prepop} | shuf &gt; col2.pop # premutation happens here paste ${col1} col2.pop &gt; rand.pop grep &quot;${spec}${loc1}\\$&quot; rand.pop &gt; r_pop1.pop grep &quot;${spec}${loc2}\\$&quot; rand.pop &gt; r_pop2.pop vcftools --gzvcf ${vcf} \\ --weir-fst-pop r_pop1.pop \\ --weir-fst-pop r_pop2.pop \\ --stdout 2&gt; fst.log 1&gt; tmp.txt grep &quot;^Weir&quot; fst.log | sed &#39;s/.* //&#39; | paste - - &gt; fst.tsv paste idx.txt fst.tsv &gt;&gt; ${run}_random_fst_b${pre}.tsv rm fst.tsv fst.log rand.pop col2.pop r_pop1.pop r_pop2.pop tmp.txt done &quot;&quot;&quot; } Finally, the results are collected and a single output file is compiled. // git 12.18 // collect all itterations and compile // output for each location pair process compile_random_results_adapt { label &#39;L_20g2h_compile_rand&#39; publishDir &quot;../../2_analysis/fst_signif/random/adapt&quot;, mode: &#39;copy&#39; input: set val( run ), file( body ), file( head ) from rand_body_out_adapt_ch.groupTuple().join(rand_header_adapt_ch, remainder: true) output: file(&quot;${run}_random_fst.tsv.gz&quot;) into random_lists_adapt_result script: &quot;&quot;&quot; cat ${head} &gt; ${run}_random_fst.tsv cat ${body} &gt;&gt; ${run}_random_fst.tsv gzip ${run}_random_fst.tsv &quot;&quot;&quot; } "],
["git-13-analysis-xi-whole-genome-phylogenies.html", "14 (git 13) Analysis XI (Whole Genome Phylogenies) 14.1 Summary 14.2 Details of analysis_phylo_whg.nf", " 14 (git 13) Analysis XI (Whole Genome Phylogenies) This pipeline can be executed as follows: cd $BASE_DIR/nf/13_analysis_phylo_whg nextflow run analysis_phylo_whg.nf 14.1 Summary The whole genome phylogenies can be reconstructed within the nextflow script analysis_phylo_whg.nf (located under $BASE_DIR/nf/analysis_phylo_whg/). 14.2 Details of analysis_phylo_whg.nf This part of the analysis was actually manged manually and not via nextflow. We still report the analysis as a .nf script as we believe this is a cleaner and more concise report of the conducted analysis. 14.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into two different streams. #!/usr/bin/env nextflow // ----------------------- DISCLAIMER ---------------------- // this pipeline was not actually run using nexflow, // but managed manually // --------------------------------------------------------- // Hamlet phylogeny // ---------------- // git 13.1 // open the SNP data set Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_hypo_whg_ch; vcf_serr_whg_ch } Next, also a file containing the sample IDs excluding the samples identified as hybrids in git 9 is loaded. // RAxML analysis, Serranus-rooted // ------------------------------- // git 13.2 // open the sample-list (excluding hybrid samples) Channel .fromPath(&quot;../../ressources/samples_hybrids.txt&quot;) .set{ hybrids_file } As a preparation for running raxml, the genotype file is subset to exclude the hybrids. Then, heterozygous sites are masked and the data is filtered for minimal allele count and physical distance thresholds. Finally, the genotypes a indirectly converted to fasta format. // git 13.3 // subset data and convert to fasta for raxml process serr_whg_genotypes { input: set vcfId, file( vcf ), file( hybrids ) from vcf_serr_whg_ch.combine( hybrids_file ) output: file( &quot;hyS_n_0.33_mac4_5kb.fas&quot; ) into raxml_serr_genotypes_ch script: &quot;&quot;&quot; # Remove hybrids from genotype data (SNPs only) vcftools \\ --gzvcf ${vcf[0]} \\ --remove ${hybrids} \\ --recode \\ --stdout | \\ gzip &gt; hyS.vcf.gz # Mask heterozygous genotypes as unknown zcat &lt; hyS.vcf.gz | \\ sed -e s/&quot;1|0&quot;/&quot;.|.&quot;/g -e s/&quot;0|1&quot;/&quot;.|.&quot;/g | \\ gzip &gt; hyS_n.vcf.gz # Apply missingness, allele count and distance filters vcftools \\ --gzvcf hyS_n.vcf.gz \\ --max-missing 0.33 \\ --mac 4 \\ --thin 5000 \\ --recode \\ --out hyS_n_0.33_mac4_5kb # Convert to fasta format (Perl script available at https://github.com/JinfengChen/vcf-tab-to-fasta) wget https://raw.githubusercontent.com/JinfengChen/vcf-tab-to-fasta/master/vcf_tab_to_fasta_alignment.pl vcf-to-tab &lt; hyS_n_0.33_mac4_5kb.vcf &gt; hyS_n_0.33_mac4_5kb.tab perl ~/apps/vcf-tab-to-fasta/vcf_tab_to_fasta_alignment.pl -i hyS_n_0.33_mac4_5kb.tab &gt; hyS_n_0.33_mac4_5kb.fas &quot;&quot;&quot; } Then, raxml can be run on the fasta formated genotypes. // git 13.4 // run raxml (Serranus-rooted) process serr_whg_raxml { publishDir &quot;../../2_analysis/raxml/&quot;, mode: &#39;copy&#39; input: file( fas ) from raxml_serr_genotypes_ch output: file( &quot;hyS_n_0.33_mac4_5kb.raxml.support&quot; ) into raxml_serr_whg_ch script: &quot;&quot;&quot; # Reconstruct phylogeny # Note: number of invariant sites for Felsenstein correction was calculated as number of # variant sites in alignment (109,660) / genome-wide proportion of variant sites # (0.05) * genome-wide proportion of invariant sites (0.95) raxml-NG --all \\ --msa hyS_n_0.33_mac4_5kb.fas \\ --model GTR+G+ASC_FELS{2083540} \\ --tree pars{20},rand{20} \\ --bs-trees 100 \\ --threads 24 \\ --worker 4 \\ --seed 123 \\ --prefix hyS_n_0.33_mac4_5kb &quot;&quot;&quot; } The same general aproach is used for the phylogeny excluding the Serranus outgroup samples. For this, a different sample list (also excluding the outgroup samples) is loaded. // RAxML analysis, floridae-rooted // ------------------------------- // git 13.5 // open the sample-list (excluding hybrid and Serranus samples) Channel .fromPath(&quot;../../ressources/samples_155.txt&quot;) .set{ hamlet_file } Like in git 13.3, the genotypes are subset and converted to fasta format. // git 13.6 // subset data and convert to fasta for raxml process hypo_whg_genotypes { input: set vcfId, file( vcf ), file( hamlets ) from vcf_hypo_whg_ch.combine(hamlet_file) output: file( &quot;hyp155_n_0.33_mac4_5kb.fas&quot; ) into raxml_hypo_genotypes_ch script: &quot;&quot;&quot; # Remove hybrid and Serranus samples from genotype data (SNPs only) vcftools \\ --gzvcf ${vcf[0]} \\ --remove ${hamlets} \\ --recode \\ --stdout | \\ gzip &gt; hyp155.vcf.gz # Mask heterozygous genotypes as unknown zcat &lt; hyp155.vcf.gz | \\ sed -e s/&quot;1|0&quot;/&quot;.|.&quot;/g -e s/&quot;0|1&quot;/&quot;.|.&quot;/g | \\ gzip &gt; hyp155_n.vcf.gz # Apply missingness, allele count and distance filters vcftools \\ --gzvcf hyp155_n.vcf.gz \\ --max-missing 0.33 \\ --mac 4 \\ --thin 5000 \\ --recode \\ --out hyp155_n_0.33_mac4_5kb # Convert to fasta format (Perl script available at https://github.com/JinfengChen/vcf-tab-to-fasta) wget https://raw.githubusercontent.com/JinfengChen/vcf-tab-to-fasta/master/vcf_tab_to_fasta_alignment.pl vcf-to-tab &lt; hyp155_n_0.33_mac4_5kb.vcf &gt; hyp155_n_0.33_mac4_5kb.tab perl ./vcf_tab_to_fasta_alignment.pl -i hyp155_n_0.33_mac4_5kb.tab &gt; hyp155_n_0.33_mac4_5kb.fas &quot;&quot;&quot; } Finally, again raxml is run (equivalent to git 13.4). // git 13.7 // run raxml (floridae-rooted) process hypo_whg_raxml { publishDir &quot;../../2_analysis/raxml/&quot;, mode: &#39;copy&#39; input: file( fas ) from raxml_hypo_genotypes_ch output: file( &quot;hyp155_n_0.33_mac4_5kb.raxml.support&quot; ) into raxml_hypo_whg_ch script: &quot;&quot;&quot; # Infer phylogeny # Note: number of invariant sites for Felsenstein correction was calculated as number of # variant sites in alignment (105,043) / genome-wide proportion of variant sites # (0.05) * genome-wide proportion of invariant sites (0.95) raxml-NG --all \\ --msa ${fas} \\ --model GTR+G+ASC_FELS{1995817} \\ --tree pars{20},rand{20} \\ --bs-trees 100 \\ --threads 24 \\ --worker 8 \\ --seed 123 \\ --prefix hyp155_n_0.33_mac4_5kb &quot;&quot;&quot; } "],
["git-14-analysis-xii-outlier-region-phylogenies.html", "15 (git 14) Analysis XII (Outlier Region Phylogenies) 15.1 Summary 15.2 Details of analysis_phylo_regions.nf", " 15 (git 14) Analysis XII (Outlier Region Phylogenies) This pipeline can be executed as follows: cd $BASE_DIR/nf/14_analysis_phylo_regions nextflow run analysis_phylo_regions.nf 15.1 Summary The phylogenies specific to particular differentiation outlier regions are reconstructed within the nextflow script analysis_phylo_regions.nf (located under $BASE_DIR/nf/14_analysis_phylo_regions/). This includes both the sample-level as well as the population-level phylogenies. 15.2 Details of analysis_phylo_regions.nf This part of the analysis was actually manged manually and not via nextflow. We still report the analysis as a .nf script as we believe this is a cleaner and more concise report of the conducted analysis. 15.2.1 Setup The nextflow script starts by opening the two specific linkage groups of the all_bp genotype data set and binding it to differentiation outlier IDs as well as to a reference table containing the genomic coordinates of all differentiation outlier regions. #!/usr/bin/env nextflow // ----------------------- DISCLAIMER ---------------------- // this pipeline was not actually run using nexflow, // but managed manually // --------------------------------------------------------- // Region-specific phylogenies // --------------------------- // git 14.1 // bundle allBP files and outlier table Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/byLG/filterd.allBP.LG04.vcf.{gz,gz.tbi}&quot;) .concat(Channel.fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/byLG/filterd.allBP.LG12.vcf.{gz,gz.tbi}&quot;)) .concat(Channel.fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/byLG/filterd.allBP.LG12.vcf.{gz,gz.tbi}&quot;)) .merge(Channel.from(&quot;LG04_1&quot;, &quot;LG12_3&quot;, &quot;LG12_4&quot;)) .combine(Channel.fromPath(&quot;../../ressources/focal_outlier.tsv&quot;)) .set{ vcf_lg_ch } Then, two different sample lists (both excluding hybrid samples, one with and one without Serranus outgroup samples) are loaded and bound to a sample mode identifier. // git 14.2 // toggle sample modes (with / without Serranus outgroup) Channel.fromPath(&quot;../../ressources/samples_155.txt&quot;) .concat(Channel.fromPath(&quot;../../ressources/samples_hybrids.txt&quot;)) .merge(Channel.from(&quot;155&quot;, &quot;hyS&quot;)) .set{ sample_mode_ch } Next, for each outlier region, the genotype data is subset to the respective outlier region. // git 14.3 // subset genotypes to outlier region process extract_regions { input: set val( vcfIdx ), file( vcf ), val( outlierId ), file( outlier_file ), file( sample_file ), val( sample_mode ) from vcf_lg_ch.combine( sample_mode_ch ) output: set file( &quot;*_${sample_mode}.vcf&quot; ), val( outlierId ), val( sample_mode ) into ( vcf_raxml_ch, vcf_pomo_ch ) script: &quot;&quot;&quot; # Extract regions of interest from genotype data (allBP), # remove hybrid / Serranus samples and indels; simplify headers head -n 1 ${outlier_file} | cut -f 1-3 &gt; outlier.bed grep ${outlierId} ${outlier_file} | cut -f 1-3 &gt;&gt; outlier.bed OUT_ALT=\\$(echo ${outlierId} | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39; | sed &#39;s/_/./&#39;) vcftools --gzvcf \\ ${vcf[0]} \\ --bed outlier.bed \\ --remove-indels \\ --remove ${sample_file} \\ --recode \\ --stdout | \\ grep -v &#39;##&#39; &gt; \\${OUT_ALT}_${sample_mode}.vcf &quot;&quot;&quot; } Then, for the population-level phylogenies, the genotypes are first converted to fasta format and then to a allele frequency format (.cf). At that point, iqtree2 is run to create the population-level phylogenies. // git 14.4 // run iqtree under pomo model process run_pomo { publishDir &quot;../../2_analysis/revPoMo/outlier_regions/&quot;, mode: &#39;copy&#39; input: set file( vcf ), val( outlierId ), val( sample_mode ) from vcf_raxml_ch output: file( &quot;*_pop.cf.treefile&quot; ) into pomo_results_ch script: &quot;&quot;&quot; OUT_ALT=\\$(echo ${outlierId} | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39; | sed &#39;s/_/./&#39;) # Convert to fasta format (Python scripts available at https://github.com/simonhmartin/genomics_general), picked up from 6.1.1 output python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py -i ${vcf} &gt; \\${OUT_ALT}_${sample_mode}.geno python \\$SFTWR/genomics_general/genoToSeq.py \\ -g \\${OUT_ALT}_${sample_mode}.geno \\ -s \\${OUT_ALT}_${sample_mode}.fas \\ -f fasta \\ --splitPhased # Reformat sample ids to provide population prefixes for cflib sed -e &#39;s/-/_/g&#39; -e &#39;s/&gt;\\(.*\\)\\([a-z]\\{6\\}\\)_\\([AB]\\)/&gt;\\2-\\1_\\3/g&#39; \\${OUT_ALT}_${sample_mode}.fas &gt; \\${OUT_ALT}_${sample_mode}_p.fas # Convert to allele frequency format (cflib library available at https://github.com/pomo-dev/cflib) \\$SFTWR/cflib/FastaToCounts.py \\${OUT_ALT}_${sample_mode}_p.fas \\${OUT_ALT}_${sample_mode}_pop.cf # IQTREE analysis under PoMo model iqtree2 \\ -nt 16 \\ -s \\${OUT_ALT}_${sample_mode}_pop.cf \\ -m HKY+F+P+N9+G4 \\ -b 100 &quot;&quot;&quot; } For the sample-level phylogenies, the genotypes are also converted to fasta format. // git 14.5 // convert genotypes to fasta for raxml process conversion_raxml { input: set file( vcf ), val( outlierId ), val( sample_mode ) from vcf_pomo_ch output: set val( outlierId ), val( sample_mode ), file( &quot;*N.fas&quot; ) into outlier_regions_ch script: &quot;&quot;&quot; OUT_ALT=\\$(echo ${outlierId} | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39; | sed &#39;s/_/./&#39;) # Replace unknown character states and asterisks (deletions as encoded by GATK) with &quot;N&quot; vcf-to-tab &lt; ${vcf} | sed -e &#39;s/\\\\.\\\\/\\\\./N\\\\/N/g&#39; -e &#39;s/[ACGTN\\\\*]\\\\/\\\\*/N\\\\/N/g&#39; &gt; \\${OUT_ALT}_${sample_mode}N.tab # Convert to fasta format (Perl script available at https://github.com/JinfengChen/vcf-tab-to-fasta) wget https://raw.githubusercontent.com/JinfengChen/vcf-tab-to-fasta/master/vcf_tab_to_fasta_alignment.pl perl ~/apps/vcf-tab-to-fasta/vcf_tab_to_fasta_alignment.pl -i \\${OUT_ALT}_${sample_mode}N.tab &gt; \\${OUT_ALT}_${sample_mode}N.fas &quot;&quot;&quot; } Then, raxml is run directly on the fasta files. // git 14.6 // run raxml process run_raxml { publishDir &quot;../../2_analysis/raxml/&quot;, mode: &#39;copy&#39; input: set val( outlierId ), val( sample_mode ), file( fas ) from outlier_regions_ch output: file( &quot;*.raxml.support&quot; ) into outlier_results_ch script: &quot;&quot;&quot; OUT_ALT=\\$(echo ${outlierId} | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39; | sed &#39;s/_/./&#39;) # Reconstruct phylogenies raxml-NG --all \\ --msa ${fas} \\ --model GTR+G \\ --tree pars{10},rand{10} \\ --bs-trees 100 \\ --threads 24 \\ --worker 8 \\ --seed 123 \\ --prefix \\${OUT_ALT}_${sample_mode}N &quot;&quot;&quot; } "],
["git-15-analysis-xiii-diversity-with-and-without-outlier-regions.html", "16 (git 15) Analysis XIII (diversity with and without outlier regions) 16.1 Summary 16.2 Details of analysis_pi.nf", " 16 (git 15) Analysis XIII (diversity with and without outlier regions) This pipeline can be executed as follows: cd $BASE_DIR/nf/15_analysis_pi source ../../sh/nextflow_alias.sh nf_run_pi 16.1 Summary The … are computed within the nextflow script analysis_pi.nf (located under $BASE_DIR/nf/15_analysis_pi/). It takes the … and computes …. 16.2 Details of analysis_pi.nf 16.2.1 Setup The nextflow script starts by … #!/usr/bin/env nextflow // This pipeline includes the analysis run on the // all callable sites data sheet (pi). // git 15.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/filterd.allBP.vcf.{gz,gz.tbi}&quot;) .set{ vcf_pi_ch } // git 15.2 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .set{ lg_pi_ch } // git 15.3 // set outlier window mode Channel .from( &quot;&quot;, &quot;_no_outlier&quot; ) .set{ outlier_mode_ch } // git 15.4 // init slining window resolutions Channel .from( 1, 5 ) .set{ kb_ch } // git 15.5 // init all sampled populations (for pi) Channel .from(&#39;indbel&#39;, &#39;maybel&#39;, &#39;nigbel&#39;, &#39;puebel&#39;, &#39;unibel&#39;, &#39;abehon&#39;, &#39;gumhon&#39;, &#39;nighon&#39;, &#39;puehon&#39;, &#39;ranhon&#39;, &#39;unihon&#39;, &#39;nigpan&#39;, &#39;puepan&#39;, &#39;unipan&#39;) .combine( vcf_pi_ch ) .combine( kb_ch ) .combine( lg_pi_ch ) .combine( outlier_mode_ch ) .set{ input_ch } // filter genotypes and convert formats process recode_genotypes { label &#39;L_32g15h_recode&#39; tag &quot;${spec}&quot; input: set val( spec ), vcfId, file( vcf ), val( kb ), val( lg ), val( outlier ) from input_ch output: set val( spec ), val( kb ), val( lg ), val( outlier ), file( &quot;*.geno.gz&quot; ), file( &quot;pop.txt&quot; ) into geno_ch script: &quot;&quot;&quot; if [ &quot;${outlier}&quot; == &quot;_no_outlier&quot; ];then tail -n +2 \\$BASE_DIR/2_analysis/summaries/fst_outliers_998.tsv | \\ cut -f 2,3,4 &gt; outlier.bed SUBSET=&quot;--exclude-bed outlier.bed&quot; else SUBSET=&quot;&quot; fi vcfsamplenames ${vcf[0]} | \\ grep ${spec} &gt; pop.txt vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --chr LG${lg} \\ \\$SUBSET \\ --recode \\ --stdout | bgzip &gt; ${spec}${outlier}.LG${lg}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i ${spec}${outlier}.LG${lg}.vcf.gz | \\ gzip &gt; ${spec}${outlier}.LG${lg}.geno.gz &quot;&quot;&quot; } // git 15.6 // calculate pi per species process pi_per_spec { label &#39;L_32g15h_pi&#39; tag &quot;${spec}&quot; input: set val( spec ), val( kb ), val( lg ), val( outlier ), file( geno ), file( pop ) from geno_ch output: set val( &quot;${spec}${outlier}.${kb}&quot; ), val( kb ), file( &quot;*0kb.csv.gz&quot; ) into pi_lg_ch script: &quot;&quot;&quot; awk &#39;{print \\$1&quot;\\\\t&quot;substr(\\$1,length(\\$1)-5,length(\\$1)-1)}&#39; ${pop} &gt; ${spec}.pop python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 \\ -s ${kb}000 \\ --popsFile ${spec}.pop \\ -p ${spec} \\ -g ${geno} \\ -o pi.${spec}${outlier}.LG${lg}.${kb}0kb.csv.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } process merge_pi { label &#39;L_32g4h_pi_merge&#39; tag &quot;${spec_outlier_kb}&quot; publishDir &quot;../../2_analysis/pi/${kb[0]}0k&quot;, mode: &#39;copy&#39; input: set val( spec_outlier_kb ), val( kb ), file( pi ) from pi_lg_ch.groupTuple() output: file( &quot;pi.${spec_outlier_kb}0kb.tsv.gz&quot; ) into pi_output_ch script: &quot;&quot;&quot; echo -e &quot;CHROM\\\\tBIN_START\\\\tBIN_END\\\\tBIN_MID_SITES\\\\tN_SITES\\\\tPI&quot; &gt; pi.${spec_outlier_kb}0kb.tsv for k in \\$(ls *.csv.gz); do zcat \\$k | \\ grep -v &quot;scaff&quot; | \\ sed -s &quot;s/,/\\t/g&quot; &gt;&gt; pi.${spec_outlier_kb}0kb.tsv done gzip pi.${spec_outlier_kb}0kb.tsv &quot;&quot;&quot; } "],
["git-16-analysis-xiv-identity-by-descent.html", "17 (git 16) Analysis XIV (Identity by Descent) 17.1 Summary 17.2 Details of analysis_ibd.nf", " 17 (git 16) Analysis XIV (Identity by Descent) This pipeline can be executed as follows: cd $BASE_DIR/nf/16_analysis_ibd nextflow run analysis_ibd.nf -c ../../nextflow.config -resume 17.1 Summary The … are computed within the nextflow script analysis_.nf (located under $BASE_DIR/nf/16_analysis_ibd/). It takes the … and computes …. Below is an overview of the steps involved in the analysis. 17.2 Details of analysis_ibd.nf 17.2.1 Setup The nextflow script starts by … #!/usr/bin/env nextflow // This pipeline includes the analysis calculating the pair-wise IBD // git 16.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ genotypes_raw_ch } // git 16.2 // drop outgroups process drop_outgroups { input: set vcfId, file( vcf ) from genotypes_raw_ch output: file( &quot;phased_mac2.no_outgroup.vcf.gz&quot; ) into genotypes_ch script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep &quot;tor\\\\|tab\\\\|flo&quot; &gt; outrgr.pop vcftools --gzvcf ${vcf[0]} \\ --remove outrgr.pop \\ --recode \\ --stdout | bgzip &gt; phased_mac2.no_outgroup.vcf.gz &quot;&quot;&quot; } // git 16.3 // Set IBD fragment sizes Channel .from([[ 25000, 10000, 7 ], [ 15000, 7500, 10 ], [ 10000, 5000, 8 ]]) .set{ seq_sizes_ch } // git 16.4 // Set filter mode Channel .from([[&quot;direct&quot;, &quot;&quot;], [&quot;bed&quot;, &quot;95&quot;]]) .set{ filtermode_ch } // git 16.5 // run truffle process run_truffle { publishDir &quot;../../2_analysis/ibd/&quot;, mode: &#39;copy&#39; input: set file( vcf ), val( sz1 ), val( sz2 ), val( sz3 ), val( mode ), val( excluding ) from genotypes_ch.combine( seq_sizes_ch ).combine( filtermode_ch ) output: set val( sz3 ), file( &quot;no_outgr_${mode}${excluding}_${sz3}.ibd.tsv&quot; ), file( &quot;no_outgr_${mode}${excluding}_${sz3}.segments.tsv&quot; ) into truffle_result script: if( mode == &#39;direct&#39; ) &quot;&quot;&quot; truffle \\ --vcf ${vcf} \\ --segments \\ --nofiltering \\ --ibs1markers ${sz1} \\ --ibs2markers ${sz2} \\ --out no_outgr_${mode}${excluding}_${sz3} \\ --cpu 8 sed &#39;s/^\\\\s*//g; s/\\\\s\\\\+/\\\\t/g&#39; no_outgr_${mode}${excluding}_${sz3}.ibd &gt; no_outgr_${mode}${excluding}_${sz3}.ibd.tsv sed &#39;s/^\\\\s*//g; s/\\\\s\\\\+/\\\\t/g&#39; no_outgr_${mode}${excluding}_${sz3}.segments &gt; no_outgr_${mode}${excluding}_${sz3}.segments.tsv &quot;&quot;&quot; else if( mode == &#39;filter&#39; ) &quot;&quot;&quot; vcftools --gzvcf ${vcf} \\ --not-chr ${excluding} \\ --recode \\ --stdout | bgzip &gt; tmp.vcf.gz truffle \\ --vcf tmp.vcf.gz \\ --segments \\ --nofiltering \\ --ibs1markers ${sz1} \\ --ibs2markers ${sz2} \\ --out no_outgr_${mode}${excluding}_${sz3} \\ --cpu 8 sed &#39;s/^\\\\s*//g; s/\\\\s\\\\+/\\\\t/g&#39; no_outgr_${mode}${excluding}_${sz3}.ibd &gt; no_outgr_${mode}${excluding}_${sz3}.ibd.tsv sed &#39;s/^\\\\s*//g; s/\\\\s\\\\+/\\\\t/g&#39; no_outgr_${mode}${excluding}_${sz3}.segments &gt; no_outgr_${mode}${excluding}_${sz3}.segments.tsv rm tmp.vcf.gz &quot;&quot;&quot; else if( mode == &#39;bed&#39; ) &quot;&quot;&quot; vcftools --gzvcf ${vcf} \\ --exclude-bed ../../../../../ressources/plugin/idb_above_${excluding}.bed \\ --recode \\ --stdout | bgzip &gt; tmp.vcf.gz truffle \\ --vcf tmp.vcf.gz \\ --segments \\ --nofiltering \\ --ibs1markers ${sz1} \\ --ibs2markers ${sz2} \\ --out no_outgr_${mode}${excluding}_${sz3} \\ --cpu 8 sed &#39;s/^\\\\s*//g; s/\\\\s\\\\+/\\\\t/g&#39; no_outgr_${mode}${excluding}_${sz3}.ibd &gt; no_outgr_${mode}${excluding}_${sz3}.ibd.tsv sed &#39;s/^\\\\s*//g; s/\\\\s\\\\+/\\\\t/g&#39; no_outgr_${mode}${excluding}_${sz3}.segments &gt; no_outgr_${mode}${excluding}_${sz3}.segments.tsv rm tmp.vcf.gz &quot;&quot;&quot; } // git 16.6 // convert IBD segments to cM process convert_to_cM { publishDir &quot;../../2_analysis/ibd/cM_converted&quot;, mode: &#39;copy&#39; input: set val( sz3 ), file( truffle_summary ) , file( truffle_segments ) from truffle_result output: set file( &quot;*.converted.tsv&quot; ), file( &quot;*.conv_summary.tsv&quot; ), file( &quot;*.conv_filterd.tsv&quot; ) into cM_result script: &quot;&quot;&quot; #!/usr/bin/env Rscript base_dir &lt;- Sys.getenv(&quot;BASE_DIR&quot;) args &lt;- c(&quot;ressources/recombination/&quot;, &quot;MAP1cm.txt&quot;, &quot;MAP1bp.txt&quot;, &quot;MAP2cm.txt&quot;, &quot;MAP2bp.txt&quot;, &quot;${truffle_segments}&quot;, &quot;${truffle_summary}&quot;) renv::activate(base_dir) library(GenomicOriginsScripts) library(hypogen) library(patchwork) library(plyranges) rec_path &lt;- str_c(base_dir, as.character(args[1])) hypo_map1_cm &lt;- as.character(args[2]) hypo_map1_bp &lt;- as.character(args[3]) hypo_map2_cm &lt;- as.character(args[4]) hypo_map2_bp &lt;- as.character(args[5]) segment_file &lt;- as.character(args[6]) summary_file &lt;- as.character(args[7]) truffle_conv &lt;- segment_file %&gt;% str_replace(pattern = &quot;.segments.tsv&quot;, replacement = &quot;.converted.tsv&quot;) %&gt;% str_remove(&quot;.*/&quot;) truffle_sum &lt;- segment_file %&gt;% str_replace(pattern = &quot;.segments.tsv&quot;, replacement = &quot;.conv_summary.tsv&quot;) %&gt;% str_remove(&quot;.*/&quot;) truffle_filt &lt;- segment_file %&gt;% str_replace(pattern = &quot;.segments.tsv&quot;, replacement = &quot;.conv_filterd.tsv&quot;) %&gt;% str_remove(&quot;.*/&quot;) read_maps &lt;- function(cm_file, bp_file){ read_tsv(cm_file) %&gt;% group_by(LG) %&gt;% mutate(LGnm = as.roman(LG) %&gt;% as.numeric(), CHROM = str_c(&quot;LG&quot;, str_pad(LGnm, width = 2, pad = 0)), cM = ifelse(LG == &quot;VIII&quot;, max(cM)-cM,cM)) %&gt;% ungroup() %&gt;% dplyr::select(Loci, cM, CHROM) %&gt;% full_join(read_tsv(bp_file) %&gt;% filter(!(duplicated(Loci) | duplicated(Loci, fromlast = TRUE))), by = c(Loci = &quot;Loci&quot;, CHROM = &quot;LG&quot;)) %&gt;% left_join(hypo_chrom_start) %&gt;% mutate(GPOS = GSTART + bp) %&gt;% filter(!is.na(GPOS), !is.na(cM)) %&gt;% arrange(GPOS) } make_lg_seg &lt;- function(lg = &quot;LG08&quot;, n = 31, gmap = gmap1){ data_pos &lt;- tibble(CHROM = rep(lg, n), start = seq(from = hypo_karyotype\\$GSTART[hypo_karyotype\\$CHROM == lg], to = hypo_karyotype\\$GEND[hypo_karyotype\\$CHROM == lg], length = n) %&gt;% floor(), GSTART = hypo_karyotype\\$GSTART[hypo_karyotype\\$CHROM == lg]) %&gt;% mutate(GPOS = start, start = start - GSTART, end = start) %&gt;% as_iranges() map_pos &lt;- gmap %&gt;% filter(CHROM == lg) %&gt;% attach_end(LG = lg) %&gt;% mutate(start = lag(bp, default = 0), start_cM = lag(cM, default = 0)) %&gt;% dplyr::select(CHROM, start, end = bp, start_cM, end_cM = cM) %&gt;% mutate(start_bp = start, end_bp = end) %&gt;% as_iranges() list(data = data_pos, map = map_pos) } attach_end &lt;- function(data, LG = &quot;LG01&quot;){ data %&gt;% bind_rows(., data %&gt;% filter(row_number() == last(row_number())) %&gt;% mutate(GPOS = hypo_karyotype\\$GEND[hypo_karyotype\\$CHROM == CHROM], bp = hypo_karyotype\\$LENGTH[hypo_karyotype\\$CHROM == CHROM], Loci = as.numeric( str_c(&quot;-99&quot;, str_remove(string = CHROM, &quot;LG&quot;)) ) )) } bin_rescaler &lt;- function(bp, start_cM, end_cM, start_bp, end_bp,...){ scales::rescale(x = bp, to = c(start_cM, end_cM), from = c(start_bp, end_bp)) } interpol_data &lt;- function(lg, ...){ data_pair &lt;- make_lg_seg(lg = lg, ...) plyranges::join_overlap_inner(data_pair\\$data, data_pair\\$map) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% dplyr::select(CHROM = CHROM.x, bp = start, GSTART, GPOS, start_cM:end_bp) %&gt;% mutate(interpol_cM = pmap_dbl(cur_data(), bin_rescaler)) } na_to_zero &lt;- function(x){ x_type &lt;- typeof(x) if_else(is.na(x), as(0,Class = x_type), x) %&gt;% as.double() %&gt;% as(Class = x_type) } convert_bp_to_cm &lt;- function(data, lg = &quot;LG08&quot;, gmap = gmap1){ gmap_in &lt;- deparse(substitute(gmap)) data_pos &lt;- data %&gt;% filter( CHROM == lg ) %&gt;% dplyr::select(PAIR, TYPE, CHROM, START, END, NMARKERS) %&gt;% mutate(seg_id = str_c(PAIR,&quot;_&quot;,CHROM,&quot;_&quot;,START)) %&gt;% pivot_longer(cols = START:END, names_to = &quot;PART&quot;, values_to = &quot;start&quot;) %&gt;% mutate(end = start) %&gt;% as_iranges() map_pos &lt;- gmap %&gt;% filter(CHROM == lg) %&gt;% attach_end(LG = lg) %&gt;% mutate(start = lag(bp, default = 0) + 1, # avoid overlapping segments - causes duplications in joining start_cM = lag(cM, default = 0)) %&gt;% dplyr::select(start, end = bp, start_cM, end_cM = cM) %&gt;% mutate(start_bp = start, end_bp = end) %&gt;% as_iranges() map_nr &lt;- str_replace(gmap_in, pattern = &quot;gmap&quot;, replacement = &quot;_m&quot;) plyranges::join_overlap_inner(data_pos, map_pos) %&gt;% as.data.frame() %&gt;% as_tibble() %&gt;% dplyr::select(CHROM = CHROM, bp = start, PAIR, TYPE, PART, start_cM:end_bp, seg_id, NMARKERS) %&gt;% mutate(interpol_cM = pmap_dbl(cur_data(), bin_rescaler)) %&gt;% pivot_wider(id_cols = c(CHROM,PAIR,TYPE,seg_id, PART,NMARKERS), values_from = c(bp,interpol_cM), names_from = PART) %&gt;% dplyr::select(-seg_id) %&gt;% mutate(length_bp = bp_END - bp_START, length_cM = interpol_cM_END - interpol_cM_START) %&gt;% set_names(value = c(&quot;CHROM&quot;, &quot;PAIR&quot;, &quot;TYPE&quot;, &quot;NMARKERS&quot;, &quot;bp_START&quot;, &quot;bp_END&quot;, str_c(c(&quot;interpol_cM_START&quot;, &quot;interpol_cM_END&quot;), map_nr), &quot;length_bp&quot;, str_c(&quot;length_cM&quot;, map_nr))) } # actual script ------------------- gmap1 &lt;- read_maps(cm_file = str_c(rec_path, hypo_map1_cm), bp_file = str_c(rec_path, hypo_map1_bp)) gmap2 &lt;- read_maps(cm_file = str_c(rec_path, hypo_map2_cm), bp_file = str_c(rec_path, hypo_map2_bp)) lgs &lt;- 1:24 %&gt;% str_pad(width = 2, pad = 0) %&gt;% str_c(&quot;LG&quot;,.) segments_individual_interpol_map1 &lt;- lgs %&gt;% map_dfr(interpol_data, n = 51) segments_individual_interpol_map2 &lt;- lgs %&gt;% map_dfr(interpol_data, n = 51, gmap = gmap2) segments_individual &lt;- vroom::vroom(segment_file) %&gt;% mutate(LENGTH = LENGTH * 10^6, START = POS * 10^6, END = START + LENGTH, PAIR = str_c(ID1, &quot;-&quot;, ID2)) segments_summary &lt;- vroom::vroom(summary_file) %&gt;% mutate(PAIR = str_c(ID1, &quot;-&quot;, ID2)) bounds_gmap1 &lt;- gmap1 %&gt;% group_by(CHROM) %&gt;% filter(cM == max(cM)) %&gt;% filter(bp == max(bp)) %&gt;% ungroup() %&gt;% dplyr::select(CHROM, cM) %&gt;% mutate(GSTART_cM = cumsum(lag(cM, default = 0)), GEND_cM = cM + GSTART_cM, GMID_cM = (GSTART_cM + GEND_cM) / 2, grp = c(&quot;even&quot;, &quot;odd&quot;)[ 1+row_number() %% 2 ]) bounds_gmap2 &lt;- gmap2 %&gt;% group_by(CHROM) %&gt;% filter(cM == max(cM)) %&gt;% filter(bp == max(bp)) %&gt;% ungroup() %&gt;% dplyr::select(CHROM, cM) %&gt;% mutate(GSTART_cM = cumsum(lag(cM, default = 0)), GEND_cM = cM + GSTART_cM, GMID_cM = (GSTART_cM + GEND_cM) / 2, grp = c(&quot;even&quot;, &quot;odd&quot;)[ 1+row_number() %% 2 ]) hypo_all_starts &lt;- hypo_karyotype %&gt;% dplyr::select(CHROM, GSTART, GEND) %&gt;% left_join(bounds_gmap1 %&gt;% dplyr::select(CHROM, GSTART_cM_m1 = GSTART_cM, GEND_cM_m1 = GEND_cM)) %&gt;% left_join(bounds_gmap2 %&gt;% dplyr::select(CHROM, GSTART_cM_m2 = GSTART_cM, GEND_cM_m2 = GEND_cM)) converted_segments &lt;- lgs %&gt;% map_dfr(convert_bp_to_cm, data = segments_individual) %&gt;% left_join( lgs %&gt;% map_dfr(convert_bp_to_cm, data = segments_individual, gmap = gmap2) ) %&gt;% left_join(hypo_all_starts) %&gt;% mutate(G_SEG_START = GSTART + bp_START, G_SEG_END = GSTART + bp_END, G_SEG_START_cM_m1 = GSTART_cM_m1 + interpol_cM_START_m1, G_SEG_END_cM_m1 = GSTART_cM_m1 + interpol_cM_END_m1, G_SEG_START_cM_m2 = GSTART_cM_m2 + interpol_cM_START_m2, G_SEG_END_cM_m2 = GSTART_cM_m2 + interpol_cM_END_m2) hypo_cM_length_map1 &lt;- max(hypo_all_starts\\$GEND_cM_m1) hypo_cM_length_map2 &lt;- max(hypo_all_starts\\$GEND_cM_m2) hypo_bp_length &lt;- hypo_karyotype\\$GEND[hypo_karyotype\\$CHROM == &quot;LG24&quot;] control &lt;- converted_segments %&gt;% ungroup() %&gt;% group_by(PAIR, TYPE) %&gt;% summarise(seq_length = sum(length_bp), n_mark = sum(NMARKERS), cm_length_m1 = sum(length_cM_m1), cm_length_m2 = sum(length_cM_m2)) %&gt;% ungroup() %&gt;% pivot_wider(id_cols = PAIR, names_from = TYPE, values_from = seq_length:cm_length_m2, values_fill = 0) %&gt;% left_join(segments_summary, ., ) %&gt;% mutate(across(.cols = seq_length_IBD1:cm_length_m2_IBD2, .fns = na_to_zero)) %&gt;% mutate(IBD0_manual = (NMARK - (n_mark_IBD1 + n_mark_IBD2)) / NMARK, IBD1_manual = n_mark_IBD1 / NMARK, IBD2_manual = n_mark_IBD2 / NMARK, icheck_0 = IBD0_manual - IBD0, icheck_1 = IBD1_manual - IBD1, icheck_2 = IBD2 - IBD2, # compile ibd by sequence map ibd0_bp = (hypo_bp_length - (seq_length_IBD1 + seq_length_IBD2)) / hypo_bp_length, ibd1_bp = seq_length_IBD1 / hypo_bp_length, ibd2_bp = seq_length_IBD2 / hypo_bp_length, # compile ibd by genetic map 1 ibd0_cM_m1 = (hypo_cM_length_map1 - (cm_length_m1_IBD1 + cm_length_m1_IBD2)) / hypo_cM_length_map1, ibd1_cM_m1 = cm_length_m1_IBD1 / hypo_cM_length_map1, ibd2_cM_m1 = cm_length_m1_IBD2 / hypo_cM_length_map1, # compile ibd by genetic map 2 ibd0_cM_m2 = (hypo_cM_length_map2 - (cm_length_m2_IBD1 + cm_length_m2_IBD2)) / hypo_cM_length_map2, ibd1_cM_m2 = cm_length_m2_IBD1 / hypo_cM_length_map2, ibd2_cM_m2 = cm_length_m2_IBD2 / hypo_cM_length_map2) cM_treshold &lt;- 0.2 summary_filterd &lt;- converted_segments %&gt;% filter(length_cM_m1 &gt; cM_treshold &amp; length_cM_m2 &gt; cM_treshold) %&gt;% ungroup() %&gt;% group_by(PAIR, TYPE) %&gt;% summarise(seq_length = sum(length_bp), n_mark = sum(NMARKERS), cm_length_m1 = sum(length_cM_m1), cm_length_m2 = sum(length_cM_m2)) %&gt;% ungroup() %&gt;% pivot_wider(id_cols = PAIR, names_from = TYPE, values_from = seq_length:cm_length_m2, values_fill = 0) %&gt;% left_join(segments_summary, ., ) %&gt;% mutate(across(.cols = seq_length_IBD1:cm_length_m2_IBD2, .fns = na_to_zero)) %&gt;% mutate(IBD0_manual = (NMARK - (n_mark_IBD1 + n_mark_IBD2)) / NMARK, IBD1_manual = n_mark_IBD1 / NMARK, IBD2_manual = n_mark_IBD2 / NMARK, icheck_0 = IBD0_manual - IBD0, icheck_1 = IBD1_manual - IBD1, icheck_2 = IBD2 - IBD2_manual, # compile ibd by sequence map ibd0_bp = (hypo_bp_length - (seq_length_IBD1 + seq_length_IBD2)) / hypo_bp_length, ibd1_bp = seq_length_IBD1 / hypo_bp_length, ibd2_bp = seq_length_IBD2 / hypo_bp_length, # compile ibd by genetic map 1 ibd0_cM_m1 = (hypo_cM_length_map1 - (cm_length_m1_IBD1 + cm_length_m1_IBD2)) / hypo_cM_length_map1, ibd1_cM_m1 = cm_length_m1_IBD1 / hypo_cM_length_map1, ibd2_cM_m1 = cm_length_m1_IBD2 / hypo_cM_length_map1, # compile ibd by genetic map 2 ibd0_cM_m2 = (hypo_cM_length_map2 - (cm_length_m2_IBD1 + cm_length_m2_IBD2)) / hypo_cM_length_map2, ibd1_cM_m2 = cm_length_m2_IBD1 / hypo_cM_length_map2, ibd2_cM_m2 = cm_length_m2_IBD2 / hypo_cM_length_map2) write_tsv(x = converted_segments, file = truffle_conv) write_tsv(x = control, file = truffle_sum) write_tsv(x = summary_filterd, file = truffle_filt) &quot;&quot;&quot; } "],
["git-17-analysis-xv-dstats.html", "18 (git 17) Analysis XV (dstats) 18.1 Summary 18.2 Details of analysis_dstats.nf", " 18 (git 17) Analysis XV (dstats) This pipeline can be executed as follows: cd $BASE_DIR/nf/17_analysis_dstats nextflow run analysis_dstats.nf -c ../../nextflow.config -resume 18.1 Summary The … are computed within the nextflow script analysis_dstats.nf (located under $BASE_DIR/nf/17_analysis_dstats/). It takes the … and computes …. Below is an overview of the steps involved in the analysis. 18.2 Details of analysis_dstats.nf 18.2.1 Setup The nextflow script starts by … #!/usr/bin/env nextflow // ----------------------- DISCLAIMER ---------------------- // this pipeline was not actually run using nextflow, // but managed manually // --------------------------------------------------------- // git 17.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ genotypes_ch } // git 17.2 // drop serranus process drop_serranus { input: set vcfId, file( vcf ) from genotypes_ch output: file( &quot;hyp_mac2.vcf.gz&quot; ) into no_serranus_ch script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep &quot;tor\\\\|tab\\\\&quot; &gt; serr.pop vcftools --gzvcf ${vcf[0]} \\ --remove serr.pop \\ --recode \\ --stdout | bgzip &gt; hyp_mac2.vcf.gz &quot;&quot;&quot; } // git 17.4 // LD filtering process ld_filter { input: file( vcf ) from no_serranus_ch output: file( &quot;hyp_mac2_ld05.vcf.gz&quot; ) into ld_filtered_ch script: &quot;&quot;&quot; bcftools +prune \\ -l 0.5 \\ -w 50kb \\ ${vcf} \\ -Oz \\ -o hyp_mac2_ld05.vcf.gz &quot;&quot;&quot; } // git 17.5 // run D trios Channel .fromPath(&quot;../../ressources/hyp_sets.txt&quot;) .set{ hyp_sets_ch } // git 17.6 // run D trios process run_dtrios { publishDir &quot;../../2_analysis/dstats/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( sets ) from ld_filtered_ch.combine( hyp_sets_ch ) output: set file( &quot;hyp_ld05_dtrios_BBAA.txt&quot; ), file( &quot;hyp_ld05_dtrios_Dmin.txt&quot; ) into dtrios_results_ch script: &quot;&quot;&quot; zcat ${vcf} &gt; hyp_mac2_ld05.vcf Dsuite Dtrios \\ -c \\ -o hyp_ld05_dtrios \\ hyp_mac2_ld05.vcf \\ ${sets} &quot;&quot;&quot; } // git 17.7 // load predefined species order Channel .fromPath(&quot;../../ressources/species_order_alpha.txt&quot;) .set{ spec_order_ch } // git 17.8 // Extract significant Dmin, BBAA trios process run_correction { publishDir &quot;../../2_analysis/dstats/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( sets ), file( spec_order) from dtrios_results_ch.combine( spec_order_ch ) output: set file( &quot;BBAA_sign_ld05.csv&quot; ), file( &quot;Dmin_sign_ld05.csv&quot; ) into dtrios_signif_ch script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/dstats.R &quot;&quot;&quot; } "],
["git-18-genotyping-iii-all-callable-sites-for-mtdna-and-unplaced-contigs.html", "19 (git 18) Genotyping III (all callable sites for mtDNA and unplaced contigs) 19.1 Summary 19.2 Details of genotyping_all_basepairs_mt.nf", " 19 (git 18) Genotyping III (all callable sites for mtDNA and unplaced contigs) This pipeline can be executed as follows: cd $BASE_DIR/nf/18_genotyping_all_basepairs_mt source ../../sh/nextflow_alias.sh nf_run_allbp_mt1 19.1 Summary The genotyping procedure is controlled by the nextflow script genotyping_all_basepairs_mt.nf (located under $BASE_DIR/nf/18_genotyping_all_basepairs_mt). Based on an intermediate step from genotyping.nf (git 1.10), this script produces a data set that includes all callable sites - that is SNPs as well a invariant sites that are covered by sequence (for mtDNA and unplaced contigs). The genotypes produced by this script are then used in the Serraninae phylogeny. 19.2 Details of genotyping_all_basepairs_mt.nf 19.2.1 Data preparation The nextflow script starts with a small header and then imports the joint genotyping likelihoods for all samples produced by genotyping.nf. Furthermore a channel is created to call mtDNA and unplaced contigs seperately. #!/usr/bin/env nextflow // git 18.1 // open genotype likelyhoods Channel .fromFilePairs(&quot;../../1_genotyping/1_gvcfs/cohort.g.vcf.{gz,gz.tbi}&quot;) .set{ vcf_cohort } Channel .from([&quot;LG_M&quot;, &quot;unplaced&quot;]) .set{ lg_mode } The samples are jointly genotyped, independently for mtDNA and unplaced contigs and including invariant sites. // git 18.2 // actual genotyping step (including invariant sites) process joint_genotype_snps { label &quot;L_88g48h_LGs_genotype&quot; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;copy&#39; input: set vcfId, file( vcf ), val( mode ) from vcf_cohort.combine( lg_mode ) output: set file( &quot;all_sites.${mode}.vcf.gz&quot; ), file( &quot;all_sites.${mode}.vcf.gz.tbi&quot; ), val( mode ) into ( all_bp_non_lg_1, all_bp_non_lg_2 ) script: if( mode == &#39;unplaced&#39; ) &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -XL=LG01 \\ -XL=LG02 \\ -XL=LG03 \\ -XL=LG04 \\ -XL=LG05 \\ -XL=LG06 \\ -XL=LG07 \\ -XL=LG08 \\ -XL=LG09 \\ -XL=LG10 \\ -XL=LG11 \\ -XL=LG12 \\ -XL=LG13 \\ -XL=LG14 \\ -XL=LG15 \\ -XL=LG16 \\ -XL=LG17 \\ -XL=LG18 \\ -XL=LG19 \\ -XL=LG20 \\ -XL=LG21 \\ -XL=LG22 \\ -XL=LG23 \\ -XL=LG24 \\ -XL=LG_M \\ -V=${vcf[0]} \\ -O=intermediate.vcf.gz \\ --include-non-variant-sites=true \\ --allow-old-rms-mapping-quality-annotation-data gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-exclude=INDEL \\ -O=all_sites.${mode}.vcf.gz rm intermediate.* &quot;&quot;&quot; else if( mode == &#39;LG_M&#39; ) &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -L=${mode} \\ -V=${vcf[0]} \\ -O=intermediate.vcf.gz \\ --include-non-variant-sites=true \\ --allow-old-rms-mapping-quality-annotation-data gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-exclude=INDEL \\ -O=all_sites.${mode}.vcf.gz rm intermediate.* &quot;&quot;&quot; } "],
["git-19-analysis-xvi-serraninae-phylogeny.html", "20 (git 19) Analysis XVI (Serraninae Phylogeny) 20.1 Summary 20.2 Details of analysis_phylo_serraninae.nf", " 20 (git 19) Analysis XVI (Serraninae Phylogeny) This pipeline can be executed as follows: cd $BASE_DIR/nf/19_analysis_phylo_serraninae nextflow run analysis_phylo_serraninae.nf -c ../../nextflow.config -resume 20.1 Summary The … are computed within the nextflow script analysis_phylo_serraninae.nf (located under $BASE_DIR/nf/19_analysis_phylo_serraninae/). It takes the … and computes …. Below is an overview of the steps involved in the analysis. 20.2 Details of analysis_phylo_serraninae.nf 20.2.1 Setup The nextflow script starts by … #!/usr/bin/env nextflow // ----------------------- DISCLAIMER ---------------------- // this pipeline was not actually run using nextflow, // but managed manually // --------------------------------------------------------- // git 19.1 Channel .fromPath(&#39;../../ressources/serraninae/Serraninae_Rabosky.phy&#39;) .set{ serraninae_phy_ch } // git 19.2 Channel .fromPath(&#39;../../ressources/serraninae/partitions.txt&#39;) .set{ partitions_ch } // git 19.3 // Obtain gene boundaries and split alignment into individual genes process get_gene_boundaries { label &#39;gene_boundaries&#39; input: set file( phy ), file( part ) from serraninae_phy_ch.combine( partitions_ch ) output: file( &quot;*serrR.aln&quot; ) into ( gene_boundaries_ch, gene_boundaries_ch2 ) script: &quot;&quot;&quot; # manually extract Serraninae from Rabosky alignment, final_alignment.phylip (FToL Dryad) &gt; Serraninae_Rabosky.phy perl \\$BASE_DIR/pl/Phylip2Fasta.pl ${phy} Serraninae_Rabosky.aln sed &#39;s/-/N/g&#39; Serraninae_Rabosky.aln &gt; Serraninae_RaboskyN.aln sed &#39;s/DNA, \\\\(.*\\\\) = \\\\(.*\\\\)-\\\\(.*\\\\)/\\\\1\\\\t\\\\2\\\\t\\\\3/g&#39; ${part} &gt; partitions.bed # (partitions.txt from FToL Dryad) awk &#39;{ print \\$2 }&#39; partitions.bed | tail -n +2 | tr &#39;\\\\n&#39; &#39;,&#39; | sed &#39;s/.\\$//&#39; # phast from http://compgen.cshl.edu/phast/ \\$SFTWR/phast/bin/msa_split Serraninae_RaboskyN.aln -r gene \\ --by-index 980,1757,2292,2974,4115,4955,5681,6614,7244,7988,8822,9869,11402,12142,12952,13663,15117,16341,17265,17910,18633,19728,20715,21522,22350,23115 awk &#39;{ print \\$1, \\$2, \\$3 }&#39; partitions.bed | xargs -n 3 sh -c &#39;mv gene.\\$1-\\$2.fa \\$0_serrR.aln&#39; &quot;&quot;&quot; } // git 19.4 // Identify genes in reference genome process identify_genes { label &#39;identify_genes&#39; input: file( aln ) from gene_boundaries_ch output: file( &quot;coord_R24_Hpue_ed.bed&quot; ) into gene_coords_ch script: &quot;&quot;&quot; # manually select query sequences from Serraninae_RaboskyN.aln &gt; queries_R24.fas mkdir blast_db cp \\$BASE_DIR/ressources/HP_genome_unmasked_01.fa blast_db/Hpue_genome_unmasked_01.fas makeblastdb -in blast_db/Hpue_genome_unmasked_01.fas -dbtype nucl -parse_seqids blastn -query queries_R24.fas -db blast_db/Hpue_genome_unmasked_01.fas -out blast_R24-Hpue_aln.txt -outfmt 0 -evalue 1e-10 blastn -query queries_R24.fas -db blast_db/Hpue_genome_unmasked_01.fas -out Rblast_R24-Hpue_tab.csv -outfmt 6 -evalue 1e-10 awk &#39;OFS = &quot;\\\\t&quot; { if (\\$9 &lt; \\$10) print \\$2, \\$9, \\$10, \\$1, &quot;+&quot; ; else print \\$2, \\$10, \\$9, \\$1, &quot;-&quot; }&#39; blast_R24-Hpue_tab.csv | \\ sed &#39;s/\\\\(.*\\\\)_\\\\(.*\\\\)\\\\t/\\\\1\\\\t/g&#39; &gt; coord_R24_Hpue.csv # manually combine segmented genes (16s, glyt, tbr1, zic1) in coord_R24_Hpue.csv &gt; coord_R24_Hpue_ed.csv sed -Ei &#39;s/_[A-Z][a-z]{3}//g&#39; coord_R24_Hpue_ed.bed &quot;&quot;&quot; } // git 19.5 Channel .fromPath(&#39;../../1_genotyping/2_raw_vcfs/all_sites.unplaced.vcf.gz&#39;) .set{ vcf_unlplaced_ch } // git 19.6 // Prepare contigs process prepare_contigs { label &#39;prepare_contigs&#39; input: file ( vcf ) from vcf_unlplaced_ch output: set file( &quot;all_sites.Contig*.vcf.gz&quot; ), file( &quot;*.vcf.gz.tbi&quot; ) into contigs_ch script: &quot;&quot;&quot; zcat &lt; ${vcf} | grep -e &#39;#&#39; -e &#39;Contig11544&#39; | bgzip &gt; all_sites.Contig11544.vcf.gz zcat &lt; ${vcf} | grep -e &#39;#&#39; -e &#39;Contig11607&#39; | bgzip &gt; all_sites.Contig11607.vcf.gz zcat &lt; ${vcf} | grep -e &#39;#&#39; -e &#39;Contig11888&#39; | bgzip &gt; all_sites.Contig11888.vcf.gz tabix -p vcf *.vcf.gz &quot;&quot;&quot; } // git 19.7 Channel .fromPath(&#39;../../ressources/serraninae/samples_to_include.ids&#39;) .set{ samples_ch } // git 19.8 // Extract genotypes and convert to Fasta process extract_genotypes { label &#39;extract_genotypes&#39; input: set file( crds ), file ( samples ) from gene_coords_ch.combine( samples_ch ) output: file( &quot;*.fas&quot; ) into genotypes_ch script: &quot;&quot;&quot; # select representative samples based on highest coverage (except outliers) and regional diversity &gt; samples_to_include.ids awk &#39;{ print $1, $2, $3, $4 }&#39; ${crds} | \\ xargs -n 4 sh -c &#39;vcftools --gzvcf \\$BASE_DIR/2_raw_vcfs/all_sites.&quot;\\$0&quot;.vcf.gz --keep ${samples} --chr &quot;\\$0&quot; --from-bp &quot;\\$1&quot; --to-bp &quot;\\$2&quot; --recode --stdout | grep -v &quot;##&quot; &gt; &quot;\\$3&quot;_hypS.vcf&#39; for FILE in ./*.vcf do vcf-to-tab &lt; \\$FILE &gt; \\${FILE%.vcf}.tab perl \\$SFTWR/vcf-tab-to-fasta/vcf_tab_to_fasta_alignment.pl -i \\${FILE%.vcf}.tab &gt; \\${FILE%.vcf}.fas rm \\${FILE%.vcf}.tab* done rev=`awk &#39;\\$5 == &quot;-&quot; { print \\$4 }&#39; ${crds}` for GENE in \\$rev do seqtk seq -r \\${GENE}_hypS.fas &gt; \\${GENE}_hypS_rev.fas mv \\${GENE}_hypS.fas \\${GENE}_hypS.fas_org done &quot;&quot;&quot; } // git 19.9 Channel .fromPath(&#39;../../ressources/serraninae/taxa_to_exclude.ids&#39;) .set{ taxa_ch } // git 19.10 // Combine Rabosky and present data process combine_data { label &#39;combine_data&#39; input: set file( aln ), file( taxa ), file( fa ) from gene_boundaries_ch2.combine( taxa_ch ).combine( genotypes_ch ) output: file( &quot;*_new.fas&quot; ) into data_combined_ch script: &quot;&quot;&quot; # manually prepared list of taxa to exclude from Rabosky&#39;s alignments &gt; taxa_to_exclude.ids for FILE in *.aln do sed -e &#39;s/N//g&#39; -e &#39;s/&gt; /&gt;/g&#39; -e &#39;/^\\$/d&#39; \\$FILE | awk &#39;BEGIN { while ((getline &lt; &quot;${taxa}&quot;) &gt;0) l[&quot;&gt;&quot;\\$1]=1 } /^&gt;/ { f=!l[\\$1] }f&#39; &gt; \\${FILE%.aln}_pre.fa done for FILE in *.fas do NAME=\\${FILE%_hypS.fas} cat \\$FILE \\${NAME}_serrR_pre.fa &gt; \\${NAME}_new.fas done &quot;&quot;&quot; } // ----------------------- DISCLAIMER ---------------------- // Between git 19.11 and git 19.12 the alignments // were additionally currated manually // --------------------------------------------------------- // git 19.11 // Align sequences and remove ambiguously aligned positions process align_sequences { label &#39;align_sequences&#39; input: file( fa ) from data_combined_ch output: file( &quot;*.aln&quot; ) into align_sequences_ch script: &quot;&quot;&quot; for FILE in ./*_new.fas do mafft --maxiterate 1000 --globalpair --adjustdirectionaccurately \\${FILE} &gt; \\${FILE%.fas}.aln done Gblocks 3_alignments/12s_new.aln -t=D -b2=0 -b3=4 -b4=5 -b5=a -e=.gb -v=100 Gblocks 3_alignments/16s_new.aln -t=D -b2=0 -b3=4 -b4=5 -b5=a -e=.gb -v=100 Gblocks 3_alignments/coi_new.aln -t=D -b2=0 -b3=4 -b4=5 -b5=a -e=.gb -v=100 Gblocks 3_alignments/cytb_new.aln -t=D -b2=0 -b3=4 -b4=5 -b5=a -e=.gb -v=100 # finalize with manual alignment editing step: # 12S: removed 45±3 bp (whole gap) &gt; 12s_new_ed.aln # 16S: removed positions 194, 252, 315 &gt; 16s_new_ed.aln # coi: removed entire gene in torpan, tabhon &gt; coi_new_ed.aln # cytb: removed entire gene in torpan, tabhon &gt; cytb_new_ed.aln &quot;&quot;&quot; } // git 19.12 // Phylogenetic inference (IQ-TREE) process phylogenetic_inference { label &#39;phylogenetic_inference&#39; publishDir &quot;../../2_analysis/fotl/&quot;, mode: &#39;copy&#39; input: file( aln ) from align_sequences_ch output: file( &quot;concat_R24ed.treefile&quot; ) into phylo_out script: &quot;&quot;&quot; mkdir alignments cp *.aln alignments/ iqtree2 -p ./alignments --prefix concat_R24ed -B 1000 -T AUTO &quot;&quot;&quot; } "],
["git-20-data-visualization.html", "21 (git 20) Data Visualization", " 21 (git 20) Data Visualization After all nextflow pipelines are successfully run to completion, each Figure (and Suppl. Figure) of the manuscript can be re-created with its respective R script located under R/fig. These are executable R scripts that can be launched from the base directory; Rscript --vanilla R/fig/plot_Fxyz.R input1 input2 ... For convenience, there also exists a bash script that can be used to re-create all Figures in one go (git 15): cd $BASE_DIR bash sh/create_figures.sh After running create_figures.sh, Figures 1 - 6 and Suppl. Figures 1 - 21 should be created withing the folder figures/. In the remaining documentation, the individual Visualization scripts are going to discussed in detail. Below is the bash code that is executed when running create_figures.sh: #/usr/bin/bash # git 20 # preparation for Fig. 1 Rscript --vanilla R/fst_permutation.R \\ 2_analysis/fst_signif/random/ # Main Figures # ------------------------------------------------- # git 20.1 # needs to be run interactively because fonts are not accessible for Rscript # Rscript --vanilla R/fig/plot_F1.R \\ # 2_analysis/fst/50k/ \\ # 2_analysis/summaries/fst_globals.txt \\ # 2_analysis/summaries/fst_permutation_summary.tsv \\ # 2_analysis/fotl/concat_R24ed.treefile # git 20.2 Rscript --vanilla R/fig/plot_F2.R \\ 2_analysis/fst/50k/multi_fst.50k.tsv.gz \\ 2_analysis/GxP/50000/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/twisst/weights/ \\ ressources/plugin/trees/ \\ 2_analysis/summaries/fst_globals.txt # git 20.3 Rscript --vanilla R/fig/plot_F3.R \\ 2_analysis/msmc/output/ \\ 2_analysis/cross_coalescence/output/ \\ 2_analysis/msmc/setup/msmc_grouping.txt \\ 2_analysis/msmc/setup/msmc_cc_grouping.txt \\ 2_analysis/summaries/fst_globals.txt # git 20.4 Rscript --vanilla R/fig/plot_F4.R \\ 2_analysis/astral/astral_5000x_5kb_v1_noS.tre \\ 2_analysis/ibd/cM_converted/no_outgr_bed95_8.conv_filterd.tsv # git 20.5 Rscript --vanilla R/fig/plot_F5.R \\ 2_analysis/twisst/weights/ \\ ressources/plugin/trees/ \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/GxP/50000/ \\ 200 \\ 5 \\ 2_analysis/revPoMo/outlier_regions/ # git 20.6 Rscript --vanilla R/fig/plot_F6.R \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/geva/ \\ 2_analysis/GxP/bySNP/ # Suppl. Figures # ------------------------------------------------- # git 20.7 Rscript --vanilla R/fig/plot_SF1.R \\ ressources/Rabosky_etal_2018/dataFiles/ratemat_enhanced.csv # git 20.8 # needs to be run interactively because fonts are not accessible for Rscript # Rscript --vanilla R/fig/plot_SF2.R \\ # ressources/Rabosky_etal_2018/ # git 20.9 Rscript --vanilla R/fig/plot_SF3.R \\ 2_analysis/pca/ # git 20.10 Rscript --vanilla R/fig/plot_SF4.R \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/summaries/fst_globals.txt # git 20.11 Rscript --vanilla R/fig/plot_SF5.R \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_globals.txt # git 20.12 Rscript --vanilla R/fig/plot_SF6.R \\ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/fst/50k/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz # git 20.13 Rscript --vanilla R/fig/plot_SF7.R \\ 2_analysis/dxy/50k/ # git 20.14 Rscript --vanilla R/fig/plot_SF8.R \\ 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/multi_fst.50k.tsv.gz \\ 2_analysis/GxP/50000/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/twisst/weights/ \\ ressources/plugin/trees/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz \\ 2_analysis/summaries/fst_globals.txt # git 20.15 Rscript --vanilla R/fig/plot_SF9.R \\ 2_analysis/pi/50k/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz # git 20.16 Rscript --vanilla R/fig/plot_SF10.R \\ 2_analysis/dxy/50k/ # git 20.17 Rscript --vanilla R/fig/plot_SF11.R \\ 2_analysis/newhyb/nh_input/NH.Results/ # git 20.18 Rscript --vanilla R/fig/plot_SF12.R \\ ressources/species_order_alpha.txt \\ 2_analysis/dstats/hyp_ld05_dtrios_BBAA.txt \\ 2_analysis/dstats/BBAA_ld05.csv \\ 2_analysis/dstats/BBAA_sign_ld05.csv # git 20.19 Rscript --vanilla R/fig/plot_SF13.R \\ 2_analysis/summaries/fst_outliers_998.tsv # git 20.20 Rscript --vanilla R/fig/plot_SF14.R \\ 2_analysis/raxml/lg04.1_155N.raxml.support \\ 2_analysis/raxml/lg12.3_155N.raxml.support \\ 2_analysis/raxml/lg12.4_155N.raxml.support # git 20.21 Rscript --vanilla R/fig/plot_SF15.R \\ 2_analysis/raxml/lg04.1_hySN.raxml.support \\ 2_analysis/raxml/lg12.3_hySN.raxml.support \\ 2_analysis/raxml/lg12.4_hySN.raxml.support # git 20.22 Rscript --vanilla R/fig/plot_SF16.R \\ 2_analysis/admixture/ \\ metadata/phenotypes.sc # git 20.23 Rscript --vanilla R/fig/plot_SF17.R \\ 2_analysis/astral/astral_5000x_5kb_v1_all.tre # git 20.24 Rscript --vanilla R/fig/plot_SF18.R \\ 2_analysis/summaries/fst_outliers_998.tsv # git 20.25 Rscript --vanilla R/fig/plot_SF19.R \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/geva/ \\ 2_analysis/GxP/bySNP/ # git 20.26 Rscript --vanilla R/fig/plot_SF20.R \\ 2_analysis/GxP/50000/ # git 20.27 Rscript --vanilla R/fig/plot_SF21.R \\ 2_analysis/pi/50k/ # ------------------------------------------------- rm Rplots.pdf "],
["figure-1.html", "22 Figure 1 22.1 Summary 22.2 Details of plot_F1.R", " 22 Figure 1 22.1 Summary This is the accessory documentation of Figure 1. The Figure can be recreated by running the R script plot_F1.R from a (bash terminal): cd $BASE_DIR Rscript --vanilla R/fig/plot_F1.R \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/summaries/fst_permutation_summary.tsv \\ 2_analysis/fotl/concat_R24ed.treefile 22.2 Details of plot_F1.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, BAMMtools and on the package hypoimg. 22.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute interactively or debug the script: #!/usr/bin/env Rscript # # Context: this script depends on the input file 2_analysis/summaries/fst_permutation_summary.tsv # which is created by R/fst_permutation.R # # run from terminal: # Rscript --vanilla R/fig/plot_F1.R \\ # 2_analysis/fst/50k/ \\ # 2_analysis/summaries/fst_globals.txt \\ # 2_analysis/summaries/fst_permutation_summary.tsv \\ # 2_analysis/fotl/concat_R24ed.treefile # =============================================================== # This script produces Figure 1 of the study &quot;Rapid radiation in a highly # diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/fst/50k/&#39;, &#39;2_analysis/summaries/fst_globals.txt&#39;, # &#39;2_analysis/summaries/fst_permutation_summary.tsv&#39;, # &quot;2_analysis/fotl/concat_R24ed.treefile&quot;) # script_name &lt;- &quot;R/fig/plot_F1.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(patchwork) library(ape) library(ggraph) library(tidygraph) library(stringr) library(ggtree) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F1.R ────────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/fst/50k/ #&gt; ★ 2: 2_analysis/summaries/fst_globals.txt #&gt; ★ 3: 2_analysis/summaries/fst_permutation_summary.tsv #&gt; ★ 4: 2_analysis/fotl/concat_R24ed.treefile #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we set a few parameters for the plot layout: # config ----------------------- fst_dir &lt;- as.character(args[1]) fst_globals &lt;- as.character(args[2]) fst_permutation_file &lt;- as.character(args[3]) tree_file &lt;- as.character(args[4]) wdh &lt;- .3 # The width of the boxplots scaler &lt;- 20 # the ratio of the Fst and the dxy axis (legacy - not really needed anymore) clr_sec &lt;- &#39;gray&#39; # the color of the secondary axis (dxy) 22.2.2 Actual Script Start # start script ------------------- tree &lt;- read.tree(tree_file) tree_rooted &lt;- root(phy = tree, outgroup = &quot;Epinephelus_maculatus&quot;) clr_neutral &lt;- rgb(.2, .2, .2) clr_highlight &lt;- &quot;gray40&quot; #&quot;#FF8029&quot; clr_stars &lt;- &quot;firebrick&quot; ### Edit tip labels tree_rooted$tip.label &lt;- tree_rooted$tip.label %&gt;% str_replace(pattern = &quot;20864abehon&quot;, &quot;Hypoplectrus_aberrans&quot;) %&gt;% str_replace(pattern = &quot;20642gumhon&quot;, &quot;Hypoplectrus_gummigutta&quot;) %&gt;% str_replace(pattern = &quot;18238indbel&quot;, &quot;Hypoplectrus_indigo&quot;) %&gt;% str_replace(pattern = &quot;PL17_122maybel&quot;, &quot;Hypoplectrus_maya&quot;) %&gt;% str_replace(pattern = &quot;18906nigpan&quot;, &quot;Hypoplectrus_nigricans&quot;) %&gt;% str_replace(pattern = &quot;18434puepan&quot;, &quot;Hypoplectrus_puella&quot;) %&gt;% str_replace(pattern = &quot;20613ranhon&quot;, &quot;Hypoplectrus_randallorum&quot;) %&gt;% str_replace(pattern = &quot;18448unipan&quot;, &quot;Hypoplectrus_unicolor&quot;) %&gt;% str_replace(pattern = &quot;PL17_160floflo&quot;, &quot;Hypoplectrus_floridae&quot;) %&gt;% str_replace(pattern = &quot;20478tabhon&quot;, &quot;Serranus_tabacarius&quot;) %&gt;% str_replace(pattern = &quot;s_tort_3torpan&quot;, &quot;Serranus_tortugarum&quot;) %&gt;% # str_replace(pattern = &quot;([A-Z])([a-z])[a-z]*_([a-z]*)&quot;, &quot;\\\\1\\\\2. \\\\3&quot;) %&gt;% str_replace(pattern = &quot;Ce.&quot;, &quot;Cp.&quot;)%&gt;% str_replace(pattern = &quot;Za.&quot;, &quot;Pl.&quot;) %&gt;% str_replace(pattern = &quot;Hy.&quot;, &quot;H.&quot;) %&gt;% str_replace(pattern = &quot;Di.&quot;, &quot;D.&quot;) %&gt;% str_replace(pattern = &quot;Ep.&quot;, &quot;E.&quot;) ### Prepare tree, categorize support values and define group tree_plus &lt;- ggtree(tree_rooted, layout = &quot;rectangular&quot;, ladderize = TRUE, right = TRUE) %&gt;% #flip(25,26) %&gt;% .$data %&gt;% mutate(support = as.numeric(label), support_class = cut(support, c(0,50,70,90,100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;))) hamlets &lt;- tree_plus$label[grepl(pattern = &quot;H. &quot;, tree_plus$label)] tree_plus &lt;- tree_plus %&gt;% groupOTU(.node = hamlets) ### Define groups our_taxa &lt;- c(&quot;H. aberrans&quot;, &quot;H. gummigutta&quot;, &quot;H. indigo&quot;, &quot;H. maya&quot;, &quot;H. nigricans&quot;, &quot;H. puella&quot;, &quot;H. randallorum&quot;, &quot;H. unicolor&quot;, &quot;H. floridae&quot;, &quot;Se. tabacarius&quot;, &quot;Se. tortugarum&quot;) hermaphrodites &lt;- tree_plus %&gt;% filter(node %in% c(60, 63)) %&gt;% mutate(x = if_else(node == 60, x - branch.length, x - branch.length *.5), y = if_else(node == 60, .5 * (y + tree_plus$y[tree_plus$node == 61]), y), star = &quot;\\U2605&quot;) c1 &lt;- &quot;transparent&quot; c2 &lt;- prismatic::clr_alpha(clr_highlight, .1) c3 &lt;- prismatic::clr_alpha(clr_highlight, .2) grad_mat &lt;- c(c1, c2, c3, c3) dim(grad_mat) &lt;- c(1, length(grad_mat)) grob_grad &lt;- rasterGrob(grad_mat, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;), interpolate = TRUE) blank_hamlet &lt;- hypoimg::hypo_outline %&gt;% ggplot()+ coord_equal()+ geom_polygon(aes(x, y), color = rgb(0,0,0,.3), fill = rgb(1, 1, 1, .3), size = .1)+ theme_void() ### Draw tree (p_tree &lt;- ggtree(tree_plus, aes(color = group), size = .2) + annotation_custom(grob = grob_grad, ymin = .2, ymax = 12.5, xmin = .04, xmax = .125)+ annotation_custom(grob = ggplotGrob(blank_hamlet), xmin = 0.09, xmax = .125, ymin = 1.2, ymax = 10.5) + geom_tiplab(aes(color = group, label = if_else(label %in% our_taxa, str_c(label,&quot;*&quot;), label)), # add asterisks to our taxa size = 1.3, hjust = -.1, fontface = &#39;italic&#39;) + ggplot2::xlim(0, 0.12) + # add extra space for long labels geom_nodepoint(data = tree_plus %&gt;% filter(!is.na(support_class)), aes(fill = support_class, size = support_class), shape = 21, color = clr_neutral) + geom_text(data = hermaphrodites, aes(x = x, y = y, label = star), family = &quot;DejaVu Sans&quot;, color = clr_stars, size = 2, vjust = .35) + scale_color_manual(values = c(clr_neutral, clr_neutral, clr_highlight)) + scale_fill_manual(values = c(`(0,50]` = &quot;transparent&quot;, `(50,70]` = &quot;white&quot;, `(70,90]` = &quot;gray&quot;, `(90,100]` = &quot;black&quot;), drop = FALSE) + scale_size_manual(values = c(`(0,50]` = 0, `(50,70]` = .8, `(70,90]` = .8, `(90,100]` = .8), na.value = 0, drop = FALSE) + geom_treescale(color = clr_neutral, fontsize = 2, linesize = .2, x = 0.045, y = 1.5) + guides(fill = guide_legend(title = &quot;Node Support Class&quot;, title.position = &quot;top&quot;, override.aes = list(color = clr_neutral), nrow = 2), size = guide_legend(title = &quot;Node Support Class&quot;, title.position = &quot;top&quot;, override.aes = list(color = clr_neutral), nrow = 2), color = &#39;none&#39;) + coord_cartesian(xlim = c(-.005,.125), ylim = c(0,45), expand = 0) + theme_void() + theme(legend.position = c(0.05,0), legend.justification = c(0,0), legend.title.align = 0, legend.key.height = unit(8,&quot;pt&quot;), legend.key.width = unit(6,&quot;pt&quot;), legend.text = element_text(color = clr_neutral), legend.title = element_text(color = clr_neutral)) ) globals &lt;- vroom::vroom(fst_globals, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% mutate(run = str_c(str_sub(run,1,3),loc,&#39;-&#39;,str_sub(run,5,7),loc), run = fct_reorder(run,weighted)) # sort run by average genome wide Fst run_ord &lt;- tibble(run = levels(globals$run), run_ord = seq_along(levels(globals$run))) # load fst permutation results fst_sig_attach &lt;- read_tsv(fst_permutation_file) %&gt;% filter( subset_type == &quot;whg&quot; ) %&gt;% mutate(loc = str_sub(run, -3, -1)) %&gt;% group_by(loc) %&gt;% mutate(loc_n = 28,#length(loc), fdr_correction_factor = sum(1 / 1:loc_n), fdr_alpha = .05 / fdr_correction_factor, is_sig = p_perm &gt; fdr_alpha) %&gt;% ungroup() # create network annotation # underlying structure for the network plots networx &lt;- tibble( loc = c(&#39;bel&#39;,&#39;hon&#39;, &#39;pan&#39;), n = c(5, 6, 3), label = list(str_c(c(&#39;ind&#39;,&#39;may&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;),&#39;bel&#39;), str_c(c(&#39;abe&#39;,&#39;gum&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;ran&#39;,&#39;uni&#39;),&#39;hon&#39;), str_c(c(&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;),&#39;pan&#39;)), weight = c(1,1.45,1)) %&gt;% purrr::pmap_dfr(network_layout) %&gt;% mutate(edges = map(edges, function(x){x %&gt;% left_join(globals,by = &quot;run&quot;) })) # plot the individual networks by location plot_list &lt;- networx %&gt;% purrr::pmap(plot_network, node_lab_shift = .2) # combine the networks into a single grob p_net &lt;- cowplot::plot_grid( plot_list[[1]] + theme(legend.position = &quot;none&quot;), plot_list[[2]] + theme(legend.position = &quot;none&quot;), plot_list[[3]] + theme(legend.position = &quot;none&quot;), ncol = 3) %&gt;% cowplot::as_grob() p2 &lt;- globals %&gt;% left_join(fst_sig_attach %&gt;% mutate(run = factor(run, levels = levels(globals$run))) ) %&gt;% ggplot(aes(color = loc)) + geom_bar(aes(x = as.numeric(run), y = weighted, alpha = is_sig, fill = after_scale(clr_lighten(color))), stat = &quot;identity&quot;,size = .2, width = .8)+ annotation_custom(p_net, ymin = .05, xmax = 24.5) + scale_x_continuous(name = &quot;Pair of sympatric species&quot;, breaks = 1:28) + scale_y_continuous(name = expression(italic(F[ST])))+ scale_color_manual(values = c(make_faint_clr(&#39;bel&#39;), make_faint_clr(&#39;hon&#39;), make_faint_clr(&#39;pan&#39;))[c(2, 4, 6)])+ scale_shape_manual(values = c(`TRUE` = 1, `FALSE` = 21)) + scale_alpha_manual(values = c(`TRUE` = .15, `FALSE` = 1)) + coord_cartesian(xlim = c(0,29), expand = c(0,0))+ theme_minimal()+ theme(text = element_text(size = plot_text_size), legend.position = &#39;none&#39;, strip.placement = &#39;outside&#39;, strip.text = element_text(size = 12), panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank(), axis.text.y.right = element_text(color = clr_sec), axis.title.y.right = element_text(color = clr_sec)) # assemble panel c-e clr_alt &lt;- clr clr_alt[[&quot;uni&quot;]] &lt;- &quot;lightgray&quot; pca_fish_scale &lt;- 1.15 pca_fish_pos &lt;- tibble(pop = GenomicOriginsScripts::pop_levels, short = str_sub(pop, 1, 3), loc = str_sub(pop, 4, 6), width = c(bel = .08, hon =.08, pan = .09)[loc] * pca_fish_scale, height = c(bel = .08, hon =.08, pan = .09)[loc] * pca_fish_scale) %&gt;% arrange(loc) %&gt;% mutate(x = c(-.18, -.01, .03, -.03, .075, -.04, -.2, -.02, -.01, -.02, -.01, -.15, 0, .05), y = c(.02, .27, -.13, -.03, .05, -.1, .05, 0, .075, -.23, .2, .06, -.2, .2)) %&gt;% select(-pop) %&gt;% group_by(loc) %&gt;% nest() pcas &lt;- c(&quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) %&gt;% map(pca_plot) fish_tib &lt;- tibble(short = names(clr)[!names(clr) %in% c(&quot;flo&quot;, &quot;tab&quot;, &quot;tor&quot;)], x = c(0.5, 3.5, 7, 9.7, 12.25, 15.25, 18, 21.5) ) key_sz &lt;- .75 p_leg &lt;- fish_tib %&gt;% ggplot() + coord_equal(xlim = c(-.05, 24), expand = 0) + geom_tile(aes(x = x, y = 0, fill = short, color = after_scale(prismatic::clr_darken(fill, .25))), width = key_sz, height = key_sz, size = .3) + geom_text(aes(x = x + .6, y = 0, label = str_c(&quot;H. &quot;, sp_names[short])), hjust = 0, fontface = &quot;italic&quot;, size = plot_text_size / ggplot2:::.pt) + pmap(fish_tib, plot_fish_lwd, width = 1, height = 1, y = 0) + scale_fill_manual(values = clr, guide = FALSE) + theme_void() p_combined &lt;- ((wrap_elements(plot = p_tree + theme(axis.title = element_blank(), text = element_text(size = plot_text_size)), clip = FALSE) + p2 ) / (pcas %&gt;% wrap_plots()) + plot_layout(heights = c(1,.75)) + plot_annotation(tag_levels = &#39;a&#39;) &amp; theme(text = element_text(size = plot_text_size), plot.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;))) p_done &lt;- cowplot::plot_grid(p_combined, p_leg, ncol = 1, rel_heights = c(1,.06)) Finally, we can export Figure 1. scl &lt;- .75 hypo_save(p_done, filename = &#39;figures/F1.pdf&#39;, width = 9 * scl, height = 6 * scl, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["figure-2.html", "23 Figure 2 23.1 Summary 23.2 Details of plot_F2.R", " 23 Figure 2 23.1 Summary This is the accessory documentation of Figure 2. The Figure can be recreated by running the R script plot_F2.R from a (bash terminal): cd $BASE_DIR Rscript --vanilla R/fig/plot_F2.R \\ 2_analysis/fst/50k/multi_fst.50k.tsv.gz \\ 2_analysis/GxP/50000/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/twisst/weights/ \\ ressources/plugin/trees/ \\ 2_analysis/summaries/fst_globals.txt 23.2 Details of plot_F2.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, BAMMtools and on the package hypoimg. 23.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute interactively or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F2.R \\ # 2_analysis/fst/50k/multi_fst.50k.tsv.gz \\ # 2_analysis/GxP/50000/ \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ # 2_analysis/twisst/weights/ \\ # ressources/plugin/trees/ \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Figure 2 of the study &quot;Rapid radiation in a highly # diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/fst/50k/multi_fst.50k.tsv.gz&#39;, # &#39;2_analysis/GxP/50000/&#39;, &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R&#39;, # &#39;2_analysis/twisst/weights/&#39;, &#39;ressources/plugin/trees/&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_F2.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F2.R ────────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/fst/50k/multi_fst.50k.tsv.gz #&gt; ★ 2: 2_analysis/GxP/50000/ #&gt; ★ 3: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 4: https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R #&gt; ★ 5: 2_analysis/twisst/weights/ #&gt; ★ 6: ressources/plugin/trees/ #&gt; ★ 7: 2_analysis/summaries/fst_globals.txt #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we set a few parameters for the plot layout: # config ----------------------- fst_file &lt;- as.character(args[1]) gxp_dir &lt;- as.character(args[2]) outlier_table &lt;- as.character(args[3]) twisst_script &lt;- as.character(args[4]) w_path &lt;- as.character(args[5]) d_path &lt;- as.character(args[6]) global_fst_file &lt;- as.character(args[7]) source(twisst_script) 23.2.2 Actual Script Start # start script ------------------- # import fst data fst_data &lt;- vroom::vroom(fst_file, delim = &#39;\\t&#39;) %&gt;% select(CHROM, BIN_START, BIN_END, N_VARIANTS, WEIGHTED_FST) %&gt;% setNames(., nm = c(&#39;CHROM&#39;, &#39;BIN_START&#39;, &#39;BIN_END&#39;, &#39;n_snps&#39;, &#39;fst&#39;) ) %&gt;% add_gpos() %&gt;% select(GPOS, fst) %&gt;% setNames(., nm = c(&#39;GPOS&#39;,&#39;value&#39;)) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;a&#39;),&#39;):joint~italic(F[ST])&#39;)) # set G x P traits to be imported traits &lt;- c(&quot;Bars.lm.50k.5k.txt.gz&quot;, &quot;Peduncle.lm.50k.5k.txt.gz&quot;, &quot;Snout.lm.50k.5k.txt.gz&quot;) # set trait figure panels trait_panels &lt;- c(Bars = str_c(&#39;bold(&#39;,project_case(&#39;d&#39;),&#39;)&#39;), Peduncle = str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;)&#39;), Snout = str_c(&#39;bold(&#39;,project_case(&#39;f&#39;),&#39;)&#39;)) # import G x P data gxp_data &lt;- str_c(gxp_dir,traits) %&gt;% purrr::map(get_gxp) %&gt;% join_list() %&gt;% gather(key = &#39;window&#39;, value = &#39;value&#39;,2:4) # import genome wide Fst data summary -------- globals &lt;- vroom::vroom(global_fst_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% mutate(run = str_c(str_sub(run,1,3),loc,&#39;-&#39;,str_sub(run,5,7),loc), run = fct_reorder(run,weighted)) # dxy and pi are only shown for one exemplary population (/pair) # select dxy pair run (15 is one of the two central runs of the 28 pairs) # here, the 15th lowest fst value is identified as &quot;selector&quot; selectors_dxy &lt;- globals %&gt;% arrange(weighted) %&gt;% .$weighted %&gt;% .[15] # import topology weighting data twisst_data &lt;- tibble(loc = c(&#39;bel&#39;,&#39;hon&#39;), panel = c(&#39;b&#39;,&#39;c&#39;) %&gt;% project_case() %&gt;% str_c(&#39;bold(&#39;,.,&#39;)&#39;)) %&gt;% purrr::pmap(match_twisst_files) %&gt;% bind_rows() %&gt;% select(GPOS, topo3,topo_rel,window,weight) # the &quot;null-weighting&quot; is computed for both locations twisst_null &lt;- tibble(window = c(str_c(&#39;bold(&#39;,project_case(&#39;b&#39;),&#39;):~italic(w)[bel]&#39;), str_c(&#39;bold(&#39;,project_case(&#39;c&#39;),&#39;):~italic(w)[hon]&#39;)), weight = c(1/15, 1/105)) # combine data types -------- data &lt;- bind_rows(fst_data, gxp_data) # import fst outliers outliers &lt;- vroom::vroom(outlier_table, delim = &#39;\\t&#39;) # the focal outlier IDs are set outlier_pick &lt;- c(&#39;LG04_1&#39;, &#39;LG12_3&#39;, &#39;LG12_4&#39;) # the table for the outlier labels is created outlier_label &lt;- outliers %&gt;% filter(gid %in% outlier_pick) %&gt;% mutate(label = letters[row_number()] %&gt;% project_inv_case(), x_shift_label = c(-1,-1.2,1)*10^7, gpos_label = gpos + x_shift_label, gpos_label2 = gpos_label - sign(x_shift_label) *.5*10^7, window = str_c(&#39;bold(&#39;,project_case(&#39;a&#39;),&#39;):joint~italic(F[ST])&#39;)) # the y height of the outlier labels and the corresponding tags is set outlier_y &lt;- .45 outlier_yend &lt;- .475 # the icons for the traits of the GxP are loaded trait_tibble &lt;- tibble(window = c(&quot;bold(d):italic(p)[Bars]&quot;, &quot;bold(e):italic(p)[Peduncle]&quot;, &quot;bold(f):italic(p)[Snout]&quot;), grob = hypo_trait_img$grob_circle[hypo_trait_img$trait %in% c(&#39;Bars&#39;, &#39;Peduncle&#39;, &#39;Snout&#39;)]) # finally, the figure is being put together p_done &lt;- ggplot()+ # add gray/white LGs background geom_hypo_LG()+ # the red highlights for the outlier regions are added geom_vline(data = outliers, aes(xintercept = gpos), color = outlr_clr)+ # the tags of the outlier labels are added geom_segment(data = outlier_label, aes(x = gpos, xend = gpos_label2, y = outlier_y, yend = outlier_yend), color = alpha(outlr_clr,1), size = .2)+ # the outlier labels are added geom_text(data = outlier_label, aes(x = gpos_label, y = outlier_yend, label = label), color = alpha(outlr_clr,1), fontface = &#39;bold&#39;, size = plot_text_size / ggplot2:::.pt)+ # the fst, delta dxy and gxp data is plotted geom_point(data = data, aes(x = GPOS, y = value),size = plot_size, color = plot_clr) + # the topology weighting data is plotted geom_line(data = twisst_data, aes(x = GPOS, y = weight, color = topo_rel), size = .4) + # the null weighting is added geom_hline(data = twisst_null, aes(yintercept = weight), color = rgb(1, 1, 1, .5), size = .4) + # the trait icons are added geom_hypo_grob(data = trait_tibble, aes(grob = grob, angle = 0, height = .65), inherit.aes = FALSE, x = .95, y = 0.65)+ # setting the scales scale_fill_hypo_LG_bg() + scale_x_hypo_LG()+ scale_color_gradient( low = &quot;#f0a830ff&quot;, high = &quot;#084082ff&quot;, guide = FALSE)+ # organizing the plot across panels facet_grid(window~.,scales = &#39;free&#39;,switch = &#39;y&#39;, labeller = label_parsed)+ # tweak plot appreance theme_hypo()+ theme(text = element_text(size = plot_text_size), legend.position = &#39;bottom&#39;, axis.title = element_blank(), strip.text = element_text(size = plot_text_size), strip.background = element_blank(), strip.placement = &#39;outside&#39;) Finally, we can export Figure 2. hypo_save(p_done, filename = &#39;figures/F2.png&#39;, width = f_width, height = f_width * .5, dpi = 600, type = &quot;cairo&quot;, comment = plot_comment) system(&quot;convert figures/F2.png figures/F2.pdf&quot;) system(&quot;rm figures/F2.png&quot;) create_metadata &lt;- str_c(&quot;exiftool -overwrite_original -Description=\\&quot;&quot;, plot_comment, &quot;\\&quot; figures/F2.pdf&quot;) system(create_metadata) "],
["figure-3.html", "24 Figure 3 24.1 Summary 24.2 Details of plot_F3.R", " 24 Figure 3 24.1 Summary This is the accessory documentation of Figure 3. The Figure can be recreated by running the R script plot_F3.R from a (bash terminal): cd $BASE_DIR Rscript --vanilla R/fig/plot_F3.R \\ 2_analysis/msmc/output/ \\ 2_analysis/cross_coalescence/output/ \\ 2_analysis/msmc/setup/msmc_grouping.txt \\ 2_analysis/msmc/setup/msmc_cc_grouping.txt \\ 2_analysis/summaries/fst_globals.txt 24.2 Details of plot_F3.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, BAMMtools and on the package hypoimg. 24.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute interactively or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F3.R \\ # 2_analysis/msmc/output/ \\ # 2_analysis/cross_coalescence/output/ \\ # 2_analysis/msmc/setup/msmc_grouping.txt \\ # 2_analysis/msmc/setup/msmc_cc_grouping.txt \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Figure 3 of the study &quot;Rapid radiation in a highly # diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/msmc/output/&#39;, &#39;2_analysis/cross_coalescence/output/&#39;, # &#39;2_analysis/msmc/setup/msmc_grouping.txt&#39;, &#39;2_analysis/msmc/setup/msmc_cc_grouping.txt&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_F3.R&quot; # ---------------------------------------- args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F3.R ────────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/msmc/output/ #&gt; ★ 2: 2_analysis/cross_coalescence/output/ #&gt; ★ 3: 2_analysis/msmc/setup/msmc_grouping.txt #&gt; ★ 4: 2_analysis/msmc/setup/msmc_cc_grouping.txt #&gt; ★ 5: 2_analysis/summaries/fst_globals.txt #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we set a few parameters for the plot layout: # config ----------------------- msmc_path &lt;- as.character(args[1]) cc_path &lt;- as.character(args[2]) msmc_group_file &lt;- as.character(args[3]) cc_group_file &lt;- as.character(args[4]) fst_globals_file &lt;- as.character(args[5]) 24.2.2 Actual Script Start # actual script ========================================================= msmc_groups &lt;- read_tsv(msmc_group_file) cc_groups &lt;- read_tsv(cc_group_file) fst_globals &lt;- vroom::vroom(fst_globals_file,delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run_prep&#39;,&#39;mean_fst&#39;,&#39;weighted_fst&#39;)) %&gt;% separate(run_prep,into = c(&#39;pop1&#39;,&#39;pop2&#39;),sep = &#39;-&#39;) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;,pop2,loc), run = fct_reorder(run,weighted_fst)) # locate cross-coalescence results msmc_files &lt;- dir(msmc_path, pattern = &#39;.final.txt.gz&#39;) cc_files &lt;- dir(cc_path, pattern = &#39;.final.txt.gz&#39;) # import effective population size data msmc_data &lt;- msmc_files %&gt;% map_dfr(.f = get_msmc, msmc_path = msmc_path) # import cross-coalescence data cc_data &lt;- cc_files %&gt;% map_dfr(get_cc, cc_groups = cc_groups, cc_path = cc_path) %&gt;% mutate( run = factor(run, levels = levels(fst_globals$run))) # color adjustments for line plots (replace white by gray) clr_alt &lt;- clr[!(names(clr) %in% c(&quot;flo&quot;,&quot;tor&quot;,&quot;tab&quot;))] clr_alt[&#39;uni&#39;] &lt;- rgb(.8,.8,.8) clr_ticks &lt;- &#39;lightgray&#39; p_msmc &lt;- msmc_data %&gt;% # remove the two first and last time segments filter(!time_index %in% c(0:2,29:31)) %&gt;% ggplot( aes(x=YBP, y=Ne, group = run_nr, colour = spec)) + # add guides for the logarithmic axes annotation_logticks(sides=&quot;tl&quot;, color = clr_ticks, size = plot_lwd) + # add the msmc data as lines geom_line(size = .3)+ # set the color scheme scale_color_manual(NULL, values = clr_alt, label = sp_labs) + # format the x axis scale_x_log10(expand = c(0,0), breaks = c(10^3, 10^4, 10^5), position = &#39;top&#39;, labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)) #labels = c(&quot;1-3 kya&quot;, &quot;10-30 kya&quot;, &quot;100-300 kya&quot;), #name = &quot;Years Before Present&quot; ) + # format the y axis scale_y_log10(labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)), breaks = c(10^3,10^4,10^5,10^6)) + # format the color legend guides(colour = guide_legend(title.position = &quot;top&quot;, override.aes = list(alpha = 1, size=1), nrow = 3, keywidth = unit(7, &quot;pt&quot;), byrow = TRUE)) + # set the axis titles labs(x = &quot;Generations Before Present&quot;, y = expression(Effective~Population~Size~(italic(N[e])))) + # set plot range coord_cartesian(xlim = c(250, 5*10^5)) + # tune plot appreance theme_minimal()+ theme(text = element_text(size = plot_text_size), axis.ticks = element_line(colour = clr_ticks), legend.position = c(1.05,-.175), legend.justification = c(1,0), legend.text.align = 0, panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(), title = element_text(face = &#39;bold&#39;), legend.spacing.y = unit(-5,&quot;pt&quot;), legend.spacing.x = unit(3, &quot;pt&quot;), axis.title = element_text(face = &#39;plain&#39;), legend.title = element_text(face = &#39;plain&#39;)) p_cc &lt;- cc_data %&gt;% # remove the two first and last time segments filter( !time_index %in% c(0:2,29:31)) %&gt;% arrange(run_nr) %&gt;% # attach fst data left_join(fst_globals %&gt;% select(run, weighted_fst)) %&gt;% ggplot(aes(x = YBP, y = Cross_coal, group = run_nr, color = weighted_fst)) + # add guides for the logarithmic axis annotation_logticks(sides=&quot;b&quot;, color = clr_ticks, size = plot_lwd) + # add the msmc data as lines geom_line(alpha = 0.2, size = .3)+ # set the color scheme scale_color_gradientn(name = expression(Global~weighted~italic(F[ST])), colours = hypogen::hypo_clr_LGs[1:24])+ # format the x axis scale_x_log10(expand = c(0,0), labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x))) + # set the axis titles guides(color = guide_colorbar(barheight = unit(3, &#39;pt&#39;), barwidth = unit(110, &#39;pt&#39;), title.position = &#39;top&#39; )) + # set the axis titles labs(x = &quot;Generations Before Present&quot;, y = &#39;Cross-coalescence Rate&#39;) + # set plot range coord_cartesian(xlim = c(250, 5*10^5)) + # tune plot appreance theme_minimal()+ theme(text = element_text(size = plot_text_size), axis.ticks = element_line(colour = clr_ticks), legend.position = c(1,.03), legend.direction = &#39;horizontal&#39;, legend.justification = c(1,0), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(), title = element_text(face = &#39;bold&#39;), axis.title = element_text(face = &#39;plain&#39;), axis.title.x = element_blank(), axis.text.x = element_blank(), legend.title = element_text(face = &#39;plain&#39;)) # combine panels a and b p_done &lt;- p_msmc / p_cc + plot_annotation(tag_levels = c(&#39;a&#39;)) &amp; theme(legend.text = element_text(size = plot_text_size_small), legend.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = &quot;pt&quot;), panel.grid.major = element_line(size = plot_lwd), axis.ticks.x = element_blank(), panel.background = element_blank(), plot.background = element_blank()) Finally, we can export Figure 3. # export figure 3 hypo_save(plot = p_done, filename = &#39;figures/F3.pdf&#39;, width = f_width_half, height = f_width_half * .95, comment = plot_comment, bg = &quot;transparent&quot;, device = cairo_pdf) "],
["figure-4.html", "25 Figure 4 25.1 Summary 25.2 Details of plot_F4.R", " 25 Figure 4 25.1 Summary This is the accessory documentation of Figure 4. The Figure can be recreated by running the R script plot_F4.R from a (bash terminal): cd $BASE_DIR Rscript --vanilla R/fig/plot_F4.R \\ 2_analysis/astral/astral_5000x_5kb_v1_noS.tre \\ 2_analysis/ibd/cM_converted/no_outgr_bed95_8.conv_filterd.tsv 25.2 Details of plot_F4.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, BAMMtools and on the package hypoimg. 25.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute interactively or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F4.R \\ # 2_analysis/astral/astral_5000x_5kb_v1_noS.tre \\ # 2_analysis/ibd/cM_converted/no_outgr_bed95_8.conv_filterd.tsv # =============================================================== # This script produces Figure 4 of the study &quot;Rapid radiation in a highly # diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/astral/astral_5000x_5kb_v1_noS.tre&quot;, # &quot;2_analysis/ibd/cM_converted/no_outgr_bed95_8.conv_filterd.tsv&quot;) # script_name &lt;- &quot;R/fig/plot_F4.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ape) library(ggtree) library(tidygraph) library(ggraph) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F4.R ────────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/astral/astral_5000x_5kb_v1_noS.tre #&gt; ★ 2: 2_analysis/ibd/cM_converted/no_outgr_bed95_8.conv_filterd.tsv #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we set a few parameters for the plot layout: # config ----------------------- tree_hypo_file &lt;- as.character(args[1]) ibd_file &lt;- as.character(args[2]) 25.2.2 Actual Script Start tree &lt;- read.tree(tree_hypo_file) tree$edge.length &lt;- replace(tree$edge.length, tree$edge.length == &quot;NaN&quot;, 0.05) # Set terminal branches to 0.05 tree_rooted &lt;- root(phy = tree, outgroup = &quot;PL17_160floflo&quot;) clr_neutral &lt;- rgb(.2, .2, .2) ### Prepare tree and categorize support values tree_plus &lt;- ggtree(tree_rooted) %&gt;% ggtree::rotate(node = 205) %&gt;% .$data %&gt;% mutate(spec = ifelse(isTip, str_sub(label, -6, -4), &quot;ungrouped&quot;), loc = ifelse(isTip, str_sub(label, -3, -1), &quot;ungrouped&quot;), support = as.numeric(label) * 100, support_class = cut(support, c(0,50,70,90,100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;)) ) t_plot &lt;- (ggtree(tree_plus, layout = &#39;fan&#39;, aes(color = spec), size = .2) %&gt;% ggtree::rotate_tree(angle = -100)) + geom_tippoint(aes(color = spec, shape = loc, fill = after_scale(color)), size = .5) + geom_nodepoint(data = tree_plus %&gt;% filter(!isTip, support_class != &quot;(0,50]&quot;), # Apply to nodes with support &gt;50 only aes(fill = support_class, size = support_class), shape = 21, color = clr_neutral) + scale_color_manual(values = c(GenomicOriginsScripts::clr2, ungrouped = &quot;gray60&quot;), guide = &#39;none&#39;) + scale_shape_manual(values = c(bel = 21, flo = 24, hon = 22, pan = 23), labels = GenomicOriginsScripts::loc_names, guide = &#39;none&#39;) + scale_fill_manual(values = c(`(0,50]` = &quot;transparent&quot;, `(50,70]` = &quot;white&quot;, `(70,90]` = &quot;gray&quot;, `(90,100]` = &quot;black&quot;), drop = FALSE) + scale_size_manual(values = c(`(0,50]` = 0, `(50,70]` = .8, `(70,90]` = .8, `(90,100]` = .8), na.value = 0, drop = FALSE) + # Add scale bar: ggtree::geom_treescale(width = .05, x = .13, y = 130, offset = -1, linesize = .2, fontsize = plot_text_size/.pt, color = clr_neutral) + scale_x_continuous(limits = c(-.05, .26), expand = c(0,0)) + guides(fill = guide_legend(title = &quot;Node Support Class&quot;, title.position = &quot;top&quot;, nrow = 2), size = guide_legend(title = &quot;Node Support Class&quot;, title.position = &quot;top&quot;, nrow = 2)) + theme_void() + theme(legend.title.align = 0.5, legend.text = element_text(color = &quot;gray20&quot;), legend.title = element_text(color = &quot;gray20&quot;)) y_sep &lt;- .55 x_shift &lt;- .5 p1 &lt;- ggplot() + coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = 0) + annotation_custom(grob = ggplotGrob(t_plot + theme(legend.position = &quot;none&quot;)), ymin = -.15 - y_sep , ymax = 1 + y_sep, xmin = .25 - x_shift, xmax = 1 + x_shift) + theme_void() data_ibd &lt;- read_tsv(ibd_file) %&gt;% mutate(ibd_total = (ibd2_cM_m1 + 0.5*ibd1_cM_m1) / (ibd0_cM_m1 + ibd1_cM_m1 + ibd2_cM_m1)) set.seed(42) p2 &lt;- data_ibd %&gt;% as_tbl_graph() %&gt;% mutate(spec = factor(str_sub(name, -6, -4), levels = names(clr)[-10:-9]), loc = factor(str_sub(name, -3, -1), levels = c(&quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;, &quot;flo&quot;))) %&gt;% ggraph( layout = &#39;fr&#39;, weights = ibd_total) + geom_edge_link(aes(alpha = ibd_total, edge_width = ibd_total), color = rgb(.1,.1,.1)) + geom_node_point(aes(fill = spec, shape = loc, color = after_scale(clr_darken(fill,.3))), size = 1.2) + scale_fill_manual(&quot;Species&quot;, values = GenomicOriginsScripts::clr2[!(names(GenomicOriginsScripts::clr2) %in% c( &quot;tor&quot;, &quot;tab&quot;))], labels = GenomicOriginsScripts::sp_labs, drop = FALSE)+ scale_edge_alpha_continuous(range = c(0, 1), guide = &quot;none&quot;) + scale_edge_width_continuous(range = c(.1, .4), guide = &quot;none&quot;) + scale_shape_manual(&quot;Site&quot;, values = c(bel = 21, flo = 24, hon = 22, pan = 23), labels = GenomicOriginsScripts::loc_names, drop = FALSE) + guides(fill = guide_legend(nrow = 2, override.aes = list(shape = 21, size = 2.5), title.position = &quot;top&quot;,label.hjust = 0), shape = guide_legend(nrow = 2, title.position = &quot;top&quot;)) + coord_fixed() + theme(text = element_text(size = GenomicOriginsScripts::plot_text_size), panel.background = element_blank()) p_done &lt;- cowplot::plot_grid(cowplot::plot_grid(p1, p2 + theme(legend.position = &quot;none&quot;), rel_widths = c(1,.9), labels = c(&quot;a&quot;, &quot;b&quot;), label_fontface = &quot;plain&quot;, label_size = plot_text_size), cowplot::plot_grid(cowplot::get_legend(t_plot + theme_minimal(base_size = GenomicOriginsScripts::plot_text_size) + theme(legend.key.width = unit(3,&quot;pt&quot;))), cowplot::get_legend(p2 + theme_minimal(base_size = GenomicOriginsScripts::plot_text_size) + theme(legend.position = &quot;bottom&quot;, legend.key.width = unit(3,&quot;pt&quot;))), rel_widths = c(.3,1)), rel_heights = c(1,.15), ncol = 1) Finally, we can export Figure 4. hypo_save(plot = p_done, filename = &quot;figures/F4.png&quot;, width = GenomicOriginsScripts::f_width, height = GenomicOriginsScripts::f_width * .65, bg = &quot;white&quot;, type = &quot;cairo&quot;, dpi = 600, comment = plot_comment) system(&quot;convert figures/F4.png figures/F4.pdf&quot;) system(&quot;rm figures/F4.png&quot;) create_metadata &lt;- str_c(&quot;exiftool -overwrite_original -Description=\\&quot;&quot;, plot_comment, &quot;\\&quot; figures/F4.pdf&quot;) system(create_metadata) "],
["figure-5.html", "26 Figure 5 26.1 Summary 26.2 Details of plot_F5.R", " 26 Figure 5 26.1 Summary This is the accessory documentation of Figure 5. The Figure can be recreated by running the R script plot_F5.R from a (bash terminal): cd $BASE_DIR Rscript --vanilla R/fig/plot_F5.R \\ 2_analysis/twisst/weights/ \\ ressources/plugin/trees/ \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/GxP/50000/ \\ 200 \\ 5 \\ 2_analysis/revPoMo/outlier_regions/ 26.2 Details of plot_F5.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, BAMMtools and on the package hypoimg. 26.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute interactively or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F5.R \\ # 2_analysis/twisst/weights/ \\ # ressources/plugin/trees/ \\ # https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # 2_analysis/dxy/50k/ \\ # 2_analysis/fst/50k/ \\ # 2_analysis/summaries/fst_globals.txt \\ # 2_analysis/GxP/50000/ \\ # 200 \\ # 5 \\ # 2_analysis/revPoMo/outlier_regions/ # =============================================================== # This script produces Figure 5 of the study &quot;Rapid radiation in a highly # diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/twisst/weights/&#39;, &#39;ressources/plugin/trees/&#39;, # &#39;https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R&#39;, # &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;2_analysis/dxy/50k/&#39;, &#39;2_analysis/fst/50k/&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;, # &#39;2_analysis/GxP/50000/&#39;, 200, 5, # &quot;2_analysis/revPoMo/outlier_regions/&quot;) # script_name &lt;- &quot;R/fig/plot_F5.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(furrr) library(ggtext) library(ape) library(ggtree) library(patchwork) library(phangorn) library(igraph) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F5.R ────────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/twisst/weights/ #&gt; ★ 2: ressources/plugin/trees/ #&gt; ★ 3: https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R #&gt; ★ 4: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 5: 2_analysis/dxy/50k/ #&gt; ★ 6: 2_analysis/fst/50k/ #&gt; ★ 7: 2_analysis/summaries/fst_globals.txt #&gt; ★ 8: 2_analysis/GxP/50000/ #&gt; ★ 9: 200 #&gt; ★ 10: 5 #&gt; ★ 11: 2_analysis/revPoMo/outlier_regions/ #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we set a few parameters for the plot layout: # config ----------------------- w_path &lt;- as.character(args[1]) d_path &lt;- as.character(args[2]) twisst_functions &lt;- as.character(args[3]) out_table &lt;- as.character(args[4]) dxy_dir &lt;- as.character(args[5]) fst_dir &lt;- as.character(args[6]) fst_globals &lt;- as.character(args[7]) gxp_dir &lt;- as.character(args[8]) twisst_size &lt;- as.numeric(args[9]) resolution &lt;- as.numeric(args[10]) pomo_path &lt;- as.character(args[11]) pomo_trees &lt;- dir(pomo_path, pattern = &quot;155_pop&quot;) source(twisst_functions, local = TRUE) plan(multiprocess) window_buffer &lt;- 2.5*10^5 26.2.2 Actual Script Start # actual script ========================================================= # locate dxy data files dxy_files &lt;- dir(dxy_dir, pattern = str_c(&#39;dxy.*[a-z]{3}.*.&#39;, resolution ,&#39;0kb-&#39;, resolution ,&#39;kb.tsv.gz&#39;)) # import dxy data dxy_data &lt;- tibble(file = str_c(dxy_dir, dxy_files)) %&gt;% purrr::pmap_dfr(get_dxy, kb = str_c(resolution, &#39;0kb&#39;)) # set traits of interest for GxP gxp_traits &lt;- c(&#39;Bars&#39;, &#39;Snout&#39;, &#39;Peduncle&#39;) # import GxP data gxp_data &lt;- str_c(gxp_dir,gxp_traits,&#39;.lm.&#39;, resolution ,&#39;0k.&#39;, resolution ,&#39;k.txt.gz&#39;) %&gt;% future_map_dfr(get_gxp_long, kb = 50) # set topology weighting color scheme twisst_clr &lt;- c(Blue = &quot;#0140E5&quot;, Bars = &quot;#E32210&quot;, Butter = &quot;#E4E42E&quot;) # set GxP color scheme gxp_clr &lt;- c(Bars = &quot;#79009f&quot;, Snout = &quot;#E48A00&quot;, Peduncle = &quot;#5B9E2D&quot;) %&gt;% darken(factor = .95) %&gt;% set_names(., nm = gxp_traits) # compute genome wide average dxy dxy_globals &lt;- dxy_data %&gt;% filter(BIN_START %% ( resolution * 10000 ) == 1 ) %&gt;% group_by( run ) %&gt;% summarise(mean_global_dxy = sum(dxy*N_SITES)/sum(N_SITES)) %&gt;% mutate(run = fct_reorder(run,mean_global_dxy)) # import genome wide average fst # and order population pairs by genomewide average fst fst_globals &lt;- vroom::vroom(fst_globals,delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run_prep&#39;,&#39;mean_fst&#39;,&#39;weighted_fst&#39;)) %&gt;% separate(run_prep,into = c(&#39;pop1&#39;,&#39;pop2&#39;),sep = &#39;-&#39;) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;, pop2, loc), run = fct_reorder(run,weighted_fst)) # locate fst data files fst_files &lt;- dir(fst_dir, pattern = str_c(&#39;.&#39;, resolution ,&#39;0k.windowed.weir.fst.gz&#39;)) # import fst data fst_data &lt;- str_c(fst_dir, fst_files) %&gt;% future_map_dfr(get_fst, kb = str_c(resolution, &#39;0k&#39;)) %&gt;% left_join(dxy_globals) %&gt;% left_join(fst_globals) %&gt;% mutate(run = refactor(., fst_globals), BIN_MID = (BIN_START+BIN_END)/2) # order dxy population pairs by genomewide average fst dxy_data &lt;- dxy_data %&gt;% left_join(dxy_globals) %&gt;% left_join(fst_globals) %&gt;% mutate(run = refactor(dxy_data, fst_globals), window = &#39;bold(italic(d[xy]))&#39;) # compute delta dxy data_dxy_summary &lt;- dxy_data %&gt;% group_by(GPOS) %&gt;% summarise(scaffold = CHROM[1], start = BIN_START[1], end = BIN_END[1], mid = BIN_MID[1], min_dxy = min(dxy), max_dxy = max(dxy), mean_dxy = mean(dxy), median_dxy = median(dxy), sd_dxy = sd(dxy), delta_dxy = max(dxy)-min(dxy)) # load fst outlier regions outlier_table &lt;- vroom::vroom(out_table, delim = &#39;\\t&#39;) %&gt;% setNames(., nm = c(&quot;outlier_id&quot;,&quot;lg&quot;, &quot;start&quot;, &quot;end&quot;, &quot;gstart&quot;,&quot;gend&quot;,&quot;gpos&quot;)) # set outlier regions of interest outlier_pick = c(&#39;LG04_1&#39;, &#39;LG12_3&#39;, &#39;LG12_4&#39;) # load group-level phylogeny data pomo_data &lt;- str_c(pomo_path, pomo_trees) %&gt;% purrr::map_dfr(import_tree) %&gt;% mutate(rooted_tree = map2(.x = tree,.y = type, .f = root_hamlets)) # select genes to label cool_genes &lt;- c(&#39;arl3&#39;,&#39;kif16b&#39;,&#39;cdx1&#39;,&#39;hmcn2&#39;, &#39;sox10&#39;,&#39;smarca4&#39;, &#39;rorb&#39;, &#39;alox12b&#39;,&#39;egr1&#39;, &#39;ube4b&#39;,&#39;casz1&#39;, &#39;hoxc8a&#39;,&#39;hoxc9&#39;,&#39;hoxc10a&#39;, &#39;hoxc13a&#39;,&#39;rarga&#39;,&#39;rarg&#39;, &#39;snai1&#39;,&#39;fam83d&#39;,&#39;mafb&#39;,&#39;sws2abeta&#39;,&#39;sws2aalpha&#39;,&#39;sws2b&#39;,&#39;lws&#39;,&#39;grm8&#39;) # load twisst data data_tables &lt;- list(bel = prep_data(loc = &#39;bel&#39;), hon = prep_data(loc = &#39;hon&#39;)) # set species sampled in belize pops_bel &lt;- c(&#39;ind&#39;, &#39;may&#39;, &#39;nig&#39;, &#39;pue&#39;, &#39;uni&#39;) # set outlier region label region_label_tibbles &lt;- tibble(outlier_id = outlier_pick, label = c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;)) # load and recolor trait icons trait_grob &lt;- tibble(svg = hypoimg::hypo_trait_img$grob_circle[hypoimg::hypo_trait_img$trait %in% gxp_traits], layer = c(4,3,7), color = gxp_clr[gxp_traits %&gt;% sort()])%&gt;% pmap(.f = hypo_recolor_svg) %&gt;% set_names(nm = gxp_traits %&gt;% sort()) # recolor second bars-layer trait_grob[[&quot;Bars&quot;]] &lt;- trait_grob[[&quot;Bars&quot;]] %&gt;% hypo_recolor_svg(layer = 7, color = gxp_clr[[&quot;Bars&quot;]]) p_pomo1 &lt;- pomo_data$tree[[1]] %&gt;% midpoint() %&gt;% ggtree::ggtree(layout = &quot;circular&quot;) %&gt;% rotate(node = 18) %&gt;% .$data %&gt;% dplyr::mutate(support = as.numeric(label), support_class = cut(support, c(0, 50, 70, 90, 100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;))) %&gt;% plot_tree(higl_node = 21, angle_in = 168, color = twisst_clr[[&quot;Blue&quot;]], xlim = c(-.015, .1), open_angle = 168) p_pomo2 &lt;- pomo_data$tree[[2]] %&gt;% midpoint() %&gt;% ggtree::ggtree(layout = &quot;circular&quot;) %&gt;% .$data %&gt;% dplyr::mutate(support = as.numeric(label), support_class = cut(support, c(0, 50, 70, 90, 100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;))) %&gt;% plot_tree(higl_node = 27, color = twisst_clr[[&quot;Bars&quot;]], xlim = c(-.015,.065), angle_in = 168, open_angle = 168) p_pomo3 &lt;- pomo_data$tree[[3]] %&gt;% midpoint() %&gt;% ggtree::ggtree(layout = &quot;circular&quot;) %&gt;% .$data %&gt;% dplyr::mutate(support = as.numeric(label), support_class = cut(support, c(0, 50, 70, 90, 100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;))) %&gt;% plot_tree(higl_node = 28, color = twisst_clr[[&quot;Butter&quot;]], xlim = c(-.01, .053), angle_in = 168, open_angle = 168) plot_list &lt;- list(p_pomo1, p_pomo2, p_pomo3) # compose base figure p_single &lt;- outlier_table %&gt;% filter(outlier_id %in% outlier_pick) %&gt;% left_join(region_label_tibbles) %&gt;% mutate(outlier_nr = row_number(), text = ifelse(outlier_nr == 1, TRUE, FALSE), trait = c(&#39;Snout&#39;, &#39;Bars&#39;, &#39;Peduncle&#39;)) %&gt;% pmap(plot_curtain, cool_genes = cool_genes, data_tables = data_tables) %&gt;% c(., plot_list) %&gt;% cowplot::plot_grid(plotlist = ., nrow = 2, rel_heights = c(1, .18), labels = letters[1:length(outlier_pick)] %&gt;% project_case(), label_size = plot_text_size) # compile legend # dummy plot for fst legend p_dummy_fst &lt;- outlier_table %&gt;% filter(row_number() == 1) %&gt;% purrr::pmap(plot_panel_fst) %&gt;% .[[1]] + guides(color = guide_colorbar(barheight = unit(3, &quot;pt&quot;), barwidth = unit(100, &quot;pt&quot;), label.position = &quot;top&quot;, ticks.colour = &quot;black&quot;)) # dummy plot for gxp legend p_dummy_gxp &lt;- outlier_table %&gt;% filter(row_number() == 1) %&gt;% purrr::pmap(plot_panel_gxp, trait = &#39;Bars&#39;) %&gt;% .[[1]] # fst legend p_leg_fst &lt;- (p_dummy_fst+theme(legend.position = &#39;bottom&#39;)) %&gt;% get_legend() # gxp legend p_leg_gxp &lt;- (p_dummy_gxp+theme(legend.position = &#39;bottom&#39;)) %&gt;% get_legend() # create sub-legend 1 p_leg_pomo &lt;- ((midpoint(pomo_data$tree[[1]]) %&gt;% ggtree(layout = &quot;circular&quot;) %&gt;% .$data %&gt;% mutate(support = as.numeric(label), support_class = cut(support, c(0,50,70,90,100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;)))) %&gt;% conditional_highlight(tree = ., higl_node = 21, highl = FALSE, support_guide = TRUE) + theme(text = element_text(size = plot_text_size), legend.position = &quot;bottom&quot;) ) %&gt;% get_legend() p_leg1 &lt;- cowplot::plot_grid(p_leg_fst, p_leg_gxp, p_leg_pomo, ncol = 1) # create sub-legend 2 (phylogeny schematics) p_leg2 &lt;- tibble(spec1 = c(&#39;indigo&#39;, &#39;indigo&#39;,&#39;unicolor&#39;), spec2 = c(&#39;maya&#39;, &#39;puella&#39;,NA), color = twisst_clr %&gt;% unname() %&gt;% darken(.,factor = .8), mode = c(rep(&#39;pair&#39;,2),&#39;isolation&#39;)) %&gt;% future_pmap(plot_leg) %&gt;% cowplot::plot_grid(plotlist = ., nrow = 1) # create sub-legend 3 p_leg &lt;- cowplot::plot_grid(p_leg1, p_leg2, nrow = 1, rel_widths = c(.6, 1)) # finalize figure p_done &lt;- cowplot::plot_grid(p_single, p_leg, ncol = 1, rel_heights = c(1, .17)) Finally, we can export Figure 5. # export figure hypo_save(plot = p_done, filename = &#39;figures/F5.pdf&#39;, width = f_width, height = f_width * .93, comment = script_name, device = cairo_pdf, bg = &quot;transparent&quot;) "],
["figure-6.html", "27 Figure 6 27.1 Summary 27.2 Details of plot_F6.R", " 27 Figure 6 27.1 Summary This is the accessory documentation of Figure 6. The Figure can be recreated by running the R script plot_F6.R from a (bash terminal): cd $BASE_DIR Rscript --vanilla R/fig/plot_F6.R \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/geva/ \\ 2_analysis/GxP/bySNP/ 27.2 Details of plot_F6.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, BAMMtools and on the package hypoimg. 27.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute interactively or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F6.R \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # 2_analysis/geva/ \\ # 2_analysis/GxP/bySNP/ # =============================================================== # This script produces Figure 6 of the study &quot;Rapid radiation in a highly # diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c( &quot;2_analysis/summaries/fst_outliers_998.tsv&quot;, # &quot;2_analysis/geva/&quot;, &quot;2_analysis/GxP/bySNP/&quot; ) # script_name &lt;- &quot;R/fig/plot_F6.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ggtext) library(ggpointdensity) library(scales) library(grid) library(prismatic) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_F6.R ────────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 2: 2_analysis/geva/ #&gt; ★ 3: 2_analysis/GxP/bySNP/ #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we set a few parameters for the plot layout: # config ----------------------- outlier_file &lt;- as.character(args[1]) geva_path &lt;- as.character(args[2]) gxp_path &lt;- as.character(args[3]) 27.2.2 Actual Script Start outlier_data &lt;- read_tsv(outlier_file) data &lt;- outlier_data[c(2, 13, 14),] %&gt;% pmap_dfr(get_gxp_and_geva) xrange &lt;- c(100, 10^6) color &lt;- rgb(1, 0.5, 0.16) base_length &lt;- 8 base_lwd &lt;- .15 base_line_clr &lt;- &quot;black&quot; splitage &lt;- tibble(intercept = 5000) gid_label &lt;- c( LG04_1 = &quot;LG04 (A)&quot;, LG12_3 = &quot;LG12 (B)&quot;, LG12_4 = &quot;LG12 (C)&quot; ) gxp_clr &lt;- c(Bars = &quot;#79009f&quot;, Snout = &quot;#E48A00&quot;, Peduncle = &quot;#5B9E2D&quot;) %&gt;% darken(factor = .95) %&gt;% set_names(., nm = c(&quot;Bars&quot;, &quot;Snout&quot;, &quot;Peduncle&quot;)) annotation_grobs &lt;- tibble(svg = hypo_trait_img$grob_circle[hypo_trait_img$trait %in% c( &#39;Snout&#39;, &#39;Bars&#39;, &#39;Peduncle&#39;)], layer = c(4,3,7), color = gxp_clr[c(1,3,2)]) %&gt;% purrr::pmap(.l = ., .f = hypo_recolor_svg) %&gt;% set_names(nm = c( &quot;LG12_3&quot;,&quot;LG12_4&quot;,&quot;LG04_1&quot;)) annotation_grobs$LG12_3 &lt;- hypo_recolor_svg(annotation_grobs$LG12_3, layer = 7, color = gxp_clr[[1]] %&gt;% clr_desaturate %&gt;% clr_lighten(.25)) annotation_grobs_tib &lt;- tibble(gid = names(annotation_grobs), grob = annotation_grobs) %&gt;% mutate( gid_label = gid_label[gid], trait = factor( c( &quot;Bars&quot;, &quot;Peduncle&quot;, &quot;Snout&quot;), levels = c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;))) highlight_rects &lt;- tibble(trait = factor( c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;), levels = c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;)), gid_label = gid_label) p_done &lt;- data %&gt;% pivot_longer(names_to = &quot;trait&quot;, values_to = &quot;p_wald&quot;, cols = Bars:Snout) %&gt;% mutate(trait = factor(trait, levels = c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;)), gid_label = gid_label[gid]) %&gt;% filter(Clock == &quot;J&quot;, Filtered == 1) %&gt;% ggplot() + geom_rect(data = highlight_rects, aes( xmin = 0, xmax = Inf, ymin = 0, ymax = Inf), color = rgb(.75,.75,.75), size = .4, fill = rgb(.9,.9,.9,.5))+ hypoimg::geom_hypo_grob(inherit.aes = FALSE, data = annotation_grobs_tib, aes(grob = grob), x = .15, y = .78, angle = 0, width = .35, height =.35)+ geom_pointdensity(size = plot_size, aes(x = PostMedian,y = p_wald))+ facet_grid(gid_label ~ trait, scales = &quot;free_y&quot;)+ scale_x_log10(labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)))+ scale_y_continuous(trans = reverselog_trans(10), labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)))+ scale_color_viridis_c(&quot;Density&quot;, option = &quot;B&quot;)+ labs(y = &quot;G x P *p* value &lt;sub&gt;Wald&lt;/sub&gt;&quot;, x = &quot;Derived allele age (generations)&quot;)+ guides(color = guide_colorbar(barwidth = unit(120, &quot;pt&quot;), barheight = unit(3, &quot;pt&quot;)))+ theme_minimal()+ theme(text = element_text(size = plot_text_size), axis.title.y = element_markdown(), legend.position = &quot;bottom&quot;, plot.subtitle = element_markdown(), axis.line = element_line(colour = base_line_clr, size = base_lwd), strip.background = element_blank(), panel.grid.minor = element_blank(), panel.grid.major = element_line(size = plot_lwd) ) Finally, we can export Figure 6. hypo_save(plot = p_done, filename = &quot;figures/F6.pdf&quot;, width = f_width_half, height = f_width_half, comment = plot_comment, device = cairo_pdf, bg = &quot;transparent&quot;) "],
["supplementary-figure-1.html", "28 Supplementary Figure 1 28.1 Summary 28.2 Details of plot_SF1.R", " 28 Supplementary Figure 1 28.1 Summary This is the accessory documentation of Figure S1. The Figure can be recreated by running the R script plot_SF1.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF1.R \\ ressources/Rabosky_etal_2018/dataFiles/ratemat_enhanced.csv 28.2 Details of plot_SF1.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 28.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF1.R \\ # ressources/Rabosky_etal_2018/dataFiles/ratemat_enhanced.csv # =============================================================== # This script produces Suppl. Figure 1 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c( &quot;ressources/Rabosky_etal_2018/dataFiles/ratemat_enhanced.csv&quot; ) # script_name &lt;- &quot;R/fig/plot_SF1.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_SF1.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: ressources/Rabosky_etal_2018/dataFiles/ratemat_enhanced.csv #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- rate_file &lt;- as.character(args[1]) ### Tip-specific speciation rates across Fish Tree of Life ### ------------------------------------------------------ ## Import FToL data rates &lt;- read_csv(file = rate_file) p_done &lt;- ggplot(rates, aes(lambda.tv)) + geom_histogram(binwidth = 0.25, colour=&quot;grey20&quot;, fill=&quot;grey80&quot;, size = .2) + labs(x = &quot;Mean speciation rate&quot;, y = &quot;Number of species&quot;) + geom_segment(aes(x = 2.5 , y = 200, xend = 2.5, yend = 1500), size = 0.2, color = &quot;#0976BA&quot;) + annotate(&quot;text&quot;, x = 2.5, y = 1750, label = &quot;Hamlets&quot;, size = plot_text_size / ggplot2:::.pt, color = &quot;#0976BA&quot;) + geom_segment(aes(x = 3.5 , y = 200, xend = 3.5, yend = 1500), size = 0.2, color = &quot;grey60&quot;) + annotate(&quot;text&quot;, x = 3.5, y = 2150, label = &quot;Haplo-\\nchromines&quot;, size = plot_text_size / ggplot2:::.pt, color = &quot;grey60&quot;) + geom_segment(aes(x = 4.5 , y = 200, xend = 4.5, yend = 1500), size = 0.2, color = &quot;grey60&quot;) + annotate(&quot;text&quot;, x = 4.5, y = 1750, label = &quot;Labeobarbus&quot;, size = plot_text_size / ggplot2:::.pt, color = &quot;grey60&quot;) + coord_cartesian(xlim = c(-.2, 5.1), ylim = c(-200, 8400), expand = 0) + theme_bw(base_size = plot_text_size) + theme(panel.grid.minor = element_blank()) Finally, we can export Figure S1. hypo_save(&quot;figures/SF1.pdf&quot;, plot = p_done, width = f_width_half, height = f_width_half * .6, comment = plot_comment, device = cairo_pdf, bg = &quot;transparent&quot;) "],
["supplementary-figure-2.html", "29 Supplementary Figure 2 29.1 Summary 29.2 Details of plot_SF2.R", " 29 Supplementary Figure 2 29.1 Summary This is the accessory documentation of Figure S2. The Figure can be recreated by running the R script plot_SF2.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF2.R \\ ressources/Rabosky_etal_2018/ 29.2 Details of plot_SF2.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 29.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF2.R \\ # ressources/Rabosky_etal_2018/ # =============================================================== # This script produces Suppl. Figure 2 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c( &quot;ressources/Rabosky_etal_2018/&quot; ) # script_name &lt;- &quot;R/fig/plot_SF2.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(BAMMtools) library(GenomicOriginsScripts) library(ggplotify) library(patchwork) library(ggforce) library(glue) library(ggtext) library(hypoimg) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_SF2.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: ressources/Rabosky_etal_2018/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- basepath &lt;- as.character(args[1]) ### Evolutionary rate analysis of Serraninae ### ---------------------------------------- ### Code adapted from bamm-project.org and Rabosky et al. 2018 (Nature) source(paste(basepath, &quot;scripts/supporting_fxns/PlottingFunctions.R&quot;, sep = &quot;&quot;)) ## Import FToL data eventfile_vr &lt;- paste(basepath, &quot;dataFiles/bamm_results/12k_tv1/event_data_thinned.csv&quot;, sep=&quot;&quot;) treefile &lt;- paste(basepath, &quot;dataFiles/bamm_results/12k_tv1/bigfish_no_outgroup.tre&quot;, sep=&quot;&quot;) tree_ftol &lt;- read.tree(treefile) ## Map event data onto time-calibrated tree bamm_ftol &lt;- getEventData(tree_ftol, eventfile_vr, burnin = 0) ## Extract Serraninae subtree Serraninae &lt;- c(&quot;Hypoplectrus_gemma&quot;, &quot;Hypoplectrus_unicolor&quot;, &quot;Hypoplectrus_gummigutta&quot;, &quot;Hypoplectrus_chlorurus&quot;, &quot;Hypoplectrus_aberrans&quot;, &quot;Hypoplectrus_nigricans&quot;, &quot;Hypoplectrus_guttavarius&quot;, &quot;Hypoplectrus_indigo&quot;, &quot;Hypoplectrus_puella&quot;, &quot;Serranus_tortugarum&quot;, &quot;Serranus_tabacarius&quot;, &quot;Schultzea_beta&quot;, &quot;Diplectrum_formosum&quot;, &quot;Diplectrum_bivittatum&quot;, &quot;Diplectrum_pacificum&quot;, &quot;Diplectrum_maximum&quot;, &quot;Serranus_notospilus&quot;, &quot;Serranus_phoebe&quot;, &quot;Serranus_psittacinus&quot;, &quot;Serranus_baldwini&quot;, &quot;Serranus_tigrinus&quot;, &quot;Paralabrax_albomaculatus&quot;, &quot;Paralabrax_dewegeri&quot;, &quot;Paralabrax_callaensis&quot;, &quot;Paralabrax_loro&quot;, &quot;Paralabrax_auroguttatus&quot;, &quot;Paralabrax_clathratus&quot;, &quot;Paralabrax_humeralis&quot;, &quot;Paralabrax_nebulifer&quot;, &quot;Paralabrax_maculatofasciatus&quot;, &quot;Zalanthias_kelloggi&quot;, &quot;Serranus_cabrilla&quot;, &quot;Serranus_atricauda&quot;, &quot;Serranus_scriba&quot;, &quot;Serranus_hepatus&quot;, &quot;Serranus_accraensis&quot;, &quot;Centropristis_striata&quot;, &quot;Chelidoperca_occipitalis&quot;, &quot;Chelidoperca_investigatoris&quot;, &quot;Chelidoperca_pleurospilus&quot;) Hamlets &lt;- c(&quot;Hypoplectrus_gemma&quot;, &quot;Hypoplectrus_unicolor&quot;, &quot;Hypoplectrus_gummigutta&quot;, &quot;Hypoplectrus_chlorurus&quot;, &quot;Hypoplectrus_aberrans&quot;, &quot;Hypoplectrus_nigricans&quot;, &quot;Hypoplectrus_guttavarius&quot;, &quot;Hypoplectrus_indigo&quot;, &quot;Hypoplectrus_puella&quot;) bamm_serrn &lt;- subtreeBAMM(bamm_ftol, tips = Serraninae) tree_serrn &lt;- as.phylo(bamm_serrn) ## Mean phylorate plot bamm_serrn_abbr &lt;- bamm_serrn bamm_serrn_abbr$tip.label &lt;- bamm_serrn$tip.label %&gt;% str_replace(pattern = &quot;([A-Z])[a-z]*_([a-z]*)&quot;, &quot;italic(\\\\1.~\\\\2)&quot;) %&gt;% str_replace(pattern = &quot;C.&quot;, &quot;Ch.&quot;) %&gt;% str_replace(pattern = &quot;Ch.~striata&quot;, &quot;Cp.~striata&quot;) %&gt;% str_replace(pattern = &quot;S.&quot;, &quot;Se.&quot;) %&gt;% str_replace(pattern = &quot;Se.~beta&quot;, &quot;Sc.~&#39;beta&#39;&quot;) %&gt;% str_replace(pattern = &quot;P.&quot;, &quot;Pa.&quot;) %&gt;% str_replace(pattern = &quot;Z.&quot;, &quot;Pl.&quot;) %&gt;% ggplot2:::parse_safe() ## Credible sets of shift configurations css &lt;- credibleShiftSet(bamm_serrn_abbr, expectedNumberOfShifts = 1, threshold = 5, set.limit = 0.95) summary(css) clr_tree &lt;- scico::scico(6, palette = &quot;berlin&quot;) %&gt;% prismatic::clr_desaturate(shift = .4) %&gt;% prismatic::clr_darken(shift = .2) clr_tree2 &lt;- colorRampPalette(RColorBrewer::brewer.pal(9,&quot;RdYlBu&quot;))(64) %&gt;% rev() clr_shift &lt;- &quot;red&quot; css2 &lt;- css css2$marg.probs[&quot;47&quot;] &lt;- .1 p1 &lt;- as.grob(function(){ par(mar = c(0,0,0,0)) plot.credibleshiftset(css2, logcolor = TRUE, add.freq.text = FALSE, border = FALSE, shiftColor = clr_shift, lwd = 2, labels = TRUE, legend = FALSE, pal = clr_tree, cex = .5) leg_shift_x &lt;- 1.3 leg_shift_y &lt;- 5 text(x = c(21.2, 40.2), y = c(15.6, 33.25), label = &quot;\\U2605&quot;, family = &quot;DejaVu Sans&quot;, col = clr_shift, cex = .5) lines(x = c(0,25) + leg_shift_x, y = c(1.5, 1.5) + leg_shift_y, col = &quot;darkgray&quot;) text(x = 12.5 + leg_shift_x, y = .5 + leg_shift_y, labels = &quot;25 MYR&quot;, cex = .4, col = &quot;darkgray&quot;) }) ## Macroevolutionary cohort analysis cmat &lt;- getCohortMatrix(bamm_serrn) p2 &lt;- as.grob(function(){ cohorts(cmat, bamm_serrn, lwd = 1.5, labels = FALSE, legend = FALSE, ofs = 0, use.plot.bammdata = TRUE, pal = clr_tree, col = clr_tree2, cex.axis = 0.1) }) p_done &lt;- (ggplot() + geom_point(data = tibble(v = c(.056, 2.4)), x = .5, y = .5, aes(color = v),alpha = 0) + scale_color_gradientn(&quot;Speciation Rate&quot;, colours = clr_tree, limits = c(.056, 2.4)) + geom_richtext(data = tibble(x = -.05, y = .12, lab = glue(&quot;Posterior Frequency: {css$frequency}&lt;br&gt;Marginal Shift Prob.: {css$marg.probs[&#39;47&#39;]}&quot;)), aes(x = x, y = y, label = lab), size = plot_text_size_small / .pt, color = clr_shift, hjust = 0, label.size = 0, label.color = &quot;transparent&quot;)+ geom_bezier0(data = tibble(x = c(.62,.5,.32), y = c(.185,.12,.12)), aes(x,y, group = 1), size = .3, color = prismatic::clr_alpha(clr_shift,.3))+ annotation_custom(p1,xmin = -.3, ymin = -.2, xmax = 1.1, ymax = 1.13) + (ggplot() + geom_point(data = tibble(v = c(0, 1)), x = .5, y = .5, aes(color = v),alpha = 0)+ scale_color_gradientn(&quot;Pairwise Correlation&quot;, colours = clr_tree2, limits = c(0, 1))+ annotation_custom(p2, xmin = -.2, ymin = -.25, xmax = 1.2, ymax = 1.075) ) &amp; plot_annotation(tag_levels = &quot;a&quot;) &amp; coord_cartesian(xlim = c(0,1), ylim = c(0,1)) &amp; guides(color = guide_colorbar(title.position = &quot;top&quot;, direction = &quot;horizontal&quot;, barheight = unit(3, &quot;pt&quot;), barwidth = unit(100, &quot;pt&quot;), ticks.colour = &quot;white&quot;))) &amp; theme_minimal(base_size = plot_text_size) &amp; theme(legend.position = c(.5, -.03), legend.justification = c(.5, 0), legend.background = element_blank(), panel.grid = element_blank(), axis.title = element_blank(), axis.text = element_blank(), plot.tag = element_text(hjust = 0), panel.background = element_blank(), plot.background = element_blank()) Finally, we can export Figure S2. hypo_save(&quot;figures/SF2.pdf&quot;, plot = p_done, width = f_width, height = f_width*.5, comment = plot_comment, device = cairo_pdf, bg = &quot;transparent&quot;) "],
["supplementary-figure-3.html", "30 Supplementary Figure 3 30.1 Summary 30.2 Details of plot_SF3.R", " 30 Supplementary Figure 3 30.1 Summary This is the accessory documentation of Figure S3. The Figure can be recreated by running the R script plot_SF3.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF3.R \\ 2_analysis/pca/ 30.2 Details of plot_SF3.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 30.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF3.R \\ # 2_analysis/pca/ # =============================================================== # This script produces Suppl. Figure 3 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/pca/&quot;) # script_name &lt;- &quot;R/fig/plot_SF3.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(patchwork) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF3.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/pca/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- pca_dir &lt;- as.character(args[1]) clr_alt &lt;- clr clr_alt[[&quot;uni&quot;]] &lt;- &quot;lightgray&quot; fish_tib &lt;- tibble(short = names(clr)[!names(clr) %in% c(&quot;flo&quot;, &quot;tab&quot;, &quot;tor&quot;)], x = c(0.5, 3.5, 7, 9.7, 12.25, 15.25, 18, 21.5)) key_sz &lt;- .75 sp_fam &lt;- rep(c(&quot;H&quot;, &quot;S&quot;, &quot;H&quot;), c(8, 2, 1)) %&gt;% set_names(nm = names(sp_names)) p_leg &lt;- fish_tib %&gt;% ggplot() + coord_equal(xlim = c(-.05, 24), expand = 0) + geom_tile(aes(x = x, y = 0,library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(vroom) library(ggtext) fill = short, color = after_scale(prismatic::clr_darken(fill, .25))), width = key_sz, height = key_sz, size = .3) + geom_text(aes(x = x + .6, y = 0, label = str_c(sp_fam[short], &quot;. &quot;, sp_names[short])), hjust = 0, fontface = &quot;italic&quot;, size = plot_text_size / ggplot2:::.pt) + pmap(fish_tib, plot_fish_lwd, width = 1, height = 1, y = 0) + scale_fill_manual(values = clr, guide = FALSE) + theme_void() p_done &lt;- cowplot::plot_grid((tibble(loc = c(&quot;bel.&quot;, &quot;hon.&quot;, &quot;pan.&quot;), mode = rep(c(&quot;subset_non_diverged&quot;), 3), pc1 = 1, pc2 = 2) %&gt;% pmap(pca_plot_no_fish) %&gt;% wrap_plots(ncol = 3) + plot_annotation(tag_levels = &quot;a&quot;) &amp; theme(plot.background = element_blank())), p_leg, ncol = 1 , rel_heights = c(1,.1)) Finally, we can export Figure S3. hypo_save(p_done, filename = &#39;figures/SF3.pdf&#39;, width = f_width, height = f_width * .38, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["supplementary-figure-4.html", "31 Supplementary Figure 4 31.1 Summary 31.2 Details of plot_SF4.R", " 31 Supplementary Figure 4 31.1 Summary This is the accessory documentation of Figure S4. The Figure can be recreated by running the R script plot_SF4.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF4.R \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/summaries/fst_globals.txt 31.2 Details of plot_SF4.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 31.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF4.R \\ # 2_analysis/fst/50k/ \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Suppl. Figure 4 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/fst/50k/&#39;, # &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_SF4.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(vroom) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF4.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/fst/50k/ #&gt; ★ 2: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 3: 2_analysis/summaries/fst_globals.txt #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- data_path &lt;- as.character(args[1]) outlier_file &lt;- as.character(args[2]) globals_file &lt;- as.character(args[3]) # load data ------------------- # locate fst data files files &lt;- dir(data_path,pattern = &#39;.50k.windowed.weir.fst.gz&#39;) # extract run names from data file names run_files &lt;- files %&gt;% str_sub(.,1,11) %&gt;% str_replace(.,pattern = &#39;([a-z]{3})-([a-z]{3})-([a-z]{3})&#39;, &#39;\\\\2\\\\1-\\\\3\\\\1&#39;) # load genome wide average fst values for each run globals &lt;- vroom::vroom(globals_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% separate(run, into = c(&#39;pop1&#39;,&#39;pop2&#39;)) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;,pop2,loc), run = fct_reorder(run,weighted)) # load all windowed fst data and collapse in to a single data frame data &lt;- purrr::pmap(tibble(file = str_c(data_path,files), run = run_files), hypo_import_windows) %&gt;% bind_rows() %&gt;% purrr::set_names(., nm = c(&#39;CHROM&#39;, &#39;BIN_START&#39;, &#39;BIN_END&#39;, &#39;N_VARIANTS&#39;, &#39;WEIGHTED_FST&#39;, &#39;MEAN_FST&#39;, &#39;GSTART&#39;, &#39;POS&#39;, &#39;GPOS&#39;, &#39;run&#39;)) %&gt;% mutate(pop1 = str_sub(run,1,3), pop2 = str_sub(run,8,10), loc = str_sub(run,4,6), run_label = str_c(&quot;*H. &quot;, sp_names[pop1],&quot;* - *H. &quot;, sp_names[pop2],&quot;*&lt;br&gt;(&quot;,loc_names[loc],&quot;)&quot; )) # create table for the indication of genome wide average fst in the plot background # (rescale covered fst range to the extent of the genome) global_bar &lt;- globals %&gt;% select(weighted,run) %&gt;% mutate(run = as.character(run)) %&gt;% setNames(.,nm = c(&#39;fst&#39;,&#39;run&#39;)) %&gt;% pmap(.,fst_bar_row_run) %&gt;% bind_rows() %&gt;% mutate(pop1 = str_sub(run,1,3), pop2 = str_sub(run,8,10), loc = str_sub(run,4,6), run_label = str_c(&quot;*H. &quot;, sp_names[pop1],&quot;* - *H. &quot;, sp_names[pop2],&quot;*&lt;br&gt;(&quot;,loc_names[loc],&quot;)&quot; ), run_label = fct_reorder(run_label,xmax_org)) # pre-calculate secondary x-axis breaks sc_ax &lt;- scales::cbreaks(c(0,max(globals$weighted)), scales::pretty_breaks(4)) # compose final figure p_done &lt;- ggplot()+ # general plot structure separated by run facet_grid( run_label ~ ., as.table = TRUE) + # add genome wide average fst in the background geom_rect(data = global_bar %&gt;% mutate(xmax = xmax * hypo_karyotype$GEND[24]), aes(xmin = 0, xmax = xmax, ymin = -Inf, ymax = Inf), color = rgb(1,1,1,0), fill = clr_below) + # add LG borders geom_vline(data = hypogen::hypo_karyotype, aes(xintercept = GEND), color = hypo_clr_lg) + # add fst data points geom_point(data = data %&gt;% mutate(run_label = factor(run_label, levels = levels(global_bar$run_label))), aes(x = GPOS, y = WEIGHTED_FST), size=.2,color = plot_clr) + # axis layout scale_x_hypo_LG(sec.axis = sec_axis(~ ./hypo_karyotype$GEND[24], breaks = (sc_ax$breaks/max(globals$weighted)), labels = sprintf(&quot;%.2f&quot;, sc_ax$breaks), name = expression(Genomic~position/~Genome~wide~weighted~italic(F[ST])))) + scale_y_continuous(name = expression(italic(&#39;F&#39;[ST])), limits = c(-.1,1), breaks = c(0,.5,1)) + # general plot layout theme_hypo() + theme(strip.text.y = element_markdown(angle = 0), strip.background = element_blank(), legend.position = &#39;none&#39;, axis.title.x = element_text(), axis.text.x.bottom = element_text(colour = &#39;darkgray&#39;)) Finally, we can export Figure S4. # export final figure hypo_save(filename = &#39;figures/SF4.png&#39;, plot = p_done, width = 8, height = 12, dpi = 600, type = &quot;cairo&quot;, comment = plot_comment) system(&quot;convert figures/SF4.png figures/SF4.pdf&quot;) system(&quot;rm figures/SF4.png&quot;) create_metadata &lt;- str_c(&quot;exiftool -overwrite_original -Description=\\&quot;&quot;, plot_comment, &quot;\\&quot; figures/SF4.pdf&quot;) system(create_metadata) "],
["supplementary-figure-5.html", "32 Supplementary Figure 5 32.1 Summary 32.2 Details of plot_SF5.R", " 32 Supplementary Figure 5 32.1 Summary This is the accessory documentation of Figure S5. The Figure can be recreated by running the R script plot_SF5.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF5.R \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_globals.txt 32.2 Details of plot_SF5.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 32.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF5.R \\ # 2_analysis/fst/50k/ \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Suppl. Figure 5 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/fst/50k/&#39;, &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_SF5.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(ggforce) library(hypoimg) library(hypogen) library(vroom) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF5.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/fst/50k/ #&gt; ★ 2: 2_analysis/summaries/fst_globals.txt #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- data_dir &lt;- as.character(args[1]) globals_file &lt;- as.character(args[2]) # script ----------------------- # locate data files files &lt;- dir(path = data_dir, pattern = &#39;.50k.windowed.weir.fst.gz&#39;) # load genome wide average fst data globals &lt;- vroom::vroom(globals_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% mutate(run = str_c(loc,&#39;-&#39;,run) %&gt;% reformat_run_name() ) # prepare data import settings within a data table (tibble) import_table &lt;- list(file = str_c(data_dir,files), fst_threshold = c(.5,.4,.3,.2,.1,.05,.02,.01)) %&gt;% cross_df() %&gt;% mutate( run = file %&gt;% str_remove(&#39;^.*/&#39;) %&gt;% str_sub(.,1,11) %&gt;% reformat_run_name()) # import dxy data and compute threshold stats get_fst_fixed &lt;- function(file, run, fst_threshold,...){ data &lt;- hypogen::hypo_import_windows(file, ...) %&gt;% mutate(rank = rank(WEIGHTED_FST, ties.method = &quot;random&quot;))%&gt;% mutate(thresh = fst_threshold) %&gt;% mutate(outl = (WEIGHTED_FST &gt; thresh) %&gt;% as.numeric()) %&gt;% filter(outl == 1 ) if(nrow(data) == 0){ return(tibble(run = run, n = 0, avg_length = NA, med_length = NA, min_length = NA, max_length = NA, sd_length = NA, overal_length = NA, threshold_value = fst_threshold)) } else { data %&gt;% # next, we want to collapse overlapping windows group_by(CHROM) %&gt;% # we check for overlap and create &#39;region&#39; IDs mutate(check = 1-(lag(BIN_END,default = 0)&gt;BIN_START), ID = str_c(CHROM,&#39;_&#39;,cumsum(check))) %&gt;% ungroup() %&gt;% # then we collapse the regions by ID group_by(ID) %&gt;% summarise(run = run[1], run = run[1], treshold_value = thresh[1], CHROM = CHROM[1], BIN_START = min(BIN_START), BIN_END = max(BIN_END)) %&gt;% mutate(PEAK_SIZE = BIN_END-BIN_START) %&gt;% summarize(run = run[1], run = run[1], n = length(ID), avg_length = mean(PEAK_SIZE), med_length = median(PEAK_SIZE), min_length = min(PEAK_SIZE), max_length = max(PEAK_SIZE), sd_length = sd(PEAK_SIZE), overal_length = sum(PEAK_SIZE), threshold_value = treshold_value[1]) } } # load data and compute statistics based on fixed fst threshold data &lt;- purrr::pmap_dfr(import_table, get_fst_fixed) %&gt;% left_join(globals) %&gt;% mutate(run = fct_reorder(run, weighted)) # pre-format labels data2 &lt;- data %&gt;% select(threshold_value,weighted,n,avg_length,overal_length) %&gt;% mutate(avg_length = avg_length/1000, overal_length = overal_length/(10^6)) %&gt;% rename(`atop(Number~of,Regions)` = &#39;n&#39;, `atop(Average~Region,Length~(kb))` = &#39;avg_length&#39;, `atop(Cum.~Region,Length~(Mb))` = &#39;overal_length&#39;) %&gt;% pivot_longer(names_to = &#39;variable&#39;,values_to = &#39;Value&#39;,3:5) %&gt;% mutate(threshold_value = str_c(&#39;italic(F[ST])~threshold:~&#39;, threshold_value), variable = factor(variable, levels = c(&#39;atop(Number~of,Regions)&#39;, &#39;atop(Average~Region,Length~(kb))&#39;, &#39;atop(Cum.~Region,Length~(Mb))&#39;))) # set font size base_line_clr &lt;- &quot;black&quot; # compile plot p_done &lt;- data2 %&gt;% # select thresholds of interest filter(!(threshold_value %in% (c(0.02,.1,0.2, 0.3, .4) %&gt;% str_c(&quot;italic(F[ST])~threshold:~&quot;,.)))) %&gt;% ggplot(aes(x = weighted, y = Value#, fill = weighted ) )+ # add red line for genome extent in lowest row geom_hline(data = tibble(variable = factor(c(&#39;atop(Cum.~Region,Length~(Mb))&#39;, &#39;atop(Average~Region,Length~(kb))&#39;, &#39;atop(Number~of,Regions)&#39;), levels = c(&#39;atop(Number~of,Regions)&#39;, &#39;atop(Average~Region,Length~(kb))&#39;, &#39;atop(Cum.~Region,Length~(Mb))&#39;)), y = c(559649677/(10^6),NA,NA)), aes(yintercept = y), color = rgb(1,0,0,.25))+ # add data points geom_point(size = plot_size, color = plot_clr )+ # define plot stucture facet_grid(variable~threshold_value, scale=&#39;free&#39;, switch = &#39;y&#39;, labeller = label_parsed)+ # configure scales scale_x_continuous(name = expression(Whole-genome~differentiation~(weighted~italic(F[ST]))), breaks = c(0,.05,.1), limits = c(-.00025,.10025), labels = c(&quot;0&quot;, &quot;0.05&quot;, &quot;0.1&quot;))+ # configure legend guides(fill = guide_colorbar(barwidth = unit(150, &quot;pt&quot;), label.position = &quot;top&quot;, barheight = unit(5,&quot;pt&quot;)))+ # tweak plot apperance theme_minimal()+ theme(axis.text = element_text(size = plot_text_size_small, color = rgb(.6,.6,.6)), axis.title.y = element_blank(), axis.text.x = element_text(vjust = .5, angle = 0), axis.title.x = element_text(vjust = -2), panel.background = element_rect(fill = rgb(.95,.95,.95,.5), color = rgb(.9,.9,.9,.5), size = .3), panel.grid.minor = element_blank(), panel.grid.major = element_line(size = plot_lwd), legend.position = &quot;bottom&quot;, strip.text = element_text(size = plot_text_size), legend.direction = &quot;horizontal&quot;, strip.placement = &#39;outside&#39;, axis.title = element_text(size = plot_text_size), legend.title = element_text(size = plot_text_size), strip.background.y = element_blank(), plot.background = element_blank()) Finally, we can export Figure S5. # export figure 5 hypo_save(filename = &#39;figures/SF5.pdf&#39;, plot = p_done, width = f_width, height = .5 * f_width, device = cairo_pdf, comment = plot_comment, bg = &quot;transparent&quot;) "],
["supplementary-figure-6.html", "33 Supplementary Figure 6 33.1 Summary 33.2 Details of plot_SF6.R", " 33 Supplementary Figure 6 33.1 Summary This is the accessory documentation of Figure S6. The Figure can be recreated by running the R script plot_SF6.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF6.R \\ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/fst/50k/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz 33.2 Details of plot_SF6.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 33.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF6.R \\ # 2_analysis/summaries/fst_globals.txt \\ # 2_analysis/fst/50k/ \\ # 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz # =============================================================== # This script produces Suppl. Figure 6 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c( &#39;2_analysis/summaries/fst_globals.txt&#39;, # &#39;2_analysis/fst/50k/&#39;, # &#39;2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz&#39;) # script_name &lt;- &quot;R/fig/plot_SF6.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(vroom) library(furrr) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_SF6.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/summaries/fst_globals.txt #&gt; ★ 2: 2_analysis/fst/50k/ #&gt; ★ 3: 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- global_fst_file &lt;- as.character(args[1]) fst_dir &lt;- as.character(args[2]) rho_dir &lt;- as.character(args[3]) # load genome wide average fst data fst_globals &lt;- vroom::vroom(global_fst_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run_prep&#39;,&#39;mean_fst&#39;,&#39;weighted_fst&#39;)) %&gt;% separate(run_prep,into = c(&#39;pop1&#39;,&#39;pop2&#39;),sep = &#39;-&#39;) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;,pop2,loc), run = fct_reorder(run,weighted_fst)) # locate sliding window fst data files fst_files &lt;- dir(fst_dir, pattern = &#39;.50k.windowed.weir.fst.gz&#39;) # load sliding window fst data fst_data &lt;- str_c(fst_dir,fst_files) %&gt;% furrr::future_map_dfr(get_fst) %&gt;% mutate(run = factor(run, levels = levels(fst_globals$run))) # load recombination rate data rho_data &lt;- vroom::vroom(rho_dir, delim = &#39;\\t&#39;) %&gt;% select(-BIN_END) # merge fst and recombination data combined_data &lt;- fst_data %&gt;% # filter fst data to &quot;non-overlapping&quot; windows filter(BIN_START %% 50000 == 1 ) %&gt;% # merge with recombination data left_join(rho_data, by = c(CHROM = &#39;CHROM&#39;, BIN_START = &#39;BIN_START&#39;)) %&gt;% # merge with genome wide average fst data left_join(.,fst_globals %&gt;% select(run, weighted_fst)) %&gt;% # add label column mutate(pop1 = str_sub(run,1,3), pop2 = str_sub(run,8,10), loc = str_sub(run,4,6), run_label = str_c(&quot;*H. &quot;, sp_names[pop1],&quot;* - *H. &quot;, sp_names[pop2],&quot;*&lt;br&gt;(&quot;,loc_names[loc],&quot;)&quot; ), run_label = fct_reorder(run_label,weighted_fst)) # nest data to run linear regression on all runs in one go model_data &lt;- combined_data %&gt;% group_by(run) %&gt;% nest() %&gt;% left_join(., fst_globals) %&gt;% mutate(mod = map(data, function(data){lm(WEIGHTED_FST ~ RHO, data = data)}), pop1 = str_sub(run,1,3), pop2 = str_sub(run,8,10), loc = str_sub(run,4,6), run_label = str_c(&quot;*H. &quot;, sp_names[pop1],&quot;* - *H. &quot;, sp_names[pop2],&quot;*&lt;br&gt;(&quot;,loc_names[loc],&quot;)&quot; )) %&gt;% bind_cols(., summarise_model(.)) %&gt;% mutate(run_label = factor(run_label, levels = levels(combined_data$run_label))) # create subplot a (hex-bins) p1 &lt;- combined_data %&gt;% ggplot()+ # add hex-bin desity layer geom_hex(bins = 30, color = rgb(0,0,0,.3), aes(fill=log10(..count..), x = RHO, y = WEIGHTED_FST))+ # add regression line geom_abline(data = model_data, color = rgb(1,1,1,.8), linetype = 2, aes(intercept = intercept, slope = slope)) + # add R^2 label geom_text(data = model_data, x = 0, y = .975, parse = TRUE, hjust = 0, vjust = 1, aes(label = str_c(&#39;italic(R)^2:~&#39;,round(r.squared,2)))) + # general plot structure (separated by run) facet_wrap(run_label ~., ncol = 5)+ # set axis layout and color scheme scale_x_continuous(name = expression(rho))+ scale_y_continuous(name = expression(italic(F[ST])),limits = c(-.05,1))+ scico::scale_fill_scico(palette = &#39;berlin&#39;) + # customize legend guides(fill = guide_colorbar(direction = &#39;horizontal&#39;, title.position = &#39;top&#39;, barheight = unit(7,&#39;pt&#39;), barwidth = unit(130,&#39;pt&#39;)))+ # general plot layout theme_minimal()+ theme(legend.position = c(.8,.08), strip.text = element_markdown()) # create subplot b (slopes) p2 &lt;- model_data %&gt;% ggplot()+ geom_point(color = plot_clr, aes(x = weighted_fst, y = slope))+ labs(x = expression(genome~wide~weighted~mean~italic(F[ST])), y = expression(slope~(f(italic(F[ST]))==a~rho+b)))+ theme_minimal() # create subplot c (R^2s) p3 &lt;- model_data %&gt;% ggplot()+ geom_point(color = plot_clr, aes(x = weighted_fst, y = r.squared))+ labs(x = expression(genome~wide~weighted~mean~italic(F[ST])), y = expression(italic(R^2)))+ theme_minimal() # compose final figure p_done &lt;- plot_grid(p1, plot_grid(p2,p3, nrow = 1, labels = letters[2:3] %&gt;% project_case()), ncol = 1, rel_heights = c(1,.3), labels = project_case(c(&quot;a&quot;))) Finally, we can export Figure S6. # export final figure hypo_save(filename = &#39;figures/SF6.pdf&#39;, plot = p_done, width = 10, height = 16, comment = plot_comment) "],
["supplementary-figure-7.html", "34 Supplementary Figure 7 34.1 Summary 34.2 Details of plot_SF7.R", " 34 Supplementary Figure 7 34.1 Summary This is the accessory documentation of Figure S7. The Figure can be recreated by running the R script plot_SF7.R: cd $BASE_DIR run from terminal: Rscript --vanilla R/fig/plot_SF7.R \\ 2_analysis/dxy/50k/ 34.2 Details of plot_SF7.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 34.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF7.R \\ # 2_analysis/dxy/50k/ # =============================================================== # This script produces Suppl. Figure 7 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/dxy/50k/&#39;) # script_name &lt;- &quot;R/fig/plot_SF7.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF7.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- dxy_path &lt;- as.character(args[1]) # locate dxy data files files &lt;- dir(dxy_path) # load dxy data data &lt;- str_c(dxy_path,files) %&gt;% purrr::map(get_dxy) %&gt;% bind_rows() %&gt;% purrr::set_names(., nm = c(&#39;scaffold&#39;, &#39;start&#39;, &#39;end&#39;, &#39;mid&#39;, &#39;sites&#39;, &#39;pi_pop1&#39;, &#39;pi_pop2&#39;, &#39;dxy&#39;, &#39;fst&#39;, &#39;GSTART&#39;, &#39;gpos&#39;, &#39;run&#39;)) # create table for the indication of genome wide average dxy in the plot background # (rescale covered dxy range to the extent of the genome) global_bar &lt;- data %&gt;% # filter to non-overlaping windows only filter( start %% 50000 == 1) %&gt;% select(sites, dxy, run) %&gt;% group_by(run) %&gt;% summarise(genome_wide_dxy = sum(sites*dxy)/sum(sites)) %&gt;% arrange(genome_wide_dxy) %&gt;% ungroup() %&gt;% mutate(run = fct_reorder(.f = run, .x = genome_wide_dxy), scaled_dxy = genome_wide_dxy/max(genome_wide_dxy)) # prepare plot annotaton images grob_tibble &lt;- global_bar %&gt;% mutate(loc = str_sub(run,4,6), right = str_sub(run,1,3), left = str_sub(run,8,10)) %&gt;% select(1,4:6) %&gt;% pmap(.,plot_pair_run) %&gt;% bind_rows() # prepare plotting elements -------- # pre-define secondary x-axis breaks sc_ax &lt;- scales::cbreaks(c(0,max(global_bar$genome_wide_dxy)), scales::pretty_breaks(4)) # pre-define secondary x-axis labels labels &lt;- str_c(c(&quot;&quot;, sc_ax$breaks[2:5]*1000), c(&quot;0&quot;, rep(&quot;\\u00B710^-3&quot;,4))) # sort pair-wise population comparisons by average genome wide dxy data &lt;- data %&gt;% mutate(run = factor(run, levels = levels(global_bar$run))) # compose final figure p_done &lt;- ggplot()+ # general plot structure separated by run facet_wrap( .~run, as.table = TRUE, ncol = 1, dir = &#39;v&#39;)+ # add genome wide average dxy in the background geom_rect(data = global_bar %&gt;% mutate(xmax = scaled_dxy * hypo_karyotype$GEND[24]), aes(xmin = 0, xmax = xmax, ymin = -Inf, ymax = Inf), color = rgb(1,1,1,0),fill = clr_below)+ # add LG borders geom_vline(data = hypogen::hypo_karyotype, aes(xintercept = GEND), color = hypo_clr_lg)+ # add dxy data points geom_point(data = data, aes(x = gpos, y = dxy), size=.2,color = plot_clr) + # add fish images geom_hypo_grob2(data = grob_tibble, aes(grob = grob, rel_x = .945, rel_y = .5), angle = 0, height = .9, width = .13)+ # axis layout scale_x_hypo_LG(sec.axis = sec_axis(~ ./hypo_karyotype$GEND[24], breaks = (sc_ax$breaks/max(global_bar$genome_wide_dxy))[1:5], labels = labels, name = expression(Genomic~position/~Genome~wide~italic(d[XY]))))+ scale_y_continuous(name = expression(italic(d[XY])), breaks = c(0,.01, .02))+ # set plot extent coord_cartesian(xlim = c(0, hypo_karyotype$GEND[24]*1.135))+ # general plot layout theme_hypo()+ theme(strip.text = element_blank(), legend.position = &#39;none&#39;, axis.title.x = element_text(), axis.text.x.bottom = element_markdown(colour = &#39;darkgray&#39;)) Finally, we can export Figure S7. # export final figure hypo_save(filename = &#39;figures/SF7.png&#39;, plot = p_done, width = 8, height = 12, dpi = 600, type = &quot;cairo&quot;, comment = plot_comment) system(&quot;convert figures/SF7.png figures/SF7.pdf&quot;) system(&quot;rm figures/SF7.png&quot;) create_metadata &lt;- str_c(&quot;exiftool -overwrite_original -Description=\\&quot;&quot;, plot_comment, &quot;\\&quot; figures/SF7.pdf&quot;) system(create_metadata) "],
["supplementary-figure-8.html", "35 Supplementary Figure 8 35.1 Summary 35.2 Details of plot_SF8.R", " 35 Supplementary Figure 8 35.1 Summary This is the accessory documentation of Figure S8. The Figure can be recreated by running the R script plot_SF8.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF8.R \\ 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/multi_fst.50k.tsv.gz \\ 2_analysis/GxP/50000/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/twisst/weights/ \\ ressources/plugin/trees/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz \\ 2_analysis/summaries/fst_globals.txt 35.2 Details of plot_SF8.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 35.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF8.R \\ # 2_analysis/dxy/50k/ \\ # 2_analysis/fst/50k/multi_fst.50k.tsv.gz \\ # 2_analysis/GxP/50000/ \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ # 2_analysis/twisst/weights/ \\ # ressources/plugin/trees/ \\ # 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Figure 8 of the study &quot;Ancestral variation, hybridization and modularity # fuel a marine radiation&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/dxy/50k/&#39;,&#39;2_analysis/fst/50k/multi_fst.50k.tsv.gz&#39;, # &#39;2_analysis/GxP/50000/&#39;, &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R&#39;, # &#39;2_analysis/twisst/weights/&#39;, &#39;ressources/plugin/trees/&#39;, # &#39;2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz&#39;, &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_SF8.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF8.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ★ 2: 2_analysis/fst/50k/multi_fst.50k.tsv.gz #&gt; ★ 3: 2_analysis/GxP/50000/ #&gt; ★ 4: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 5: https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R #&gt; ★ 6: 2_analysis/twisst/weights/ #&gt; ★ 7: ressources/plugin/trees/ #&gt; ★ 8: 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz #&gt; ★ 9: 2_analysis/summaries/fst_globals.txt #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- dxy_dir &lt;- as.character(args[1]) fst_file &lt;- as.character(args[2]) gxp_dir &lt;- as.character(args[3]) outlier_table &lt;- as.character(args[4]) twisst_script &lt;- as.character(args[5]) w_path &lt;- as.character(args[6]) d_path &lt;- as.character(args[7]) recombination_file &lt;- as.character(args[8]) global_fst_file &lt;- as.character(args[9]) source(twisst_script) # start script ------------------- # import fst data fst_data &lt;- vroom::vroom(fst_file, delim = &#39;\\t&#39;) %&gt;% select(CHROM, BIN_START, BIN_END, N_VARIANTS, WEIGHTED_FST) %&gt;% setNames(., nm = c(&#39;CHROM&#39;, &#39;BIN_START&#39;, &#39;BIN_END&#39;, &#39;n_snps&#39;, &#39;fst&#39;) ) %&gt;% add_gpos() %&gt;% select(GPOS, fst) %&gt;% setNames(., nm = c(&#39;GPOS&#39;,&#39;value&#39;)) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;a&#39;),&#39;):joint~italic(F[ST])&#39;)) # locate dxy data files dxy_files &lt;- dir(dxy_dir) # import dxy data dxy_data &lt;- str_c(dxy_dir,dxy_files) %&gt;% purrr::map(get_dxy) %&gt;% bind_rows() %&gt;% select(N_SITES:GPOS, run) %&gt;% mutate(pop1 = str_sub(run,1,6), pop2 = str_sub(run,8,13)) # compute delta dxy dxy_summary &lt;- dxy_data %&gt;% group_by(GPOS) %&gt;% summarise(delta_dxy = max(dxy)-min(dxy), sd_dxy = sd(dxy), delt_pi = max(c(max(PI_POP1),max(PI_POP2))) - min(c(min(PI_POP1),min(PI_POP2)))) %&gt;% ungroup() %&gt;% setNames(., nm = c(&#39;GPOS&#39;, str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):\\u0394~italic(d[xy])&#39;), str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):italic(d[xy])~(sd)&#39;), str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):\\u0394~italic(\\u03C0)&#39;))) %&gt;% gather(key = &#39;window&#39;, value = &#39;value&#39;,2:4) %&gt;% filter(window == str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):\\u0394~italic(d[xy])&#39;)) # set G x P traits to be imported traits &lt;- c(&quot;Bars.lm.50k.5k.txt.gz&quot;, &quot;Peduncle.lm.50k.5k.txt.gz&quot;, &quot;Snout.lm.50k.5k.txt.gz&quot;) # set trait figure panels trait_panels &lt;- c(Bars = str_c(&#39;bold(&#39;,project_case(&#39;h&#39;),&#39;)&#39;), Peduncle = str_c(&#39;bold(&#39;,project_case(&#39;i&#39;),&#39;)&#39;), Snout = str_c(&#39;bold(&#39;,project_case(&#39;j&#39;),&#39;)&#39;)) # import G x P data gxp_data &lt;- str_c(gxp_dir,traits) %&gt;% purrr::map(get_gxp) %&gt;% join_list() %&gt;% gather(key = &#39;window&#39;, value = &#39;value&#39;,2:4) # import genome wide Fst data summary -------- globals &lt;- vroom::vroom(global_fst_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% mutate(run = str_c(str_sub(run,1,3),loc,&#39;-&#39;,str_sub(run,5,7),loc), run = fct_reorder(run,weighted)) # dxy and pi are only shown for one exemplary population (/pair) # select dxy pair run (15 is one of the two central runs of the 28 pairs) # here, the 15th lowest fst value is identified as &quot;selector&quot; selectors_dxy &lt;- globals %&gt;% arrange(weighted) %&gt;% .$weighted %&gt;% .[15] # the dxy population pair corresponding to the selector is identified select_dxy_runs &lt;- globals %&gt;% filter(weighted %in% selectors_dxy) %&gt;% .$run %&gt;% as.character() # then thne dxy data is subset based on the selector dxy_select &lt;- dxy_data %&gt;% filter(run %in% select_dxy_runs) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;b&#39;),&#39;): italic(d[XY])&#39;)) # the pi data is filtered on a similar logic to the dxy data # first a table with the genome wide average pi for each population is compiled # (based on the first populations from the dxy data table # which contains pi for both populations) pi_summary_1 &lt;- dxy_data %&gt;% group_by(pop1,run) %&gt;% summarise(avg_pi = mean(PI_POP1)) %&gt;% ungroup() %&gt;% purrr::set_names(., nm = c(&#39;pop&#39;,&#39;run&#39;,&#39;avg_pi&#39;)) # the mean genome wide average pi is compiled for all the second populations # from the dxy data # then, the average of all the comparisons is computed for each population pi_summary &lt;- dxy_data %&gt;% group_by(pop2,run) %&gt;% summarise(avg_pi = mean(PI_POP2)) %&gt;% ungroup() %&gt;% purrr::set_names(., nm = c(&#39;pop&#39;,&#39;run&#39;,&#39;avg_pi&#39;)) %&gt;% bind_rows(pi_summary_1) %&gt;% group_by(pop) %&gt;% summarise(n = length(pop), mean_pi = mean(avg_pi), min_pi = min(avg_pi), max_pi = max(avg_pi), sd_pi = sd(avg_pi)) %&gt;% arrange(n) # one of the central populations with respect to average genome # wide pi is identified # for this, the 7th lowest pi value of the 14 populations is # determined as &quot;selector&quot; selectors_pi &lt;- pi_summary %&gt;% .$mean_pi %&gt;% sort() %&gt;% .[7] # the respective population is identified select_pi_pops &lt;- pi_summary %&gt;% filter(mean_pi %in% selectors_pi) %&gt;% .$pop %&gt;% as.character() # then the dxy data is subset by that population and the average pi over # all pair-wise runs is calculated for each window pi_data_select &lt;- dxy_data %&gt;% select(GPOS, PI_POP1, pop1 )%&gt;% purrr::set_names(., nm = c(&#39;GPOS&#39;,&#39;pi&#39;,&#39;pop&#39;)) %&gt;% bind_rows(.,dxy_data %&gt;% select(GPOS, PI_POP2, pop2 )%&gt;% purrr::set_names(., nm = c(&#39;GPOS&#39;,&#39;pi&#39;,&#39;pop&#39;))) %&gt;% group_by(GPOS,pop) %&gt;% summarise(n = length(pop), mean_pi = mean(pi), min_pi = min(pi), max_pi = max(pi), sd_pi = sd(pi)) %&gt;% filter(pop %in% select_pi_pops) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;c&#39;),&#39;):~\\u03C0&#39;)) # import recombination data recombination_data &lt;- vroom::vroom(recombination_file,delim = &#39;\\t&#39;) %&gt;% add_gpos() %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;d&#39;),&#39;):~\\u03C1&#39;)) # import topology weighting data twisst_data &lt;- tibble(loc = c(&#39;bel&#39;,&#39;hon&#39;), panel = c(&#39;f&#39;,&#39;g&#39;) %&gt;% project_case() %&gt;% str_c(&#39;bold(&#39;,.,&#39;)&#39;)) %&gt;% purrr::pmap(match_twisst_files) %&gt;% bind_rows() %&gt;% select(GPOS, topo3,topo_rel,window,weight) # the &quot;null-weighting&quot; is computed for both locations twisst_null &lt;- tibble(window = c(str_c(&#39;bold(&#39;,project_case(&#39;f&#39;),&#39;):~italic(w)[bel]&#39;), str_c(&#39;bold(&#39;,project_case(&#39;g&#39;),&#39;):~italic(w)[hon]&#39;)), weight = c(1/15, 1/105)) # combine data types -------- data &lt;- bind_rows(dxy_summary, fst_data, gxp_data) # import fst outliers outliers &lt;- vroom::vroom(outlier_table, delim = &#39;\\t&#39;) # the focal outlier IDs are set outlier_pick &lt;- c(&#39;LG04_1&#39;, &#39;LG12_3&#39;, &#39;LG12_4&#39;) # the table for the outlier labels is created outlier_label &lt;- outliers %&gt;% filter(gid %in% outlier_pick) %&gt;% mutate(label = letters[row_number()] %&gt;% project_inv_case(), x_shift_label = c(-1,-1.2,1)*10^7, gpos_label = gpos + x_shift_label, gpos_label2 = gpos_label - sign(x_shift_label) *.5*10^7, window = str_c(&#39;bold(&#39;,project_case(&#39;a&#39;),&#39;):joint~italic(F[ST])&#39;)) # the y height of the outlier labels and the corresponding tags is set outlier_y &lt;- .45 outlier_yend &lt;- .475 # the icons for the traits of the GxP are loaded trait_tibble &lt;- tibble(window = c(&quot;bold(h):italic(p)[Bars]&quot;, &quot;bold(i):italic(p)[Peduncle]&quot;, &quot;bold(j):italic(p)[Snout]&quot;), grob = hypo_trait_img$grob_circle[hypo_trait_img$trait %in% c(&#39;Bars&#39;, &#39;Peduncle&#39;, &#39;Snout&#39;)]) # finally, the figure is being put together p_done &lt;- ggplot()+ # add gray/white LGs background geom_hypo_LG()+ # the red highlights for the outlier regions are added geom_vline(data = outliers, aes(xintercept = gpos), color = outlr_clr)+ # the tags of the outlier labels are added geom_segment(data = outlier_label, aes(x = gpos, xend = gpos_label2, y = outlier_y, yend = outlier_yend), color = alpha(outlr_clr,1), size = .2)+ # the outlier labels are added geom_text(data = outlier_label, aes(x = gpos_label, y = outlier_yend, label = label), color = alpha(outlr_clr,1), fontface = &#39;bold&#39;, size = plot_text_size / ggplot2:::.pt)+ # the fst, delta dxy and gxp data is plotted geom_point(data = data, aes(x = GPOS, y = value),size = plot_size, color = plot_clr) + # the dxy data is plotted geom_point(data = dxy_select,aes(x = GPOS, y = dxy),size = plot_size, color = plot_clr)+ # the pi data is plotted geom_point(data = pi_data_select, aes(x = GPOS, y = mean_pi),size = plot_size, color = plot_clr) + # the roh data is plotted geom_point(data = recombination_data, aes(x = GPOS, y = RHO),size = plot_size, color = plot_clr) + # the smoothed rho is plotted geom_smooth(data = recombination_data, aes(x = GPOS, y = RHO, group = CHROM), color = &#39;red&#39;, se = FALSE, size = .4) + # the topology weighting data is plotted geom_line(data = twisst_data, aes(x = GPOS, y = weight, color = topo_rel), size = .4) + # the null weighting is added geom_hline(data = twisst_null, aes(yintercept = weight), color = rgb(1, 1, 1, .5), size = .4) + # the trait icons are added geom_hypo_grob(data = trait_tibble, aes(grob = grob, angle = 0, height = .65), inherit.aes = FALSE, x = .95, y = 0.65)+ # setting the scales scale_fill_hypo_LG_bg() + scale_x_hypo_LG()+ scale_color_gradient( low = &quot;#f0a830ff&quot;, high = &quot;#084082ff&quot;, guide = FALSE)+ # organizing the plot across panels facet_grid(window~.,scales = &#39;free&#39;,switch = &#39;y&#39;, labeller = label_parsed)+ # tweak plot appreance theme_hypo()+ theme(text = element_text(size = plot_text_size), legend.position = &#39;bottom&#39;, axis.title = element_blank(), strip.text = element_text(size = plot_text_size), strip.background = element_blank(), strip.placement = &#39;outside&#39;) Finally, we can export Figure S8. hypo_save(p_done, filename = &#39;figures/SF8.png&#39;, width = f_width, height = f_width * .9, dpi = 600, type = &quot;cairo&quot;, comment = plot_comment) system(&quot;convert figures/SF8.png figures/SF8.pdf&quot;) system(&quot;rm figures/SF8.png&quot;) create_metadata &lt;- str_c(&quot;exiftool -overwrite_original -Description=\\&quot;&quot;, plot_comment, &quot;\\&quot; figures/SF8.pdf&quot;) system(create_metadata) "],
["supplementary-figure-9.html", "36 Supplementary Figure 9 36.1 Summary 36.2 Details of plot_SF9.R", " 36 Supplementary Figure 9 36.1 Summary This is the accessory documentation of Figure S9. The Figure can be recreated by running the R script plot_SF9.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF9.R \\ 2_analysis/pi/50k/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz 36.2 Details of plot_SF9.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 36.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF9.R \\ # 2_analysis/pi/50k/ \\ # 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz # =============================================================== # This script produces Suppl. Figure 9 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/pi/50k/&#39;, # &#39;2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz&#39;) # script_name &lt;- &quot;R/fig/plot_SF9.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(vroom) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF9.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/pi/50k/ #&gt; ★ 1: 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz #&gt; ────────────────────────────────────────── /current/working/directory ── # config ----------------------- pi_path &lt;- as.character(args[1]) rho_path &lt;- as.character(args[2]) # locate pi data files files &lt;- dir(pi_path, pattern = &#39;^pi.[a-z]{6}.50k&#39;) # load pi data data &lt;- str_c(pi_path, files) %&gt;% purrr::map(get_pi) %&gt;% bind_rows() # compute genome wide average pi for the subplot order global_bar &lt;- data %&gt;% filter( BIN_START %% 50000 == 1) %&gt;% select(N_SITES, PI, spec) %&gt;% group_by(spec) %&gt;% summarise(genome_wide_pi = sum(N_SITES*PI)/sum(N_SITES)) %&gt;% arrange(genome_wide_pi) %&gt;% ungroup() %&gt;% mutate(spec = fct_reorder(.f = spec, .x = genome_wide_pi), scaled_pi = genome_wide_pi/max(genome_wide_pi)) # load recombination data rho_data &lt;- vroom(rho_path, delim = &#39;\\t&#39;) %&gt;% select(-BIN_END) # merge pi and recombination data combined_data &lt;- data %&gt;% # filter pi data to &quot;non-overlapping&quot; windows filter(BIN_START %% 50000 == 1 ) %&gt;% # reorder populations by genome wide average pi mutate(spec = factor(spec, levels = levels(global_bar$spec))) %&gt;% # merge with recombination data left_join(rho_data, by = c(CHROM = &#39;CHROM&#39;, BIN_START = &#39;BIN_START&#39;)) # create table with fish annotations grob_tibble2 &lt;- global_bar$spec %&gt;% purrr::map(fish_plot2) %&gt;% bind_rows() # compose final figure p &lt;- combined_data %&gt;% ggplot()+ # add fish annotations geom_hypo_grob2(data = grob_tibble2, aes(grob = grob, rel_x = .25,rel_y = .75), angle = 0, height = .5,width = .5)+ # add hex-bin desity layer geom_hex(bins = 30,color = rgb(0,0,0,.3), aes(fill=log10(..count..), x = RHO, y = PI))+ # general plot structure (separated by run) facet_wrap(spec ~., ncol = 3)+ # set axis layout and color scheme scale_x_continuous(name = expression(rho))+ scale_y_continuous(name = expression(pi))+ scico::scale_fill_scico(palette = &#39;berlin&#39;) + # customize legend guides(fill = guide_colorbar(direction = &#39;horizontal&#39;, title.position = &#39;top&#39;, barheight = unit(7,&#39;pt&#39;), barwidth = unit(130,&#39;pt&#39;)))+ # general plot layout theme_minimal()+ theme(legend.position = c(.84,.01), strip.text = element_blank()) Finally, we can export Figure S9. # export final figure hypo_save(filename = &#39;figures/SF9.pdf&#39;, plot = p, width = 8, height = 10, comment = plot_comment) # =============== combined_data %&gt;% filter( BIN_START %% 50000 == 1) %&gt;% group_by(spec) %&gt;% summarise(genom_avg_pi = sum(PI*N_SITES)/sum(N_SITES)) %&gt;% write_tsv(&quot;2_analysis/summaries/pi_globals.tsv&quot;) "],
["supplementary-figure-10.html", "37 Supplementary Figure 10 37.1 Summary 37.2 Details of plot_SF10.R", " 37 Supplementary Figure 10 37.1 Summary This is the accessory documentation of Figure S10. The Figure can be recreated by running the R script plot_SF10.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF10.R \\ 2_analysis/dxy/50k/ 37.2 Details of plot_SF10.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 37.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF10.R \\ # 2_analysis/dxy/50k/ # =============================================================== # This script produces Suppl. Figure 10 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/dxy/50k/&quot;) # script_name &lt;- &quot;R/fig/plot_SF10.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypogen) library(hypoimg) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF10.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- dxy_path &lt;- as.character(args[1]) # locate dxy data files files &lt;- dir(dxy_path) # load dxy data data &lt;- str_c(dxy_path,files) %&gt;% purrr::map(get_dxy) %&gt;% bind_rows() %&gt;% purrr::set_names(., nm = c(&#39;scaffold&#39;, &#39;start&#39;, &#39;end&#39;, &#39;mid&#39;, &#39;sites&#39;, &#39;pi_pop1&#39;, &#39;pi_pop2&#39;, &#39;dxy&#39;, &#39;fst&#39;, &#39;GSTART&#39;, &#39;gpos&#39;, &#39;run&#39;)) genome_wide_avg &lt;- data %&gt;% group_by(run) %&gt;% summarise(avg_dxy = mean(dxy)) %&gt;% ungroup() %&gt;% arrange(avg_dxy) model_data &lt;- data %&gt;% pivot_longer(cols = starts_with(&quot;pi_pop&quot;), names_to = &quot;pi_pop&quot;, values_to= &quot;pi&quot;) %&gt;% mutate(pop = str_remove(pi_pop,&quot;pi_pop&quot;) %&gt;% str_c(&quot;Pop. &quot;,.)) %&gt;% # filter fst data to &quot;non-overlapping&quot; windows filter(start %% 50000 == 1 ) %&gt;% group_by(run, pop) %&gt;% nest() %&gt;% mutate(mod = map(data, function(data){lm(pi ~ dxy, data = data)})) %&gt;% bind_cols(., summarise_model(.)) dxy_subplot &lt;- function(select_idx){ run_select &lt;- genome_wide_avg$run[select_idx] plt_data &lt;- data %&gt;% filter(run %in% run_select) %&gt;% pivot_longer(cols = starts_with(&quot;pi_pop&quot;), names_to = &quot;pi_pop&quot;, values_to= &quot;pi&quot;) %&gt;% mutate(pop = str_remove(pi_pop,&quot;pi_pop&quot;) %&gt;% str_c(&quot;Pop. &quot;,.)) base_lwd &lt;- .15 base_line_clr &lt;- &quot;black&quot; p &lt;- plt_data %&gt;% ggplot(aes(x = dxy, y = pi))+ facet_grid(pop ~ run,switch = &quot;y&quot;)+ geom_hex(bins = 30, color = rgb(0,0,0,.3), aes(fill=log10(..count..)))+ # add regression line geom_abline(data = model_data %&gt;% filter(run %in% run_select), color = rgb(1,1,1,.8), linetype = 2, aes(intercept = intercept, slope = slope)) + # add R^2 label geom_text(data = model_data%&gt;% filter(run %in% run_select), x = 0, y = .022, parse = TRUE, hjust = 0, vjust = 1, size = 3, aes(label = str_c(&#39;italic(R)^2:~&#39;,round(r.squared, 3)))) + scale_y_continuous(&quot;\\U03C0&quot;, breaks = c(0,.01,.02),labels = c(&quot;0&quot;, &quot;0.01&quot;, &quot;0.02&quot;))+ scale_x_continuous(expression(italic(d[XY])), breaks = c(0,.01,.02),labels = c(&quot;0&quot;, &quot;0.01&quot;, &quot;0.02&quot;))+ scico::scale_fill_scico(palette = &#39;berlin&#39;, limits = c(0,4.2))+ guides(fill = guide_colorbar(direction = &#39;horizontal&#39;, title.position = &#39;top&#39;, barheight = unit(7,&#39;pt&#39;), barwidth = unit(130,&#39;pt&#39;)))+ # general plot layout theme_minimal()+ theme(legend.position = &quot;bottom&quot;, axis.title.y = element_text(face = &quot;italic&quot;), strip.placement = &quot;outside&quot;, strip.background.x = element_rect(fill = rgb(.95,.95,.95), colour = base_line_clr,size = base_lwd), panel.border = element_rect(size = base_lwd, color = base_line_clr %&gt;% clr_lighten(factor = .8), fill = rgb(1,1,1,0)) ) p } ps &lt;- list(1:7, 8:14, 15:21, 22:28) %&gt;% map(dxy_subplot) p_done &lt;- plot_grid(ps[[1]] + theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()), ps[[2]] + theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()), ps[[3]] + theme(legend.position = &quot;none&quot;, axis.title.x = element_blank()), ps[[4]] + theme(legend.position = &quot;none&quot;), ps[[4]] %&gt;% get_legend(), ncol = 1, rel_heights = c(1,1,1,1,.3)) Finally, we can export Figure S10. # export final figure scl &lt;- 1.2 hypo_save(filename = &#39;figures/SF10.pdf&#39;, plot = p_done, width = f_width * scl, height = f_width * 1.15 * scl, device = cairo_pdf, comment = plot_comment) "],
["supplementary-figure-11.html", "38 Supplementary Figure 11 38.1 Summary 38.2 Details of plot_SF11.R", " 38 Supplementary Figure 11 38.1 Summary This is the accessory documentation of Figure S11. The Figure can be recreated by running the R script plot_SF11.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF11.R \\ 2_analysis/newhyb/nh_input/NH.Results/ 38.2 Details of plot_SF11.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 38.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF11.R \\ # 2_analysis/newhyb/nh_input/NH.Results/ # =============================================================== # This script produces Suppl. Figure 11 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/newhyb/nh_input/NH.Results/&quot;) # script_name &lt;- &quot;R/fig/plot_SF11.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(prismatic) library(paletteer) library(patchwork) library(ggtext) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF11.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/newhyb/nh_input/NH.Results/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- base_dir &lt;- as.character(args[1]) # locate hybridization data files folders &lt;- dir(base_dir) # load data and create plots by location p_loc &lt;- c(&quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) %&gt;% map(plot_loc) # compose figure from the individual panels p_done &lt;- (p_loc[[1]] + guides(fill = guide_legend(title = &quot;Hybrid Class&quot;)) + theme_hyb(legend.position = c(1,1)) ) + (p_loc[[2]] + theme_hyb() ) + (p_loc[[3]] + theme_hyb() ) + plot_layout(ncol = 1, heights = c(10,15,3) %&gt;% label_spacer())+ plot_annotation(tag_levels = &#39;a&#39;) Finally, we can export Figure S11. # export the final figure hypo_save(filename = &quot;figures/SF11.pdf&quot;, plot = p_done, height = 16, width = 10, device = cairo_pdf, comment = plot_comment) "],
["supplementary-figure-12.html", "39 Supplementary Figure 12 39.1 Summary 39.2 Details of plot_SF12.R", " 39 Supplementary Figure 12 39.1 Summary This is the accessory documentation of Figure S12. The Figure can be recreated by running the R script plot_SF12.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF12.R \\ ressources/species_order_alpha.txt \\ 2_analysis/dstats/hyp_ld05_dtrios_BBAA.txt \\ 2_analysis/dstats/BBAA_ld05.csv \\ 2_analysis/dstats/BBAA_sign_ld05.csv 39.2 Details of plot_SF12.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 39.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF12.R \\ # ressources/species_order_alpha.txt \\ # 2_analysis/dstats/hyp_ld05_dtrios_BBAA.txt \\ # 2_analysis/dstats/BBAA_ld05.csv \\ # 2_analysis/dstats/BBAA_sign_ld05.csv # =============================================================== # This script produces Suppl. Figure 12 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;ressources/species_order_alpha.txt&quot;, # &quot;2_analysis/dstats/hyp_ld05_dtrios_BBAA.txt&quot;, # &quot;2_analysis/dstats/BBAA_ld05.csv&quot;, # &quot;2_analysis/dstats/BBAA_sign_ld05.csv&quot;) # script_name &lt;- &quot;R/fig/plot_SF12.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(prismatic) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF12.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: ressources/species_order_alpha.txt #&gt; ★ 2: 2_analysis/dstats/hyp_ld05_dtrios_BBAA.txt #&gt; ★ 3: 2_analysis/dstats/BBAA_ld05.csv #&gt; ★ 4: 2_analysis/dstats/BBAA_sign_ld05.csv #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- species_order_file &lt;- as.character(args[1]) trios_file &lt;- as.character(args[2]) bbaa_file &lt;- as.character(args[3]) signif_file &lt;- as.character(args[4]) two_chr_to_sorted_pair &lt;- function(P2, P3, ...){ n1 &lt;- as.numeric(factor(P2, levels = sorter$group)) n2 &lt;- as.numeric(factor(P3, levels = sorter$group)) char_sorted &lt;- if(n1 &lt; n2){c(P2, P3)} else { c(P3, P2)} str_c(char_sorted[[1]], &quot;-&quot;, char_sorted[[2]]) } sorter &lt;- read_tsv(species_order_file, col_names = &quot;group&quot;) p_cap &lt;- 16 data &lt;- read_tsv(bbaa_file) %&gt;% filter(p_adjusted &lt;= .05) %&gt;% mutate(pair = pmap_chr(.,two_chr_to_sorted_pair)) %&gt;% group_by(pair) %&gt;% mutate(p_is_min = p_adjusted == min(p_adjusted)) %&gt;% filter(p_is_min) %&gt;% mutate(d_is_max = Dstatistic == max(Dstatistic)) %&gt;% filter(d_is_max) %&gt;% ungroup() %&gt;% separate(pair, into = c(&quot;p_left&quot;, &quot;p_right&quot;), sep = &quot;-&quot;, remove = FALSE) %&gt;% mutate(p_adjusted = if_else(p_adjusted &lt; 10^-p_cap,10^-p_cap, p_adjusted )) data_prep &lt;- data %&gt;% dplyr::select(p_left, p_right, Dstatistic, p_adjusted) %&gt;% bind_rows(data %&gt;% dplyr::select(p_left = p_right, p_right = p_left, Dstatistic, p_adjusted)) data_full &lt;- cross_df(list(p_left = sorter$group , p_right = sorter$group)) %&gt;% left_join(data_prep) %&gt;% mutate(p_left = factor(p_left, levels = sorter$group), p_right = factor(p_right, levels = rev(sorter$group))) data_signif &lt;- read_tsv(signif_file) %&gt;% mutate(pair = pmap_chr(.,two_chr_to_sorted_pair)) %&gt;% separate(pair, into = c(&quot;p_left&quot;, &quot;p_right&quot;), sep = &quot;-&quot;, remove = FALSE) %&gt;% mutate(p_left = factor(p_left, levels = sorter$group), p_right = factor(p_right, levels = rev(sorter$group))) %&gt;% mutate(p_adjusted = if_else(p_adjusted &lt; 10^-p_cap, 10^-p_cap, p_adjusted) ) clr &lt;- scales::colour_ramp(colors = RColorBrewer::brewer.pal(5,&quot;RdYlBu&quot;)[c(1,2,4,5)])((1:7)/7) %&gt;% clr_saturate(.1) d_lim &lt;- c(0, .01) p_lim &lt;- c(1, p_cap) p_done &lt;- data_full %&gt;% filter(p_left != &quot;Outgroup&quot; ) %&gt;% mutate(check1 = as.numeric(p_left), check2 = as.numeric(p_right)) %&gt;% filter(check2 &lt; 17 - check1) %&gt;% ggplot()+ geom_tile(aes(x = p_left, y = p_right, fill = Dstatistic, color = after_scale(clr_darken(fill))) ) + geom_point(data = data_signif %&gt;% filter(p_value &lt; .05), aes(x = p_left, y = p_right, size = -log10(p_adjusted)), shape = 1, alpha = .4) + scale_fill_gradientn(colours = rev(clr), limits = d_lim, na.value = rgb(1,1,1,.2)) + scale_size(range = c(.1, 6), limits = c(1,16), breaks = c(1,8,16)) + guides(fill = guide_colorbar(title = &quot;D&quot;, title.position = &quot;top&quot;, barwidth = unit(.6,&quot;npc&quot;), barheight = unit(4,&quot;pt&quot;), order = 1), size = guide_legend(title = &quot;-log&lt;sub&gt;10&lt;/sub&gt; *( p&lt;sub&gt;adjusted&lt;/sub&gt; )*&quot;, order = 2, title.position = &quot;top&quot;)) + coord_equal() + theme_minimal(base_size = plot_text_size) + theme(legend.position = c(.95,1), legend.text.align = .5,legend.title.align = 1, legend.justification = c(1,1), legend.direction = &quot;horizontal&quot;, legend.box.just = &quot;right&quot;, panel.grid = element_blank(), axis.text.x = element_text(angle = 90, vjust = .5), axis.title = element_blank(), legend.title = element_markdown()) Finally, we can export Figure S12. hypoimg::hypo_save(filename = &quot;figures/SF12.pdf&quot;, width = f_width_half, height = f_width_half, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["supplementary-figure-13.html", "40 Supplementary Figure 13 40.1 Summary 40.2 Details of plot_SF13.R", " 40 Supplementary Figure 13 40.1 Summary This is the accessory documentation of Figure S13. The Figure can be recreated by running the R script plot_SF13.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF13.R \\ 2_analysis/summaries/fst_outliers_998.tsv 40.2 Details of plot_SF13.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 40.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF13.R \\ # 2_analysis/summaries/fst_outliers_998.tsv # =============================================================== # This script produces Suppl. Figure 13 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/summaries/fst_outliers_998.tsv&quot;) # script_name &lt;- &quot;R/fig/plot_SF13.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(tidygraph) library(ggraph) library(prismatic) library(patchwork) library(IRanges) library(plyranges) library(hypogen) library(hypoimg) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF13.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- outlier_file &lt;- as.character(args[1]) outlier_regions &lt;- read_tsv(outlier_file) hap_to_perc &lt;- 100 / (166 * 165) iterations &lt;- c(&quot;25/10 kb&quot;,&quot;10/5 kb&quot;,&quot;15/7.5 kb&quot;) %&gt;% set_names(value = c(&quot;7&quot;, &quot;8&quot;, &quot;10&quot;)) idx &lt;- 10 import_map1 &lt;- function(idx, filtmode = &quot;bed95&quot;){ read_tsv(glue::glue(&quot;2_analysis/ibd/cM_converted/no_outgr_{filtmode}_{idx}.conv_filterd.tsv&quot;)) %&gt;% mutate(ibd_total = (ibd2_cM_m1 + 0.5*ibd1_cM_m1) / (ibd0_cM_m1 + ibd1_cM_m1 + ibd2_cM_m1)) } import_map2 &lt;- function(idx, filtmode = &quot;bed95&quot;){ read_tsv(glue::glue(&quot;2_analysis/ibd/cM_converted/no_outgr_{filtmode}_{idx}.conv_filterd.tsv&quot;)) %&gt;% mutate(ibd_total = (ibd2_cM_m2 + 0.5*ibd1_cM_m2) / (ibd0_cM_m2 + ibd1_cM_m2 + ibd2_cM_m2)) } import_bp &lt;- function(idx, filtmode = &quot;bed95&quot;){ read_tsv(glue::glue(&quot;2_analysis/ibd/cM_converted/no_outgr_{filtmode}_{idx}.conv_summary.tsv&quot;)) %&gt;% mutate(ibd_total = (ibd2_bp + 0.5*ibd1_bp) / (ibd0_bp + ibd1_bp + ibd2_bp)) } import_truffle &lt;- function(idx, filtmode = &quot;direct&quot;){ itteration_names &lt;- c(str_c(&quot;10-&quot;,6:3),&quot;7&quot;,&quot;8&quot;,&quot;9&quot;,&quot;10&quot;) read_tsv(glue::glue(&quot;2_analysis/ibd/no_outgr_{filtmode}_{itteration_names[idx]}.ibd.tsv&quot;)) %&gt;% mutate(ibd_total = (IBD2 + 0.5*IBD1) / (IBD0 + IBD1 + IBD2)) } iterations &lt;- c(str_c(&quot;2/5*10^&quot;,6:3,&quot; BP&quot;), &quot;25/10 kb&quot;, &quot;10/5 kb&quot;, &quot;7-5/3 kb&quot;, &quot;15/7.5 kb&quot;) plot_network &lt;- function(idx, filt = 0, import_fun = import_map1, x = &quot;cM_map1&quot;, filtmode = &quot;direct&quot;, x_ax = TRUE, y_ax = TRUE, ...){ clr2 &lt;- GenomicOriginsScripts::clr[!(names(GenomicOriginsScripts::clr) %in% c(&quot;flo&quot;, &quot;tor&quot;, &quot;tab&quot;))] clr2[&quot;uni&quot;] &lt;- rgb(.9,.9,.9) data &lt;- import_fun(idx, filtmode = filtmode) set.seed(42) p &lt;- data %&gt;% as_tbl_graph() %E&gt;% filter(ibd_total &gt; filt) %N&gt;% mutate(spec = str_sub(name,-6,-4), loc = str_sub(name,-3,-1)) %&gt;% ggraph( layout = &#39;fr&#39;, weights = ibd_total) + geom_edge_link(aes(alpha = ibd_total), color = rgb(.1,.1,.1), edge_width = .15) + geom_node_point(aes(fill = spec, shape = loc, color = after_scale(clr_darken(fill,.3))), size = .7) + labs(y = glue::glue(&quot;Seq. Length: {iterations[idx]}&quot;), x = x) + scale_fill_manual(&quot;Species&quot;, values = GenomicOriginsScripts::clr[!(names(GenomicOriginsScripts::clr) %in% c(&quot;flo&quot;, &quot;tor&quot;, &quot;tab&quot;))], labels = GenomicOriginsScripts::sp_labs)+ scale_edge_alpha_continuous(range = c(0,1), guide = &quot;none&quot;) + scale_shape_manual(&quot;Site&quot;, values = 21:23, labels = GenomicOriginsScripts::loc_names) + scale_x_continuous(position = &quot;top&quot;) + guides(fill = guide_legend(title.position = &quot;top&quot;, nrow = 2, override.aes = list(shape = 21, size = 2.5)), shape = guide_legend(title.position = &quot;top&quot;, nrow = 2, override.aes = list(size = 2.5))) + coord_equal() + theme(panel.background = element_blank(), axis.title.y = element_text(), axis.title.x = element_text()) if(!x_ax){ p &lt;- p + theme(axis.title.x = element_blank())} if(!y_ax){ p &lt;- p + theme(axis.title.y = element_blank())} p } plts_cM &lt;- tibble(idx = rep(c(7, 10, 8), each = 3), import_fun = rep(list(import_bp, import_map1, import_map2), 3), x = rep(c(&quot;bp_cM_filt.&quot;, &quot;cM_map1&quot;, &quot;cM_map2&quot;), 3), x_ax = rep(c(TRUE, FALSE), c(3, 6)), y_ax = rep(FALSE, 9), filtmode = &quot;bed95&quot;) %&gt;% bind_rows(tibble(idx = rep(c(5, 8, 6), 2), import_fun = rep(list(import_truffle), 6), x = rep(c(&quot;truffle&quot;, &quot;bed95&quot;), each = 3), x_ax = rep(rep(c(TRUE, FALSE), 1:2), 2), y_ax = rep(c(TRUE, FALSE), each = 3), filtmode = rep(c(&quot;direct&quot;, &quot;bed95&quot;), each = 3)) ) %&gt;% left_join(tibble(x = c(&quot;truffle&quot;, &quot;bed95&quot;, &quot;bp_cM_filt.&quot;, &quot;cM_map1&quot;, &quot;cM_map2&quot;), plot_order = seq_along(x))) %&gt;% arrange(plot_order) %&gt;% pmap(plot_network) p_done &lt;- plts_cM %&gt;% wrap_plots(nrow = 3, byrow = FALSE, guides = &quot;collect&quot;) + plot_annotation(tag_levels = &quot;a&quot;) &amp; theme(text = element_text(size = plot_text_size), plot.tag.position = c(0, 1), legend.position = &quot;bottom&quot;, legend.key = element_blank(), legend.direction = &quot;horizontal&quot;, legend.background = element_blank(), legend.box = &quot;horizontal&quot;, legend.text.align = 0, plot.subtitle = element_text()) Finally, we can export Figure S13. hypo_save(plot = p_done, filename = &quot;figures/SF13.png&quot;, width = f_width, height = .75*f_width, dpi = 600, type = &quot;cairo&quot;, bg = &quot;transparent&quot;, comment = plot_comment) system(&quot;convert figures/SF13.png figures/SF13.pdf&quot;) system(&quot;rm figures/SF13.png&quot;) create_metadata &lt;- str_c(&quot;exiftool -overwrite_original -Description=\\&quot;&quot;, plot_comment, &quot;\\&quot; figures/SF13.pdf&quot;) system(create_metadata) "],
["supplementary-figure-14.html", "41 Supplementary Figure 14 41.1 Summary 41.2 Details of plot_SF14.R", " 41 Supplementary Figure 14 41.1 Summary This is the accessory documentation of Figure S14. The Figure can be recreated by running the R script plot_SF14.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF14.R \\ 2_analysis/raxml/lg04.1_155N.raxml.support \\ 2_analysis/raxml/lg12.3_155N.raxml.support \\ 2_analysis/raxml/lg12.4_155N.raxml.support 41.2 Details of plot_SF14.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 41.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF14.R \\ # 2_analysis/raxml/lg04.1_155N.raxml.support \\ # 2_analysis/raxml/lg12.3_155N.raxml.support \\ # 2_analysis/raxml/lg12.4_155N.raxml.support # =============================================================== # This script produces Suppl. Figure 14 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/raxml/lg04.1_155N.raxml.support&quot;, # &quot;2_analysis/raxml/lg12.3_155N.raxml.support&quot;, # &quot;2_analysis/raxml/lg12.4_155N.raxml.support&quot;) # script_name &lt;- &quot;R/fig/plot_SF14.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ape) library(ggtree) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF14.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/raxml/lg04.1_155N.raxml.support #&gt; ★ 2: 2_analysis/raxml/lg12.3_155N.raxml.support #&gt; ★ 3: 2_analysis/raxml/lg12.4_155N.raxml.support #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- tree_file_lg04_1 &lt;- as.character(args[1]) tree_file_lg12_3 &lt;- as.character(args[2]) tree_file_lg12_4 &lt;- as.character(args[3]) trees &lt;- c(tree_file_lg04_1, tree_file_lg12_3, tree_file_lg12_4) %&gt;% map(.f = function(file){ read.tree(file) %&gt;% root(phy = ., outgroup = &quot;PL17_160floflo&quot;)} ) clr_neutral &lt;- rgb(.6, .6, .6) lyout &lt;- &#39;circular&#39; tree_data &lt;- trees %&gt;% map(.f = function(tree_in){ open_tree(ggtree(tree_in, layout = lyout), 180) %&gt;% .$data %&gt;% mutate(spec = ifelse(isTip, str_sub(label, -6, -4), &quot;ungrouped&quot;), support = as.numeric(label), support_class = cut(support, c(0,50,70,90,100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;)) )} ) p1 &lt;- plot_outl_tree(tree_data[[1]]) p2 &lt;- plot_outl_tree(tree_data[[2]], show_legend = FALSE) p3 &lt;- plot_outl_tree(tree_data[[3]], show_legend = FALSE) p_done &lt;- p1 + p2 + p3 + plot_annotation(tag_levels = &#39;a&#39;) + plot_layout(ncol = 1) Finally, we can export Figure S14. hypo_save(plot = p_done, filename = &quot;figures/SF14.pdf&quot;, width = f_width, height = f_width * 1.5, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["supplementary-figure-15.html", "42 Supplementary Figure 15 42.1 Summary 42.2 Details of plot_SF15.R", " 42 Supplementary Figure 15 42.1 Summary This is the accessory documentation of Figure S15. The Figure can be recreated by running the R script plot_SF15.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF15.R \\ 2_analysis/raxml/lg04.1_hySN.raxml.support \\ 2_analysis/raxml/lg12.3_hySN.raxml.support \\ 2_analysis/raxml/lg12.4_hySN.raxml.support 42.2 Details of plot_SF15.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 42.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF15.R \\ # 2_analysis/raxml/lg04.1_hySN.raxml.support \\ # 2_analysis/raxml/lg12.3_hySN.raxml.support \\ # 2_analysis/raxml/lg12.4_hySN.raxml.support # =============================================================== # This script produces Suppl. Figure 15 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/raxml/lg04.1_hySN.raxml.support&quot;, # &quot;2_analysis/raxml/lg12.3_hySN.raxml.support&quot;, # &quot;2_analysis/raxml/lg12.4_hySN.raxml.support&quot;) # script_name &lt;- &quot;R/fig/plot_SF15.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ape) library(ggtree) library(patchwork) library(phangorn) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF15.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/raxml/lg04.1_hySN.raxml.support #&gt; ★ 2: 2_analysis/raxml/lg12.3_hySN.raxml.support #&gt; ★ 3: 2_analysis/raxml/lg12.4_hySN.raxml.support #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- tree_file_lg04_1 &lt;- as.character(args[1]) tree_file_lg12_3 &lt;- as.character(args[2]) tree_file_lg12_4 &lt;- as.character(args[3]) trees &lt;- c(tree_file_lg04_1, tree_file_lg12_3, tree_file_lg12_4) %&gt;% map(.f = function(file){ read.tree(file) %&gt;% root(phy = ., outgroup = c(&quot;28393torpan&quot;, &quot;s_tort_3torpan&quot;, &quot;20478tabhon&quot; )) %&gt;% midpoint()} ) clr_neutral &lt;- rgb(.6, .6, .6) lyout &lt;- &#39;circular&#39; tree_data &lt;- trees %&gt;% map(.f = function(tree_in){ open_tree(ggtree(tree_in, layout = lyout), 180) %&gt;% .$data %&gt;% mutate(spec = ifelse(isTip, str_sub(label, -6, -4), &quot;ungrouped&quot;), support = as.numeric(label), support_class = cut(support, c(0,50,70,90,100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;)) )} ) p1 &lt;- plot_outl_tree_s(tree_data[[1]]) p2 &lt;- plot_outl_tree_s(tree_data[[2]], show_legend = FALSE) p3 &lt;- plot_outl_tree_s(tree_data[[3]], show_legend = FALSE) p_done &lt;- p1 + p2 + p3 + plot_annotation(tag_levels = &#39;a&#39;) + plot_layout(ncol = 1) Finally, we can export Figure S15. hypo_save(plot = p_done, filename = &quot;figures/SF15.pdf&quot;, width = f_width, height = f_width * 1.5, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["supplementary-figure-16.html", "43 Supplementary Figure 16 43.1 Summary 43.2 Details of plot_SF16.R", " 43 Supplementary Figure 16 43.1 Summary This is the accessory documentation of Figure S16. The Figure can be recreated by running the R script plot_SF16.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF16.R \\ 2_analysis/admixture/ \\ metadata/phenotypes.sc 43.2 Details of plot_SF16.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 43.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF16.R \\ # 2_analysis/admixture/ \\ # metadata/phenotypes.sc # =============================================================== # This script produces Suppl. Figure 16 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c( &quot;2_analysis/admixture/&quot;, &quot;metadata/phenotypes.sc&quot;) # script_name &lt;- &quot;R/fig/plot_SF16.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(paletteer) library(patchwork) library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_SF16.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/admixture/ #&gt; ★ 2: metadata/phenotypes.sc #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- admx_path &lt;- as.character(args[1]) pheno_file &lt;- as.character(args[2]) # load outlier window IDs (crop from admixture result file names) gids &lt;- dir(admx_path, pattern = &quot;pop.*15.txt&quot;) %&gt;% str_remove(&quot;pop.&quot;) %&gt;% str_remove(&quot;.15.txt&quot;) # load phenotype data pheno_data &lt;- read_sc(pheno_file) %&gt;% select(id, Bars, Peduncle, Snout) %&gt;% filter(!is.na(Bars)) # load admixture data data &lt;- gids %&gt;% map_dfr(data_amdx, admx_path = admx_path, k = 2) # associate phenotypic trait with outlier region pheno_facet &lt;- tibble( trait = c(&quot;Snout&quot;,&quot;Bars&quot;, &quot;Peduncle&quot;), gid = c(&quot;LG04_1&quot;, &quot;LG12_3&quot;, &quot;LG12_4&quot;)) %&gt;% mutate(facet_label = str_c(gid, &quot; / &quot;, trait)) # set outlier region labels gid_labels &lt;- c(LG04_1 = &quot;LG04 (A)&quot;, LG12_3 = &quot;LG12 (B)&quot;, LG12_4 = &quot;LG12 (C)&quot;) # set outlier region phenotypic traits gid_traits &lt;- c(LG04_1 = &quot;Snout&quot;, LG12_3 = &quot;Bars&quot;, LG12_4 = &quot;Peduncle&quot;) # set path to trait images trait_icons &lt;- c(LG04_1 = &quot;&lt;img src=&#39;ressources/img/snout_c.png&#39; width=&#39;60&#39; /&gt; &quot;, LG12_3 = &quot;&lt;img src=&#39;ressources/img/bars_c.png&#39; width=&#39;60&#39; /&gt; &quot;, LG12_4 = &quot;&lt;img src=&#39;ressources/img/peduncle_c.png&#39; width=&#39;60&#39; /&gt; &quot;) # format phenotype data pheno_plot_data &lt;- data %&gt;% filter(!duplicated(id)) %&gt;% select(id:id_order) %&gt;% left_join(pheno_data,by = c( id_nr = &quot;id&quot;)) %&gt;% arrange(spec, Bars, Peduncle, Snout, id) %&gt;% mutate(ord_nr = row_number()) %&gt;% pivot_longer(names_to = &quot;trait&quot;, values_to = &quot;phenotype&quot;, cols = Bars:Snout) %&gt;% left_join(pheno_facet) # helper for consistent sample order across all panels sample_order &lt;- pheno_plot_data %&gt;% filter(!duplicated(id)) %&gt;% select(id, ord_nr) # create plot panels a-c p_ad &lt;- c(&quot;LG04_1&quot;, &quot;LG12_3&quot;, &quot;LG12_4&quot;) %&gt;% purrr::map(adm_plot, data = data) # create dummy plot for the phenotype legend p_phno &lt;- pheno_plot_data %&gt;% ggplot(aes(x = ord_nr))+ geom_point(aes(y = trait, fill = factor(phenotype)),shape = 21)+ scale_fill_manual(&quot;Phenotype&lt;br&gt;&lt;img src=&#39;ressources/img/all_traits_c.png&#39; width=&#39;110&#39; /&gt;&quot;, values = c(`0` = &quot;white&quot;, `1` = &quot;black&quot;), na.value = &quot;gray&quot;, labels = c(&quot;absent&quot;, &quot;present&quot;, &quot;not scored&quot;))+ guides(fill = guide_legend(ncol = 1))+ theme_minimal()+ theme(legend.title = element_markdown(hjust = .5), legend.position = &quot;bottom&quot;) # prepare table with fish annotations for the species indication tib_drawing &lt;- pheno_plot_data %&gt;% group_by(spec) %&gt;% summarise(pos = (min(ord_nr)+max(ord_nr))*.5) %&gt;% ungroup() # create sub-plot for species indication p_spec &lt;- pheno_plot_data %&gt;% group_by(spec) %&gt;% summarise(start = min(ord_nr)-1, end = max(ord_nr)) %&gt;% ggplot(aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf))+ # add colored backgroud boxes geom_rect(aes(fill = spec), color = &quot;black&quot;)+ # add fish images (tib_drawing %&gt;% pmap(add_spec_drawing))+ # set axis layout scale_y_continuous(breaks = .5, labels = c( &quot;Species&quot;), limits = c(0,1))+ scale_x_discrete(breaks = sample_order$ord_nr, labels = sample_order$id, expand = c(0,0)) + # set species color scheme scale_fill_manual(&quot;Species&quot;, values = clr, labels = sp_labs)+ # set general plot layout theme_minimal()+ theme(plot.title = element_text(size = 9), legend.position = &quot;bottom&quot;, legend.text.align = 0, axis.title = element_blank(), axis.ticks = element_blank(), axis.text.x = element_blank()) # create sub-plot for samplong location indication p_loc &lt;- pheno_plot_data %&gt;% ggplot(aes(x = factor(ord_nr)))+ # add colored boxes geom_raster(aes(y = 0, fill = loc))+ # set axis layout scale_y_continuous(breaks = c(0),labels = c(&quot;Location&quot;))+ scale_x_discrete(breaks = sample_order$ord_nr, labels = sample_order$id) + # set location color scheme scale_fill_manual(&quot;Location&quot;, values = clr_loc, loc_names)+ # set general plot layout theme_minimal()+ theme(plot.title = element_text(size = 9), legend.position = &quot;bottom&quot;, axis.title = element_blank(), axis.ticks = element_blank(), axis.text.x = element_blank()) # compose legend from individual legend parts p_l &lt;- (get_legend(p_phno) %&gt;% ggdraw()) + (get_legend(p_spec) %&gt;% ggdraw()) + (get_legend(p_loc) %&gt;% ggdraw()) + plot_layout(nrow = 1) # finalize figure p_prep &lt;- p_ad[[1]] + p_ad[[2]] + p_ad[[3]]+ p_spec + p_loc + p_l + plot_layout(ncol = 1, heights = c(.4,.4,.4,.08,.02,.1)) &amp; theme(legend.position = &quot;none&quot;, axis.text = element_text(size = 12)) # crop final figure (remove whitespace on left margin) p_done &lt;- ggdraw(p_prep, xlim = c(.023,1)) Finally, we can export Figure S16. # export final figure scl &lt;- .9 hypo_save(&quot;figures/SF16.pdf&quot;, plot = p_done, width = 16*scl, height = 10*scl, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["supplementary-figure-17.html", "44 Supplementary Figure 17 44.1 Summary 44.2 Details of plot_SF17.R", " 44 Supplementary Figure 17 44.1 Summary This is the accessory documentation of Figure S17. The Figure can be recreated by running the R script plot_SF17.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF17.R \\ 2_analysis/astral/astral_5000x_5kb_v1_all.tre 44.2 Details of plot_SF17.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 44.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF17.R \\ # 2_analysis/astral/astral_5000x_5kb_v1_all.tre # =============================================================== # This script produces Suppl. Figure 17 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/astral/astral_5000x_5kb_v1_all.tre&quot;) # script_name &lt;- &quot;R/fig/plot_SF17.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ape) library(ggtree) library(tidygraph) library(ggraph) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF17.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/astral/astral_5000x_5kb_v1_all.tre #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- tree_hypo_file &lt;- as.character(args[1]) tree &lt;- read.tree(tree_hypo_file) tree$edge.length &lt;- replace(tree$edge.length, tree$edge.length == &quot;NaN&quot;, 0.05) # Set terminal branches to 0.05 tree$edge.length[c( 81, 83)] &lt;- tree$edge.length[c( 81, 83)] * 0.1 tree_rooted &lt;- root(phy = tree, outgroup = c(&quot;s_tort_3torpan&quot;, &quot;20478tabhon&quot;, &quot;28393torpan&quot;)) clr_neutral &lt;- rgb(.2, .2, .2) ### Prepare tree and categorize support values tree_plus &lt;- ggtree(tree_rooted) %&gt;% .$data %&gt;% mutate(spec = ifelse(isTip, str_sub(label, -6, -4), &quot;ungrouped&quot;), loc = ifelse(isTip, str_sub(label, -3, -1), &quot;ungrouped&quot;), support = as.numeric(label) * 100, support_class = cut(support, c(0,50,70,90,100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;)), `branch.length` = if_else(node %in% c( 212, 213), `branch.length` * .0001, `branch.length`), branch_type = if_else(node %in% c(212, 213), &quot;broken&quot;, &quot;whole&quot;) ) t_plot &lt;- (ggtree(tr = tree_plus, layout = &#39;fan&#39;, aes(color = spec, linetype = branch_type), size = .2)) + #%&gt;% geom_tippoint(aes(color = spec, shape = loc, fill = after_scale(color)), size = .5) + geom_nodepoint(data = tree_plus %&gt;% filter(!isTip, support_class != &quot;(0,50]&quot;), # Apply to nodes with support &gt;50 only aes(fill = support_class, size = support_class), shape = 21, color = clr_neutral) + scale_color_manual(values = c(GenomicOriginsScripts::clr2, ungrouped = &quot;gray60&quot;), labels = GenomicOriginsScripts::sp_labs) + scale_shape_manual(values = c(bel = 21, flo = 24, hon = 22, pan = 23), labels = GenomicOriginsScripts::loc_names) + scale_fill_manual(values = c(`(0,50]` = &quot;transparent&quot;, `(50,70]` = &quot;white&quot;, `(70,90]` = &quot;gray&quot;, `(90,100]` = &quot;black&quot;), drop = FALSE) + scale_size_manual(values = c(`(0,50]` = 0, `(50,70]` = .8, `(70,90]` = .8, `(90,100]` = .8), na.value = 0, drop = FALSE) + scale_linetype_manual(values = c(whole = 1, broken = 3), guide = &quot;none&quot;) + # Add scale bar: ggtree::geom_treescale(width = .2, x = .13, y = 85.5, offset = -7, linesize = .2, fontsize = plot_text_size/.pt, color = clr_neutral) + guides(fill = guide_legend(title = &quot;Node Support Class&quot;, title.position = &quot;top&quot;, nrow = 2, label.hjust = 0), size = guide_legend(title = &quot;Node Support Class&quot;, title.position = &quot;top&quot;, nrow = 2, label.hjust = 0), shape = guide_legend(title = &quot;Location&quot;, title.position = &quot;top&quot;, nrow = 2, label.hjust = 0), color = guide_legend(title = &quot;Species&quot;, title.position = &quot;top&quot;, ncol = 2, label.hjust = 0)) + theme_void() + theme(legend.position = &#39;bottom&#39;, legend.title.align = 0, legend.text = element_text(color = &quot;gray20&quot;), legend.title = element_text(color = &quot;gray20&quot;)) y_sep &lt;- .1 x_shift &lt;- .1 p_tdone &lt;- ggplot() + coord_equal(xlim = c(0, 1), ylim = c(0, 1), expand = 0) + annotation_custom(grob = ggplotGrob(t_plot + theme(legend.position = &quot;none&quot;)), ymin = 0 - y_sep , ymax = 1 + y_sep, xmin = 0 - x_shift, xmax = 1 + x_shift) + theme_void() p_done &lt;- cowplot::plot_grid(p_tdone, cowplot::get_legend(t_plot + theme_minimal(base_size = plot_text_size) + theme(legend.position = &quot;right&quot;, legend.title.align =0, legend.key.height = unit(7,&quot;pt&quot;))),rel_widths = c(1,.5)) Finally, we can export Figure S17. scl &lt;- .75 hypo_save(p_done, filename = &#39;figures/SF17.pdf&#39;, width = f_width, height = .6 * f_width, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["supplementary-figure-18.html", "45 Supplementary Figure 18 45.1 Summary 45.2 Details of plot_SF18.R", " 45 Supplementary Figure 18 45.1 Summary This is the accessory documentation of Figure S18. The Figure can be recreated by running the R script plot_SF18.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF18.R \\ 2_analysis/summaries/fst_outliers_998.tsv 45.2 Details of plot_SF18.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 45.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF18.R \\ # 2_analysis/summaries/fst_outliers_998.tsv # =============================================================== # This script produces Suppl. Figure 18 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/summaries/fst_outliers_998.tsv&quot;) # script_name &lt;- &quot;R/fig/plot_SF18.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(tidygraph) library(ggraph) library(prismatic) library(patchwork) library(IRanges) library(plyranges) library(hypogen) library(hypoimg) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF18.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- outlier_file &lt;- as.character(args[1]) outlier_regions &lt;- read_tsv(outlier_file) hap_to_perc &lt;- 100 / (166 * 165) iterations &lt;- c(str_c(&quot;2/5*10^&quot;,6:3,&quot; BP&quot;),&quot;25/10 kb&quot;,&quot;10/5 kb&quot;,&quot;7-5/3 kb&quot;,&quot;15/7.5 kb&quot;) itteration_names &lt;- c(str_c(&quot;10-&quot;,6:3),&quot;7&quot;,&quot;8&quot;,&quot;9&quot;,&quot;10&quot;) plot_ibd_gw &lt;- function(n_zeros, y_lim = c(0, 4), filtmode = &quot;direct&quot;, y_lab = &quot;&quot;){ data_seg &lt;- vroom::vroom(glue::glue(&quot;2_analysis/ibd/no_outgr_{filtmode}_{itteration_names[n_zeros]}.segments.tsv&quot;), delim = &quot;\\t&quot;, col_types = &quot;cccciidcdci&quot;) %&gt;% left_join(hypogen::hypo_chrom_start) %&gt;% mutate(start = POS * 10^6, end = start + (LENGTH * 10^6), gstart = GSTART + start, gend = start + (LENGTH * 10^6), ibd_hplo = str_remove(TYPE,&quot;IBD&quot;) %&gt;% as.integer()) data_seg %&gt;% dplyr::select(seqnames = CHROM,start,end,gstart,GSTART,TYPE,ibd_hplo,ID1,ID2) %&gt;% arrange(gstart) %&gt;% dplyr::select(-gstart) %&gt;% as_granges() %&gt;% GenomicRanges::coverage(weight = &quot;ibd_hplo&quot;) %&gt;% plyranges::as_ranges() %&gt;% as_tibble() %&gt;% dplyr::select(CHROM = seqnames, start, end, width, score) %&gt;% left_join(hypogen::hypo_chrom_start) %&gt;% mutate(gstart = GSTART + start, gend = GSTART + end) %&gt;% dplyr::select(CHROM, gstart, gend, score) %&gt;% pivot_longer(gstart:gend,values_to = &quot;GPOS&quot;, names_to = &quot;PART&quot;) %&gt;% ggplot() + geom_hypo_LG() + geom_vline(data = outlier_regions, aes(xintercept = gpos), color = rgb(1,0,0,.2), size = .3) + geom_step(aes(x = GPOS, y = score * hap_to_perc, group = CHROM), color = rgb(.3,.3,.3), size = .3) + geom_ribbon(aes(x = GPOS, ymin = 0, ymax = score * hap_to_perc, group = CHROM), fill = rgb(0,0,0,.5)) + scale_hypobg_manual(values = c(&quot;transparent&quot;,rgb(.9,.9,.9,.9),&quot;red&quot;,&quot;blue&quot;) %&gt;% set_names(nm = c(&quot;even&quot;, &quot;odd&quot;, &quot;a&quot;,&quot;b&quot;)), guide = &quot;none&quot;)+ scale_x_hypo_LG() + labs(y = str_c(&quot;IBD Score (&quot; , iterations[n_zeros], &quot;)&quot;)) + coord_cartesian(ylim = y_lim, expand = 0) + theme_hypo() } data_seg &lt;- vroom::vroom(glue::glue(&quot;2_analysis/ibd/no_outgr_direct_10.segments.tsv&quot;), delim = &quot;\\t&quot;, col_types = &quot;cccciidcdci&quot;) %&gt;% left_join(hypogen::hypo_chrom_start) %&gt;% mutate(start = POS * 10^6, end = start + (LENGTH * 10^6), gstart = GSTART + start, gend = start + (LENGTH * 10^6), ibd_hplo = str_remove(TYPE,&quot;IBD&quot;) %&gt;% as.integer()) data_compact &lt;- data_seg %&gt;% dplyr::select(seqnames = CHROM,start,end,gstart,GSTART,TYPE,ibd_hplo,ID1,ID2) %&gt;% arrange(gstart) %&gt;% dplyr::select(-gstart) %&gt;% as_granges() %&gt;% GenomicRanges::coverage(weight = &quot;ibd_hplo&quot;) %&gt;% plyranges::as_ranges() %&gt;% as_tibble() %&gt;% dplyr::select(CHROM = seqnames, start, end, width, score) %&gt;% left_join(hypogen::hypo_chrom_start) %&gt;% mutate(gstart = GSTART + start, gend = GSTART + end) %&gt;% dplyr::select(CHROM, gstart, gend, score) total_cov_lenght &lt;- data_compact %&gt;% mutate(length = gend-gstart) %&gt;% .$length %&gt;% sum() data_sorted &lt;- data_compact %&gt;% mutate(length = gend-gstart) %&gt;% group_by(score) %&gt;% summarise(length = sum(length)) %&gt;% ungroup() %&gt;% mutate(length = if_else(score == 0, length + hypo_karyotype$GEND[24]-total_cov_lenght, # attach uncovered chrom ends length), gend = cumsum(length), gstart = lag(gend,default = 0)) perc_cutoff &lt;- .95 perc_score &lt;- data_sorted %&gt;% filter(gstart &lt; hypo_karyotype$GEND[24] * perc_cutoff, gend &gt; hypo_karyotype$GEND[24] * perc_cutoff) %&gt;% .$score plts &lt;- c(5,8,6) %&gt;% map2(.y = list(c(0,26), # .55), c(0,26), # 4), c(0,26)), plot_ibd_gw, filtmode = &quot;direct&quot;) p_done &lt;- (plts[[1]] + plts[[2]] + geom_hline(yintercept = perc_score * hap_to_perc, color = &quot;#11C269&quot;, size = .3, alpha = .7) + plts[[3]] + plot_layout(ncol = 1)) + plot_annotation(tag_levels = &quot;a&quot;) &amp; theme(text = element_text(size = plot_text_size), plot.tag.position = c(0, 1), legend.position = &quot;bottom&quot;, legend.key = element_blank(), legend.direction = &quot;horizontal&quot;, legend.background = element_blank(), legend.box = &quot;horizontal&quot;, legend.text.align = 0) Finally, we can export Figure S18. hypo_save(plot = p_done, filename = &quot;figures/SF18.png&quot;, width = f_width, height = .55 * f_width, dpi = 600, type = &quot;cairo&quot;, bg = &quot;transparent&quot;, comment = plot_comment) system(&quot;convert figures/SF18.png figures/SF18.pdf&quot;) system(&quot;rm figures/SF18.png&quot;) create_metadata &lt;- str_c(&quot;exiftool -overwrite_original -Description=\\&quot;&quot;, plot_comment, &quot;\\&quot; figures/SF18.pdf&quot;) system(create_metadata) "],
["supplementary-figure-19.html", "46 Supplementary Figure 19 46.1 Summary 46.2 Details of plot_SF19.R", " 46 Supplementary Figure 19 46.1 Summary This is the accessory documentation of Figure S19. The Figure can be recreated by running the R script plot_SF19.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF19.R \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/geva/ \\ 2_analysis/GxP/bySNP/ 46.2 Details of plot_SF19.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 46.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF19.R \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # 2_analysis/geva/ \\ # 2_analysis/GxP/bySNP/ # =============================================================== # This script produces Suppl. Figure 19 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c( &quot;2_analysis/summaries/fst_outliers_998.tsv&quot;, &quot;2_analysis/geva/&quot;, &quot;2_analysis/GxP/bySNP/&quot; ) # script_name &lt;- &quot;R/fig/plot_SF19.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ggtext) library(ggpointdensity) library(scales) library(grid) library(prismatic) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_SF19.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 2: 2_analysis/geva/ #&gt; ★ 3: 2_analysis/GxP/bySNP/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- outlier_file &lt;- as.character(args[1]) geva_path &lt;- as.character(args[2]) gxp_path &lt;- as.character(args[3]) outlier_data &lt;- read_tsv(outlier_file) data1 &lt;- outlier_data[1:3,] %&gt;% pmap_dfr(get_gxp_and_geva) data2 &lt;- outlier_data[4:6,] %&gt;% pmap_dfr(get_gxp_and_geva) data3 &lt;- outlier_data[7:9,] %&gt;% pmap_dfr(get_gxp_and_geva) data4 &lt;- outlier_data[10:12,] %&gt;% pmap_dfr(get_gxp_and_geva) data5 &lt;- outlier_data[13:15,] %&gt;% pmap_dfr(get_gxp_and_geva) data6 &lt;- outlier_data[16:18,] %&gt;% pmap_dfr(get_gxp_and_geva) data &lt;- data1 %&gt;% bind_rows(data2) %&gt;% bind_rows(data3) %&gt;% bind_rows(data4) %&gt;% bind_rows(data5) %&gt;% bind_rows(data6) xrange &lt;- c(100, 10^6) color &lt;- rgb(1, 0.5, 0.16) base_length &lt;- 8 base_lwd &lt;- .15 base_line_clr &lt;- &quot;black&quot; splitage &lt;- tibble(intercept = 5000) gid_label &lt;- outlier_data$gid gid_label[c(2, 13, 14)] &lt;- c( LG04_1 = &quot;LG04 (A)&quot;, LG12_3 = &quot;LG12 (B)&quot;, LG12_4 = &quot;LG12 (C)&quot; ) gxp_clr &lt;- c(Bars = &quot;#79009f&quot;, Snout = &quot;#E48A00&quot;, Peduncle = &quot;#5B9E2D&quot;) %&gt;% darken(factor = .95) %&gt;% set_names(., nm = c(&quot;Bars&quot;, &quot;Snout&quot;, &quot;Peduncle&quot;)) annotation_grobs &lt;- tibble(svg = hypo_trait_img$grob_circle[hypo_trait_img$trait %in% c( &#39;Snout&#39;, &#39;Bars&#39;, &#39;Peduncle&#39;)], layer = c(4,3,7), color = gxp_clr[c(1,3,2)]) %&gt;% purrr::pmap(.l = ., .f = hypo_recolor_svg) %&gt;% set_names(nm = c( &quot;LG12_3&quot;,&quot;LG12_4&quot;,&quot;LG04_1&quot;)) annotation_grobs$LG12_3 &lt;- hypo_recolor_svg(annotation_grobs$LG12_3, layer = 7, color = gxp_clr[[1]] %&gt;% clr_desaturate %&gt;% clr_lighten(.25)) annotation_grobs_tib &lt;- tibble(gid = names(annotation_grobs), grob = annotation_grobs) %&gt;% mutate( gid_label = gid_label[gid], trait = factor( c( &quot;Bars&quot;, &quot;Peduncle&quot;, &quot;Snout&quot;), levels = c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;))) highlight_rects &lt;- tibble(trait = factor( c(NA, &quot;Snout&quot;, rep(NA, 10), &quot;Bars&quot;, &quot;Peduncle&quot;, rep(NA, 4)), levels = c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;)), gid_label = gid_label) p_1 &lt;- data %&gt;% pivot_longer(names_to = &quot;trait&quot;, values_to = &quot;p_wald&quot;, cols = Bars:Snout) %&gt;% mutate(trait = factor(trait, levels = c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;)), gid_label = gid_label[gid]) %&gt;% filter(Clock == &quot;J&quot;, Filtered == 1, gid %in% outlier_data$gid[1:9], !(gid %in% outlier_data$gid[2])) %&gt;% ggplot() p_2 &lt;- data %&gt;% pivot_longer(names_to = &quot;trait&quot;, values_to = &quot;p_wald&quot;, cols = Bars:Snout) %&gt;% mutate(trait = factor(trait, levels = c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;)), gid_label = gid_label[gid]) %&gt;% filter(Clock == &quot;J&quot;, Filtered == 1, gid %in% outlier_data$gid[10:18], !(gid %in% outlier_data$gid[c(13:14)])) %&gt;% ggplot() complete_p &lt;- function(p){ p + geom_pointdensity(size = plot_size, aes(x = PostMedian,y = p_wald)) + facet_grid(gid ~ trait, scales = &quot;free_y&quot; ) + scale_x_log10(labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x))) + scale_y_continuous(trans = reverselog_trans(10), #limits = c(10^0, 10^-90), labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x))) + scale_color_viridis_c(&quot;Density&quot;, option = &quot;B&quot;, limits = c(0,750)#, limits = c(0,1100) ) + labs(y = &quot;G x P *p* value &lt;sub&gt;Wald&lt;/sub&gt;&quot;, x = &quot;Derived allele age (generations)&quot;) + guides(color = guide_colorbar(title.position = &quot;top&quot;, barwidth = unit(.4, &quot;npc&quot;), barheight = unit(3, &quot;pt&quot;))) + theme_minimal() + theme(text = element_text(size = plot_text_size), axis.title.y = element_markdown(), legend.position = &quot;bottom&quot;, plot.subtitle = element_markdown(), axis.line = element_line(colour = base_line_clr, size = base_lwd), strip.background = element_blank(), panel.grid.minor = element_blank(), panel.grid.major = element_line(size = plot_lwd)) } p_done &lt;- cowplot::plot_grid( complete_p(p_1) + theme(legend.position = &quot;none&quot;), complete_p(p_2) + theme(legend.box.margin = unit(c(7,0,7,0),&quot;pt&quot;))) Finally, we can export Figure S19. hypo_save(plot = p_done, filename = &quot;figures/SF19.pdf&quot;, width = f_width, height = f_width, comment = plot_comment, device = cairo_pdf, bg = &quot;transparent&quot;) "],
["supplementary-figure-20.html", "47 Supplementary Figure 20 47.1 Summary 47.2 Details of plot_SF20.R", " 47 Supplementary Figure 20 47.1 Summary This is the accessory documentation of Figure S20. The Figure can be recreated by running the R script plot_SF20.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF20.R \\ 2_analysis/GxP/50000/ 47.2 Details of plot_SF20.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 47.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF20.R \\ # 2_analysis/GxP/50000/ # =============================================================== # This script produces Suppl. Figure 20 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/GxP/50000/&#39;) # script_name &lt;- &quot;R/fig/plot_SF20.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF20.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/GxP/50000/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- gxp_path &lt;- as.character(args[1]) # configure which gxp data to load trait_tib &lt;- tibble(file = dir(gxp_path) %&gt;% .[str_detect(.,&quot;Bars|Peduncle|Snout&quot;)]) %&gt;% mutate(prep = file) %&gt;% separate(prep , into = c(&quot;trait&quot;, &quot;model_type&quot;, &quot;win&quot;, &quot;step&quot;, &quot;filetype&quot;, &quot;zip&quot;), sep = &quot;\\\\.&quot;) %&gt;% select(file, trait, model_type) %&gt;% mutate(path = gxp_path) # load gxp data data &lt;- pmap_dfr(trait_tib,get_gxp_both_models) # compose final figure p_done &lt;- data %&gt;% ggplot(aes(x = gpos, y = AVG_p_wald))+ # add gray/white LGs background geom_hypo_LG()+ # add gxp data points geom_point(color = plot_clr, size = .3)+ # set axis layout scale_x_hypo_LG()+ scale_fill_hypo_LG_bg()+ # set axis titles labs(y = expression(G~x~P~(average~italic(p)[wald])))+ # general plot structure separated by model type and trait facet_grid(trait+model_type ~ ., scales = &quot;free_y&quot;)+ # general plot layout theme_hypo() Finally, we can export Figure S20. # export final figure hypo_save(filename = &quot;figures/SF20.png&quot;, plot = p_done, width = 11, height = 7, dpi = 600, type = &quot;cairo&quot;, comment = plot_comment) system(&quot;convert figures/SF20.png figures/SF20.pdf&quot;) system(&quot;rm figures/SF20.png&quot;) create_metadata &lt;- str_c(&quot;exiftool -overwrite_original -Description=\\&quot;&quot;, plot_comment, &quot;\\&quot; figures/SF20.pdf&quot;) system(create_metadata) "],
["supplementary-figure-21.html", "48 Supplementary Figure 21 48.1 Summary 48.2 Details of plot_SF21.R", " 48 Supplementary Figure 21 48.1 Summary This is the accessory documentation of Figure S21. The Figure can be recreated by running the R script plot_SF21.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF21.R \\ 2_analysis/pi/50k/ 48.2 Details of plot_SF21.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 48.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF21.R \\ # 2_analysis/pi/50k/ # =============================================================== # This script produces Suppl. Figure 21 of the study &quot;Rapid radiation in a # highly diverse marine environment&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/pi/50k/&#39;) # script_name &lt;- &quot;R/fig/plot_SF21.R&quot; args &lt;- commandArgs(trailingOnly = FALSE) The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. # setup ----------------------- renv::activate() library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF21.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/pi/50k/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- pi_path &lt;- as.character(args[1]) # locate pi data files files &lt;- dir(pi_path, pattern = &#39;^pi.[a-z]{6}.50k&#39;) # load pi data data &lt;- str_c(pi_path, files) %&gt;% purrr::map(get_pi) %&gt;% bind_rows() # create table for the indication of genome wide average pi in the plot background # (rescale covered pi range to the extent of the genome) global_bar &lt;- data %&gt;% # filter to non-overlaping windows only filter( BIN_START %% 50000 == 1) %&gt;% select(N_SITES, PI, spec) %&gt;% group_by(spec) %&gt;% summarise(genome_wide_pi = sum(N_SITES*PI)/sum(N_SITES)) %&gt;% arrange(genome_wide_pi) %&gt;% ungroup() %&gt;% mutate(spec = fct_reorder(.f = spec, .x = genome_wide_pi), scaled_pi = genome_wide_pi/max(genome_wide_pi)) global_bar %&gt;% write_tsv(&quot;2_analysis/summaries/pi_globals.tsv&quot;) # prepare plot annotaton images grob_tibble &lt;- global_bar$spec %&gt;% purrr::map(fish_plot) %&gt;% bind_rows() # prepare plotting elements -------- # pre-define secondary x-axis breaks sc_ax &lt;- scales::cbreaks(c(0,max(global_bar$genome_wide_pi)), scales::pretty_breaks(4)) # pre-define secondary x-axis labels labels &lt;- str_c(c(&quot;&quot;, sc_ax$breaks[2:6]*1000), c(&quot;0&quot;, rep(&quot;\\u00B710^-3&quot;,5))) # sort pair-wise population comparisons by average genome wide pi data &lt;- data %&gt;% mutate(spec = factor(spec, levels = levels(global_bar$spec))) # compose final figure p_done &lt;- ggplot()+ # general plot structure separated by run facet_wrap( .~spec, as.table = TRUE, ncol = 1, dir = &#39;v&#39;)+ # add genome wide average pi in the background geom_rect(data = global_bar %&gt;% mutate(xmax = scaled_pi * hypo_karyotype$GEND[24]), aes(xmin = 0, xmax = xmax, ymin = -Inf, ymax = Inf), color = rgb(1,1,1,0), fill = clr_below)+ # add LG borders geom_vline(data = hypogen::hypo_karyotype,aes(xintercept = GEND),color = hypo_clr_lg)+ # add pi data points geom_point(data = data, aes(x = gpos, y = PI), size=.2, color = plot_clr) + # add fish images geom_hypo_grob2(data = grob_tibble, aes(grob = grob, rel_x = .975, rel_y = .5), angle = 0, height = .8, width = .12)+ # set axis layout scale_x_hypo_LG(sec.axis = sec_axis(~ ./hypo_karyotype$GEND[24], breaks = (sc_ax$breaks/max(global_bar$genome_wide_pi)), labels = labels, name = &quot;Genomic position/ Genome wide *\\u03C0*&quot;))+ scale_y_continuous(name = &quot;*\\u03C0*&quot;, breaks = c(0,.01,.02))+ # set plot extent coord_cartesian(xlim = c(0, hypo_karyotype$GEND[24]*1.06))+ # general plot layout theme_hypo()+ theme(strip.text = element_blank(), legend.position = &#39;none&#39;, axis.title.x = element_markdown(), axis.title.y = element_markdown(), axis.text.x.bottom = element_markdown(colour = &#39;darkgray&#39;)) Finally, we can export Figure S21. # export final figure hypo_save(filename = &#39;figures/SF21.png&#39;, plot = p_done, width = 8, height = 8, dpi = 600, type = &quot;cairo&quot;, comment = plot_comment) system(&quot;convert figures/SF21.png figures/SF21.pdf&quot;) system(&quot;rm figures/SF21.png&quot;) create_metadata &lt;- str_c(&quot;exiftool -overwrite_original -Description=\\&quot;&quot;, plot_comment, &quot;\\&quot; figures/SF21.pdf&quot;) system(create_metadata) # export S.Tab. 4 global_bar %&gt;% dplyr::select(- scaled_pi) %&gt;% mutate(loc = loc_names[str_sub(spec, -3,-1)], spec = str_c(&quot;H. &quot;, sp_names[str_sub(spec, 1,3)]), genome_wide_pi = sprintf(&quot;%.5f&quot;,genome_wide_pi)) %&gt;% pivot_wider(names_from = spec, values_from = genome_wide_pi, values_fill = &quot;-&quot;) %&gt;% knitr::kable(format = &quot;latex&quot;) "]
]
