[
["index.html", "Script repository (Hench et al. supplement) 1 Intro 1.1 Analysis 1.2 Prerequesites 1.3 Figures 1.4 R setup", " Script repository (Hench et al. supplement) Kosmas Hench and Martin Helmkampf 2021-08-20 1 Intro Disclaimer: We are currently still in the process of updating the documentation after the last round of revisions. Some sections are thus still not up to date. This repository contains the complete workflow used in the paper “Ancestral variation, hybridization and modularity fuel a marine radiation.” The individual chapters of this documentation follow the separate main steps of the workflow. Each of the chapters thus refers to an individual prefix in the git x.x references of the papers method section. The individual steps partly depend on each other - especially git 1 - git 3 should be executed in order and before the other steps. 1.1 Analysis A documentation of the data preparation and the data analysis (git 1.x - 14.x) can be found at: git 1.x: Genotyping git 2.x: Genotyping all base pairs git 3.x: Analysis (FST &amp; GxP) git 4.x: Analysis (dXY &amp; \\(\\pi\\)) git 5.x: Analysis (topolgy weighting) git 6.x: Analysis (\\(\\rho\\)) git 7.x: Analysis (PCA) git 8.x: Analysis (demographic history) git 9.x: Analysis (hybridization) git 10.x: Analysis (admixture) git 11.x: Analysis (allele age) git 12.x: Analysis (FST permutation) git 13.x: Analysis (whg phylogeny) git 14.x: Analysis (outlier region phylogeny) 1.2 Prerequesites All scripts assume two variables to be set within the bash environment: $BASE_DIR is assumed to point to the base folder of this repository $SFTWR is a folder that contains all the software dependencies that are used within the scripts The analysis is controlled using the workflow manager nextflow and uses slightly different configurations across the individual pipelines. The exact commands used to execute the analysis during the development of the publication are stored within the aliases set within sh/nextflow_alias.sh. Furthermore, external dependencies need to be downloaded and deployed at the expected places (s. README.md at the ressources folder). 1.3 Figures The creation of the figures is bundled in a single script (git 15) which can be executed once all nextflow scripts have successfully run. cd $BASE_DIR bash sh/create_figures.sh This is basically just a wrapper script that will run all scripts located under $BASE_DIR/R/fig. Under this location, you will find one R script per figure (and suppl. figure). So if you are only interested in a single figure - that is the place to start looking. Furthermore, a more detailed documentation exists for all the figure scripts used for the manuscript: F1, F2, F3, F4 F5 and F6 as well as for all the supplementary figures: SF1, SF2, SF3, SF4, SF5, SF6, SF7, SF8, SF9, SF10, SF11, SF12, SF13, SF14, SF15 and SF16. 1.4 R setup There is an additional R package needed to run the plotting scripts for the figures ({GenomicOriginsScripts}). This depends on several non-CRAN R-packages, so to be able to install the package successfully, the following packages will also need to be installed: # installing non-CRAN dependencies install.packages(&quot;remotes&quot;) remotes::install_bioc(&quot;rtracklayer&quot;) remotes::install_github(&quot;YuLab-SMU/ggtree&quot;) remotes::install_github(&quot;k-hench/hypogen&quot;) remotes::install_github(&quot;k-hench/hypoimg&quot;) # installing GenomicOriginsScripts remotes::install_github(&quot;k-hench/GenomicOriginsScripts&quot;) Once these non-CRAN packages are installed, it should be possible to re-create the used R environment using the {renv} package. After opening the RStudio project (hamlet_radiation.Rproj), call: # restoring R environment install.packages(&quot;renv&quot;) renv::restore() Apart from the specific R packages that can be retrieved via {renv} from the renv.lock file, the used R setup at the at time of compilation is as follows: sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: Ubuntu 20.04.2 LTS ## ## Matrix products: default ## BLAS: /usr/local/lib/R/lib/libRblas.so ## LAPACK: /usr/local/lib/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=de_DE.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=de_DE.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=de_DE.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=de_DE.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_4.0.3 magrittr_2.0.1 bookdown_0.19 htmltools_0.5.1.1 ## [5] tools_4.0.3 yaml_2.2.1 stringi_1.5.3 rmarkdown_2.7.6 ## [9] knitr_1.31 stringr_1.4.0 digest_0.6.27 xfun_0.22 ## [13] rlang_0.4.10 evaluate_0.14 "],
["git-1-genotyping-i-snps-only.html", "2 (git 1) Genotyping I (SNPs only) 2.1 Summary 2.2 Details of genotyping.nf", " 2 (git 1) Genotyping I (SNPs only) This pipeline can be executed as follows: cd $BASE_DIR/nf/01_genotyping source ../sh/nextflow_alias.sh nf_run_gatk 2.1 Summary The genotyping procedure is controlled by the nextflow script genotyping.nf (located under $BASE_DIR/nf/01_genotyping/). It takes the analysis from the raw sequencing data to the genotyped and phased SNPs. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the raw data input, red arrows depict output that is exported for further use.) 2.2 Details of genotyping.nf 2.2.1 Data preparation The nextflow script starts with a small header and then opens the analysis by reading a table with meta data about the samples. The table is parsed and the values are stored in nextflow variables. #!/usr/bin/env nextflow /* =============================================================== Disclaimer: This pipeline needs a lot of time &amp; memory to run: All in all we used roughly 10 TB and ran for about 1 Month (mainly due to limited bandwidth on the cluster durint the &quot;receive_tuple step) =============================================================== */ // git 1.1 /* open the pipeline based on the metadata spread sheet that includes all information necessary to assign read groups to the sequencing data, split the spread sheet by row and feed it into a channel */ Channel .fromPath(&#39;../../metadata/file_info.txt&#39;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ id:row.id, label:row.label, file_fwd:row.file_fwd, file_rev:row.file_rev, flowcell_id_fwd:row.flowcell_id_fwd, lane_fwd:row.lane_fwd, company:row.company] } .set { samples_ch } Below is a little preview of the table containing the sample meta data: id label spec geo date coord_N coord_W company .. 16_21-30 16_21-30nigpan nig pan 2016 NA NA duke … 16_21-30 16_21-30nigpan nig pan 2016 NA NA duke … 16_31-40 16_31-40unipan uni pan 2016 NA NA duke … 16_31-40 16_31-40unipan uni pan 2016 NA NA duke … 17996 17996indbel ind bel 2004-07-27 16.801 -88.079 novogene … 17997 17997indbel ind bel 2004-07-27 16.801 -88.079 novogene … … … … … … … … … The first step to prepare the data for the GATK best practices, is to convert the sample sequences from *.fq to *.bam format to assign read groups: // git 1.2 /* for every sequencing file, convert into ubam format and assign read groups */ process split_samples { label &#39;L_20g2h_split_samples&#39; input: val x from samples_ch output: set val( &quot;${x.label}.${x.lane_fwd}&quot; ), file( &quot;${x.label}.${x.lane_fwd}.ubam.bam&quot; ) into ubams_mark, ubams_merge script: &quot;&quot;&quot; echo -e &quot;---------------------------------&quot; echo -e &quot;Label:\\t\\t${x.label}\\nFwd:\\t\\t${x.file_fwd}\\nRev:\\t\\t${x.file_rev}&quot; echo -e &quot;Flowcell:\\t${x.flowcell_id_fwd}\\nLane:\\t\\t${x.lane_fwd}&quot; echo -e &quot;Read group:\\t${x.flowcell_id_fwd}.${x.lane_fwd}\\nCompany:\\t${x.company}&quot; mkdir -p \\$BASE_DIR/temp_files gatk --java-options &quot;-Xmx20G&quot; \\ FastqToSam \\ -SM=${x.label} \\ -F1=\\$BASE_DIR/data/seqdata/${x.file_fwd} \\ -F2=\\$BASE_DIR/data/seqdata/${x.file_rev} \\ -O=${x.label}.${x.lane_fwd}.ubam.bam \\ -RG=${x.label}.${x.lane_fwd} \\ -LB=${x.label}&quot;.lib1&quot; \\ -PU=${x.flowcell_id_fwd}.${x.lane_fwd} \\ -PL=Illumina \\ -CN=${x.company} \\ --TMP_DIR=\\$BASE_DIR/temp_files; &quot;&quot;&quot; } The second step is marking the Illumina adapters. // git 1.3 /* for every ubam file, mark Illumina adapters */ process mark_adapters { label &#39;L_20g2h_mark_adapters&#39; tag &quot;${sample}&quot; input: set val( sample ), file( input ) from ubams_mark output: set val( sample ), file( &quot;*.adapter.bam&quot;) into adapter_bams file &quot;*.adapter.metrics.txt&quot; into adapter_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx18G&quot; \\ MarkIlluminaAdapters \\ -I=${input} \\ -O=${sample}.adapter.bam \\ -M=${sample}.adapter.metrics.txt \\ -TMP_DIR=\\$BASE_DIR/temp_files; &quot;&quot;&quot; } We need to pass on the unaligned .bam file and the file containing the adapter information together, so the output of the first two processes are matched by the combined sample and sequencing lane information. // git 1.4 adapter_bams .combine(ubams_merge, by:0) .set {merge_input} For the actual mapping, the sequences are transformed back into .fq format, aligned using bwa and merged back with their original read group information. // git 1.5 /* this step includes a 3 step pipeline: * - re-transformatikon into fq format * - mapping aginst the reference genome_file * - merging with the basuch ubams to include read group information */ process map_and_merge { label &#39;L_75g24h8t_map_and_merge&#39; tag &quot;${sample}&quot; input: set val( sample ), file( adapter_bam_input ), file( ubam_input ) from merge_input output: set val( sample ), file( &quot;*.mapped.bam&quot; ) into mapped_bams script: &quot;&quot;&quot; set -o pipefail gatk --java-options &quot;-Xmx68G&quot; \\ SamToFastq \\ -I=${adapter_bam_input} \\ -FASTQ=/dev/stdout \\ -INTERLEAVE=true \\ -NON_PF=true \\ -TMP_DIR=\\$BASE_DIR/temp_files | \\ bwa mem -M -t 8 -p \\$BASE_DIR/ressources/HP_genome_unmasked_01.fa /dev/stdin | gatk --java-options &quot;-Xmx68G&quot; \\ MergeBamAlignment \\ --VALIDATION_STRINGENCY SILENT \\ --EXPECTED_ORIENTATIONS FR \\ --ATTRIBUTES_TO_RETAIN X0 \\ -ALIGNED_BAM=/dev/stdin \\ -UNMAPPED_BAM=${ubam_input} \\ -OUTPUT=${sample}.mapped.bam \\ --REFERENCE_SEQUENCE=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa.gz \\ -PAIRED_RUN true \\ --SORT_ORDER &quot;unsorted&quot; \\ --IS_BISULFITE_SEQUENCE false \\ --ALIGNED_READS_ONLY false \\ --CLIP_ADAPTERS false \\ --MAX_RECORDS_IN_RAM 2000000 \\ --ADD_MATE_CIGAR true \\ --MAX_INSERTIONS_OR_DELETIONS -1 \\ --PRIMARY_ALIGNMENT_STRATEGY MostDistant \\ --UNMAPPED_READ_STRATEGY COPY_TO_TAG \\ --ALIGNER_PROPER_PAIR_FLAGS true \\ --UNMAP_CONTAMINANT_READS true \\ -TMP_DIR=\\$BASE_DIR/temp_files &quot;&quot;&quot; } Next, the duplicates are being marked. // git 1.6 /* for every mapped sample,sort and mark duplicates * (intermediate step is required to create .bai file) */ process mark_duplicates { label &#39;L_32g30h_mark_duplicates&#39; publishDir &quot;../../1_genotyping/0_sorted_bams/&quot;, mode: &#39;symlink&#39; tag &quot;${sample}&quot; input: set val( sample ), file( input ) from mapped_bams output: set val { sample - ~/\\.(\\d+)/ }, val( sample ), file( &quot;*.dedup.bam&quot;) into dedup_bams file &quot;*.dedup.metrics.txt&quot; into dedup_metrics script: &quot;&quot;&quot; set -o pipefail gatk --java-options &quot;-Xmx30G&quot; \\ SortSam \\ -I=${input} \\ -O=/dev/stdout \\ --SORT_ORDER=&quot;coordinate&quot; \\ --CREATE_INDEX=false \\ --CREATE_MD5_FILE=false \\ -TMP_DIR=\\$BASE_DIR/temp_files \\ | \\ gatk --java-options &quot;-Xmx30G&quot; \\ SetNmAndUqTags \\ --INPUT=/dev/stdin \\ --OUTPUT=intermediate.bam \\ --CREATE_INDEX=true \\ --CREATE_MD5_FILE=true \\ -TMP_DIR=\\$BASE_DIR/temp_files \\ --REFERENCE_SEQUENCE=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa.gz gatk --java-options &quot;-Xmx30G&quot; \\ MarkDuplicates \\ -I=intermediate.bam \\ -O=${sample}.dedup.bam \\ -M=${sample}.dedup.metrics.txt \\ -MAX_FILE_HANDLES=1000 \\ -TMP_DIR=\\$BASE_DIR/temp_files rm intermediate* &quot;&quot;&quot; } As a preparation for the actual genotyping, the .bam files are being indexed. // git 1.7 /* index al bam files */ process index_bam { label &#39;L_32g1h_index_bam&#39; tag &quot;${sample}&quot; input: set val( sample ), val( sample_lane ), file( input ) from dedup_bams output: set val( sample ), val( sample_lane ), file( input ), file( &quot;*.bai&quot;) into ( indexed_bams, pir_bams ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx30G&quot; \\ BuildBamIndex \\ -INPUT=${input} &quot;&quot;&quot; } At this point the preparation of the sequencing is done and we can start with the genotyping. (The output of the data preparation is split and one copy is later also used to prepare the read aware phasing in the process called extractPirs.) 2.2.2 Genotying Since some of our samples were split over several lanes, we now need to collect all .bam files for each sample. // git 1.8 /* collect all bam files for each sample */ indexed_bams .groupTuple() .set {tubbled} Now, we can create the genotype likelihoods for each individual sample. // git 1.9 /* create one *.g.vcf file per sample */ process receive_tuple { label &#39;L_36g47h_receive_tuple&#39; publishDir &quot;../../1_genotyping/1_gvcfs/&quot;, mode: &#39;symlink&#39; tag &quot;${sample}&quot; input: set sample, sample_lane, bam, bai from tubbled output: file( &quot;*.g.vcf.gz&quot;) into gvcfs file( &quot;*.vcf.gz.tbi&quot;) into tbis script: &quot;&quot;&quot; INPUT=\\$(echo ${bam} | sed &#39;s/\\\\[/-I /g; s/\\\\]//g; s/,/ -I/g&#39;) gatk --java-options &quot;-Xmx35g&quot; HaplotypeCaller \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ \\$INPUT \\ -O ${sample}.g.vcf.gz \\ -ERC GVCF &quot;&quot;&quot; } The individual genotype likelihoods are collected and combined for the entire data set. // git 1.10 /* collect and combine all *.g.vcf files */ process gather_gvcfs { label &#39;L_O88g90h_gather_gvcfs&#39; publishDir &quot;../../1_genotyping/1_gvcfs/&quot;, mode: &#39;symlink&#39; echo true input: file( gvcf ) from gvcfs.collect() file( tbi ) from tbis.collect() output: set file( &quot;cohort.g.vcf.gz&quot; ), file( &quot;cohort.g.vcf.gz.tbi&quot; ) into ( gcvf_snps, gvcf_acs, gvcf_indel ) script: &quot;&quot;&quot; GVCF=\\$(echo &quot; ${gvcf}&quot; | sed &#39;s/ /-V /g; s/vcf.gz/vcf.gz /g&#39;) gatk --java-options &quot;-Xmx85g&quot; \\ CombineGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ \\$GVCF \\ -O cohort.g.vcf.gz &quot;&quot;&quot; } All samples are jointly genotyped. // git 1.11 /* actual genotyping step (varinat sites only) */ process joint_genotype_snps { label &#39;L_O88g90h_joint_genotype&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;symlink&#39; input: set file( vcf ), file( tbi ) from gcvf_snps output: set file( &quot;raw_var_sites.vcf.gz&quot; ), file( &quot;raw_var_sites.vcf.gz.tbi&quot; ) into ( raw_var_sites, raw_var_sites_to_metrics ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=${vcf} \\ -O=intermediate.vcf.gz gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-include=SNP \\ -O=raw_var_sites.vcf.gz rm intermediate.* &quot;&quot;&quot; } The output of this process is split and used to collect the genotype metrics to inform the hard filtering of SNPs and to pass on the genotypes to the process called filterSNPs. At this point we create a channel containing all 24 hamlet linkage groups (LGs). This is used later (in the process called extractPirs) since all LGs are phased separately and only located at this part of the script for historical reasons (sorry :/). // git 1.12 /* generate a LG channel */ Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .into{ LG_ids1; LG_ids2 } The metrics of the raw genotypes are collected. // git 1.13 /* produce metrics table to determine filtering thresholds - ups forgot to extract SNPS first*/ process joint_genotype_metrics { label &#39;L_28g5h_genotype_metrics&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;move&#39; input: set file( vcf ), file( tbi ) from raw_var_sites_to_metrics output: file( &quot;${vcf}.table.txt&quot; ) into raw_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx25G&quot; \\ VariantsToTable \\ --variant=${vcf} \\ --output=${vcf}.table.txt \\ -F=CHROM -F=POS -F=MQ \\ -F=QD -F=FS -F=MQRankSum -F=ReadPosRankSum \\ --show-filtered &quot;&quot;&quot; } Based on the thresholds derived from the genotype metrics, the genotypes are first tagged and then filtered. After this, the data is filtered for missingness and only bi-allelic SNPs are selected. // git 1.14 /* filter snps basaed on locus annotations, missingness and type (bi-allelic only) */ process filterSNPs { label &#39;L_78g10h_filter_Snps&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;symlink&#39; input: set file( vcf ), file( tbi ) from raw_var_sites output: set file( &quot;filterd_bi-allelic.vcf.gz&quot; ), file( &quot;filterd_bi-allelic.vcf.gz.tbi&quot; ) into filtered_snps script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ -O=intermediate.filterd.vcf.gz \\ --exclude-filtered vcftools \\ --gzvcf intermediate.filterd.vcf.gz \\ --max-missing-count 17 \\ --max-alleles 2 \\ --stdout \\ --recode | \\ bgzip &gt; filterd_bi-allelic.vcf.gz tabix -p vcf filterd_bi-allelic.vcf.gz rm intermediate.* &quot;&quot;&quot; } At this point, the genotying is done. 2.2.3 Phasing To get from genotypes to haplotypes, we apply read-aware phasing using Shapeit. This takes the original sequencing reads into account, so in the first step the reads are screened for Phase Informative Reads (i.e. reads containing more than a single SNP). This needs to be done for each LG independently, so we first need to split the genotypes before running extractPIRs. // git 1.15 // extract phase informative reads from // alignments and SNPs process extractPirs { label &#39;L_78g10h_extract_pirs&#39; input: val( lg ) from LG_ids2 set val( sample ), val( sample_lane ), file( input ), file( index ) from pir_bams.collect() set file( vcf ), file( tbi ) from filtered_snps output: set val( lg ), file( &quot;filterd_bi-allelic.LG${lg}.vcf.gz&quot; ), file( &quot;filterd_bi-allelic.LG${lg}.vcf.gz.tbi&quot; ), file( &quot;PIRsList-LG${lg}.txt&quot; ) into pirs_lg script: &quot;&quot;&quot; LG=&quot;LG${lg}&quot; awk -v OFS=&#39;\\t&#39; -v dir=\\$PWD -v lg=\\$LG &#39;{print \\$1,dir&quot;/&quot;\\$2,lg}&#39; \\$BASE_DIR/metadata/bamlist_proto.txt &gt; bamlist.txt vcftools \\ --gzvcf ${vcf} \\ --chr \\$LG \\ --stdout \\ --recode | \\ bgzip &gt; filterd_bi-allelic.LG${lg}.vcf.gz tabix -p vcf filterd_bi-allelic.LG${lg}.vcf.gz extractPIRs \\ --bam bamlist.txt \\ --vcf filterd_bi-allelic.LG${lg}.vcf.gz \\ --out PIRsList-LG${lg}.txt \\ --base-quality 20 \\ --read-quality 15 &quot;&quot;&quot; } Using those PIRs, we can then proceed with the actual phasing. The resulting haplotypes are converted back into .vcf format. // git 1.16 // run the actual phasing process run_shapeit { label &#39;L_75g24h8t_run_shapeit&#39; input: set val( lg ), file( vcf ), file( tbi ), file( pirs ) from pirs_lg output: file( &quot;phased-LG${lg}.vcf.gz&quot; ) into phased_lgs script: &quot;&quot;&quot; LG=&quot;LG${lg}&quot; shapeit \\ -assemble \\ --input-vcf ${vcf} \\ --input-pir ${pirs} \\ --thread 8 \\ -O phased-LG${lg} shapeit \\ -convert \\ --input-hap phased-LG${lg} \\ --output-vcf phased-LG${lg}.vcf bgzip phased-LG${lg}.vcf &quot;&quot;&quot; } After the phasing, we merge the LGs back together to get a single data set. We export a comple data set as well as one that was filtered for a minor allele count of at least two. // git 1.17 // merge the phased LGs back together. // the resulting vcf file represents // the &#39;SNPs only&#39; data set process merge_phased { label &#39;L_28g5h_merge_phased_vcf&#39; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &#39;move&#39; input: file( vcf ) from phased_lgs.collect() output: set file( &quot;phased.vcf.gz&quot; ), file( &quot;phased.vcf.gz.tbi&quot; ) into phased_vcf set file( &quot;phased_mac2.vcf.gz&quot; ), file( &quot;phased_mac2.vcf.gz.tbi&quot; ) into phased_mac2_vcf script: &quot;&quot;&quot; vcf-concat \\ phased-LG* | \\ grep -v ^\\$ | \\ tee phased.vcf | \\ vcftools --vcf - --mac 2 --recode --stdout | \\ bgzip &gt; phased_mac2.vcf.gz bgzip phased.vcf tabix -p vcf phased.vcf.gz tabix -p vcf phased_mac2.vcf.gz &quot;&quot;&quot; } Finally, we are done with the entire genotyping procedure for the SNPs olny data set. 2.2.4 Indel masks The genotyping.nf workflow contains an appendix that makes use of the genotyping likelihoods created in step git 1.10 to create an indel mask that is later used in the inference of the hamlet demographic history (git 8.x). (This part is excluded from the initial visualization of this script) We restart by reopening the joint-sample genotype likelyhoods file and calling the indels from it. /* ========================================= */ /* appendix: generate indel masks for msmc: */ // git 1.18 // reopen the gvcf file to also genotype indels process joint_genotype_indel { label &#39;L_O88g90h_genotype_indel&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from gvcf_indel output: set file( &quot;raw_var_indel.vcf.gz&quot; ), file( &quot;raw_var_indel.vcf.gz.tbi&quot; ) into ( raw_indel, raw_indel_to_metrics ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$REF_GENOME \\ -V=${vcf} \\ -O=intermediate.vcf.gz gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$REF_GENOME \\ -V=intermediate.vcf.gz \\ --select-type-to-include=INDEL \\ -O=raw_var_indel.vcf.gz rm intermediate.* &quot;&quot;&quot; } We export the the indel genotype metrics to determine cutoff values for the hard filtering step. // git 1.19 // export indel metrics for filtering process indel_metrics { label &#39;L_28g5h_genotype_metrics&#39; publishDir &quot;../../1_genotyping/2_raw_vcfs/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from raw_indel_to_metrics output: file( &quot;${vcf}.table.txt&quot; ) into raw_indel_metrics script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx25G&quot; \\ VariantsToTable \\ --variant=${vcf} \\ --output=${vcf}.table.txt \\ -F=CHROM -F=POS -F=MQ \\ -F=QD -F=FS -F=MQRankSum -F=ReadPosRankSum \\ --show-filtered &quot;&quot;&quot; } Based on the exported metrics the genotypes are being filtered. // git 1.20 // hard filter indels and create mask process filterIndels { label &#39;L_78g10h_filter_indels&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from raw_indel output: set file( &quot;filterd.indel.vcf.gz&quot; ), file( &quot;filterd.indel.vcf.gz.tbi&quot; ) into filtered_indel file( &quot;indel_mask.bed.gz&quot; ) into indel_mask_ch /* FILTER THRESHOLDS NEED TO BE UPDATED */ script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$REF_GENOME \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;SOR &gt; 3.0&quot; \\ --filter-name &quot;filter_SOR&quot; \\ --filter-expression &quot;InbreedingCoeff &lt; -0.25&quot; \\ --filter-name &quot;filter_InbreedingCoeff&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$REF_GENOME \\ -V=intermediate.vcf.gz \\ -O=filterd.indel.vcf.gz \\ --exclude-filtered zcat filterd.indel.vcf.gz | \\ awk &#39;! /\\\\#/&#39; | \\ awk &#39;{if(length(\\$4) &gt; length(\\$5)) print \\$1&quot;\\\\t&quot;(\\$2-6)&quot;\\\\t&quot;(\\$2+length(\\$4)+4); else print \\$1&quot;\\\\t&quot;(\\$2-6)&quot;\\\\t&quot;(\\$2+length(\\$5)+4)}&#39; | \\ gzip -c &gt; indel_mask.bed.gz rm intermediate.* &quot;&quot;&quot; } Since we need one indel mask per linkage group, we create a channel of LGs. // git 1.21 /* create channel of linkage groups */ Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .into{ lg_ch } The linkage group channel is combined with the filtered indels. // git 1.22 // attach linkage groups to indel masks lg_ch.combine( filtered_indel ).set{ filtered_indel_lg } Finally, one mask per linkage group is created from the indel positions. // git 1.23 // split indel mask by linkage group process split_indel_mask { label &#39;L_loc_split_indel_mask&#39; publishDir &quot;../../ressources/indel_masks/&quot;, mode: &#39;copy&#39; input: set val( lg ), file( bed ) from filtered_indel_lg output: set val( lg ), file( &quot;indel_mask.${lg}.bed.gz &quot; ) into lg_indel_mask script: &quot;&quot;&quot; gzip -cd ${bed} | \\ grep ${lg} | \\ gzip -c &gt; indel_mask.${lg}.bed.gz &quot;&quot;&quot; } All indel masks are exported to the resources folder within the root directory. "],
["git-2-genotyping-ii-all-callable-sites.html", "3 (git 2) Genotyping II (all callable sites) 3.1 Summary 3.2 Details of genotyping_all_basepairs.nf", " 3 (git 2) Genotyping II (all callable sites) This pipeline can be executed as follows: cd $BASE_DIR/nf/02_genotyping_all_basepairs source ../sh/nextflow_alias.sh nf_run_allbp 3.1 Summary The genotyping procedure is controlled by the nextflow script genotyping_all_basepairs.nf (located under $BASE_DIR/nf/02_genotyping_all_basepairs/). Based on an intermediate step from genotyping.nf (git 1.10), this script produces a data set that includes all callable sites - that is SNPs as well a invariant sites that are covered by sequence. Below is an overview of the steps involved in the genotyping process. (The green dot indicates the data input, red arrows depict output that is exported for further use.) 3.2 Details of genotyping_all_basepairs.nf 3.2.1 Data preparation The nextflow script starts with a small header and then imports the joint genotyping likelihoods for all samples produced by genotyping_all_basepairs.nf. #!/usr/bin/env nextflow // git 2.1 // open genotype likelyhoods Channel .fromFilePairs(&quot;../../1_genotyping/1_gvcfs/cohort.g.vcf.{gz,gz.tbi}&quot;) .set{ vcf_cohort } The genotyping of the different linkage groups is going to happen in parallel, so we need to initialize a channel for the 24 LGs. // git 2.2 // initialize LG channel Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .set{ ch_LG_ids } The genotyping likelihoods are combined, effectively linking the data set to the 24 parallel LGs. // git 2.3 // combine genotypes and LGs ch_LG_ids.combine( vcf_cohort ).set{ vcf_lg_combo } The samples are jointly genotyped, independently for each LG and including invariant sites. // git 2.4 // actual genotyping step (including invariant sites) process joint_genotype_snps { label &quot;L_O88g90h_LGs_genotype&quot; input: set val( lg ), vcfId, file( vcf ) from vcf_lg_combo output: set val( &#39;all&#39; ), val( lg ), file( &quot;all_site*.vcf.gz&quot; ), file( &quot;all_site*.vcf.gz.tbi&quot; ) into all_bp_by_location script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx85g&quot; \\ GenotypeGVCFs \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -L=LG${lg} \\ -V=${vcf[0]} \\ -O=intermediate.vcf.gz \\ --include-non-variant-sites=true gatk --java-options &quot;-Xmx85G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ --select-type-to-exclude=INDEL \\ -O=all_sites.LG${lg}.vcf.gz rm intermediate.* &quot;&quot;&quot; } The genotypes of the different LGs are merged. // git 2.5 // merge all LGs process merge_genotypes { label &#39;L_78g5h_merge_genotypes&#39; echo true input: set val( dummy ), val( lg ), file( vcf ), file( tbi ) from all_bp_by_location.groupTuple() output: file( &quot;all_sites.vcf.gz&quot; ) into all_bp_merged script: &quot;&quot;&quot; INPUT=\\$(ls -1 *vcf.gz | sed &#39;s/^/ -I /g&#39; | cat \\$( echo )) gatk --java-options &quot;-Xmx85g&quot; \\ GatherVcfs \\ \\$INPUT \\ -O=all_sites.vcf.gz &quot;&quot;&quot; } The genotypes are hard filtered based on various genotyping scores. // git 2.6 // quality based filtering process filterSNP_first { label &#39;L_105g30h_filter_gt1&#39; input: file( vcf ) from all_bp_merged output: set file( &quot;intermediate.filterd.vcf.gz&quot; ), file( &quot;intermediate.filterd.vcf.gz.tbi&quot; ) into filtered_snps_first script: &quot;&quot;&quot; module load openssl1.0.2 tabix -p vcf ${vcf} gatk --java-options &quot;-Xmx75G&quot; \\ VariantFiltration \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V ${vcf} \\ -O=intermediate.vcf.gz \\ --filter-expression &quot;QD &lt; 2.5&quot; \\ --filter-name &quot;filter_QD&quot; \\ --filter-expression &quot;FS &gt; 25.0&quot; \\ --filter-name &quot;filter_FS&quot; \\ --filter-expression &quot;MQ &lt; 52.0 || MQ &gt; 65.0&quot; \\ --filter-name &quot;filter_MQ&quot; \\ --filter-expression &quot;MQRankSum &lt; -0.2 || MQRankSum &gt; 0.2&quot; \\ --filter-name &quot;filter_MQRankSum&quot; \\ --filter-expression &quot;ReadPosRankSum &lt; -2.0 || ReadPosRankSum &gt; 2.0 &quot; \\ --filter-name &quot;filter_ReadPosRankSum&quot; \\ --QUIET true &amp;&gt; var_filt.log gatk --java-options &quot;-Xmx75G&quot; \\ SelectVariants \\ -R=\\$BASE_DIR/ressources/HP_genome_unmasked_01.fa \\ -V=intermediate.vcf.gz \\ -O=intermediate.filterd.vcf.gz \\ --exclude-filtered \\ --QUIET true \\ --verbosity ERROR &amp;&gt; var_select.log &quot;&quot;&quot; } A second filtering is based on the missingness of samples. // git 2.7 // missingness based filtering // the resulting vcf file represents // the &#39;all BP&#39; data set process filterSNP_second { label &#39;L_105g30h_filter_gt2&#39; publishDir &quot;../../1_genotyping/3_gatk_filtered/&quot;, mode: &#39;copy&#39; input: set file( vcf ), file( tbi ) from filtered_snps_first output: file( &quot;filterd.allBP.vcf.gz&quot; ) into filtered_snps script: &quot;&quot;&quot; module load openssl1.0.2 vcftools \\ --gzvcf ${vcf} \\ --max-missing-count 17 \\ --stdout \\ --recode | \\ bgzip &gt; filterd.allBP.vcf.gz &quot;&quot;&quot; } Finally, we are done with the second version of genotyping. "],
["git-3-analysis-i-fst-gxp.html", "4 (git 3) Analysis I (FST &amp; GxP) 4.1 Summary 4.2 Details of analysis_fst_gxp.nf", " 4 (git 3) Analysis I (FST &amp; GxP) This pipeline can be executed as follows: cd $BASE_DIR/nf/03_analysis_fst_gxp source ../sh/nextflow_alias.sh nf_run_basic 4.1 Summary The genetic differentiation, as well as the genotype x phenotype association, are computed within the nextflow script analysis_fst_gxp.nf (located under $BASE_DIR/nf/03_analysis_fst_gxp/). It takes the SNPs only data set and computes \\(F_{ST}\\) and the GxP association. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 4.2 Details of analysis_fst_gxp.nf 4.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into three different streams. #!/usr/bin/env nextflow // git 3.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_locations; vcf_filter; vcf_gxp; vcf_adapt } Since we are going to work on the three sampling locations independently, we create channel for the locations. // git 3.2 // initialize location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } Then we attach the genotypes to the locations. // git 3.3 // attach genotypes to location locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } Next, we define the species sets sampled at the individual locations. // git 3.4 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } For each location, the data is subset to include only local hamlets. // git 3.5 // subset data to local hamlets process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.pop&quot; ) into ( vcf_loc_pair1, vcf_loc_pair2, vcf_loc_pair3 ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.vcf.gz &quot;&quot;&quot; } As we also want to compute global statistics, we create another subset including all sampled hamlets but excluding the outgroups. // git 3.6 // subset the global data set to hamlets only process subset_vcf_hamlets_only { label &quot;L_20g15h_filter_hamlets_only&quot; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.vcf.gz&quot; //module &quot;R3.5.2&quot; input: set vcfId, file( vcf ) from vcf_filter output: file( &quot;hamlets_only.vcf.gz*&quot; ) into vcf_hamlets_only set file( &quot;hamlets_only.vcf.gz*&quot; ), file( &quot;hamlets_only.pop.txt&quot; ) into vcf_multi_fst script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni&quot; &gt; outgroup.pop vcfsamplenames ${vcf[0]} | \\ grep &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; hamlets_only.pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --remove outgroup.pop \\ --recode \\ --stdout | gzip &gt; hamlets_only.vcf.gz &quot;&quot;&quot; } 4.2.2 FST Using this subsets, we compute the global differentiation among all hamlet populations. // ----------- Fst section ----------- // git 3.7 // compute global fst process fst_multi { label &#39;L_20g15h_fst_multi&#39; publishDir &quot;../../2_analysis/fst/50k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.50k.tsv.gz&quot; publishDir &quot;../../2_analysis/fst/10k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.10k.tsv.gz&quot; publishDir &quot;../../2_analysis/fst/logs/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.log&quot; publishDir &quot;../../2_analysis/summaries&quot;, mode: &#39;copy&#39; , pattern: &quot;fst_outliers_998.tsv&quot; //conda &quot;$HOME/miniconda2/envs/py3&quot; //module &quot;R3.5.2&quot; input: set file( vcf ), file( pop ) from vcf_multi_fst output: file( &quot;multi_fst*&quot; ) into multi_fst_output file( &quot;fst_outliers_998.tsv&quot; ) into fst_outlier_output script: &quot;&quot;&quot; awk &#39;{print \\$1&quot;\\\\t&quot;\\$2\\$3}&#39; ${pop} &gt; pop.txt for k in abehon gumhon indbel maybel nigbel nighon nigpan puebel puehon puepan ranhon unibel unihon unipan; do grep \\$k pop.txt | cut -f 1 &gt; pop.\\$k.txt done POP=&quot;--weir-fst-pop pop.abehon.txt \\ --weir-fst-pop pop.gumhon.txt \\ --weir-fst-pop pop.indbel.txt \\ --weir-fst-pop pop.maybel.txt \\ --weir-fst-pop pop.nigbel.txt \\ --weir-fst-pop pop.nighon.txt \\ --weir-fst-pop pop.nigpan.txt \\ --weir-fst-pop pop.puebel.txt \\ --weir-fst-pop pop.puehon.txt \\ --weir-fst-pop pop.puepan.txt \\ --weir-fst-pop pop.ranhon.txt \\ --weir-fst-pop pop.unibel.txt \\ --weir-fst-pop pop.unihon.txt \\ --weir-fst-pop pop.unipan.txt&quot; # fst by SNP # ---------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --stdout 2&gt; multi_fst_snp.log | \\ gzip &gt; multi_fst.tsv.gz # fst 50kb window # --------------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --fst-window-step 5000 \\ --fst-window-size 50000 \\ --stdout 2&gt; multi_fst.50k.log | \\ gzip &gt; multi_fst.50k.tsv.gz # fst 10kb window # --------------- vcftools --gzvcf ${vcf} \\ \\$POP \\ --fst-window-step 1000 \\ --fst-window-size 10000 \\ --stdout 2&gt; multi_fst.10k.log | \\ gzip &gt; multi_fst_snp.tsv.gz Rscript --vanilla \\$BASE_DIR/R/table_fst_outliers.R multi_fst.50k.tsv.gz &quot;&quot;&quot; } Then, we set up the pair wise species comparisons… // git 3.8 // prepare pairwise fsts // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ // ------------------------------ /* channel content after joinig: set [0:val(loc), 1:file(vcf), 2:file(pop), 3:val(spec1), 4:val(spec2)]*/ // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .join( vcf_loc_pair1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .join( vcf_loc_pair2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .join( vcf_loc_pair3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } … and run them. // git 3.9 // compute pairwise fsts process fst_run { label &#39;L_32g4h_fst_run&#39; publishDir &quot;../../2_analysis/fst/50k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.50k.windowed.weir.fst.gz&quot; publishDir &quot;../../2_analysis/fst/10k/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.10k.windowed.weir.fst.gz&quot; publishDir &quot;../../2_analysis/fst/logs/&quot;, mode: &#39;copy&#39; , pattern: &quot;${loc}-${spec1}-${spec2}.log&quot; input: set val( loc ), file( vcf ), file( pop ), val( spec1 ), val( spec2 ) from all_fst_pairs_ch output: set val( loc ), file( &quot;*.50k.windowed.weir.fst.gz&quot; ), file( &quot;${loc}-${spec1}-${spec2}.log&quot; ) into fst_50k file( &quot;*.10k.windowed.weir.fst.gz&quot; ) into fst_10k_output file( &quot;${loc}-${spec1}-${spec2}.log&quot; ) into fst_logs script: &quot;&quot;&quot; grep ${spec1} ${pop} &gt; pop1.txt grep ${spec2} ${pop} &gt; pop2.txt vcftools --gzvcf ${vcf} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-step 5000 \\ --fst-window-size 50000 \\ --out ${loc}-${spec1}-${spec2}.50k 2&gt; ${loc}-${spec1}-${spec2}.log vcftools --gzvcf ${vcf} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-size 10000 \\ --fst-window-step 1000 \\ --out ${loc}-${spec1}-${spec2}.10k gzip *.windowed.weir.fst &quot;&quot;&quot; } The genome wide summaries are compiled from the log files of VCFtools. // git 3.10 /* collect the VCFtools logs to crate a table with the genome wide fst values */ process fst_globals { label &#39;L_loc_fst_globals&#39; publishDir &quot;../../2_analysis/summaries&quot;, mode: &#39;copy&#39; , pattern: &quot;fst_globals.txt&quot; //module &quot;R3.5.2&quot; input: file( log ) from fst_logs.collect() output: file( &quot;fst_globals.txt&quot; ) into fst_glob script: &quot;&quot;&quot; cat *.log | \\ grep -E &#39;Weir and Cockerham|--out&#39; | \\ grep -A 3 50k | \\ sed &#39;/^--/d; s/^.*--out //g; s/.50k//g; /^Output/d; s/Weir and Cockerham //g; s/ Fst estimate: /\\t/g&#39; | \\ paste - - - | \\ cut -f 1,3,5 | \\ sed &#39;s/^\\\\(...\\\\)-/\\\\1\\\\t/g&#39; &gt; fst_globals.txt &quot;&quot;&quot; } 4.2.3 GxP The software used for the GxP association (gemma) uses genotypes in plink file format, so the first step was to convert the input. // ----------- G x P section ----------- // git 3.11 // reformat genotypes (1) process plink12 { label &#39;L_20g2h_plink12&#39; input: set vcfId, file( vcf ) from vcf_gxp output: set file( &quot;GxP_plink.map&quot; ), file( &quot;GxP_plink.ped&quot; ) into plink_GxP script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --plink \\ --out GxP_plink plink \\ --file GxP_plink \\ --recode12 \\ --out hapmap &quot;&quot;&quot; } // git 3.12 // reformat genotypes (2) process GxP_run { label &#39;L_20g2h_GxP_binary&#39; input: set file( map ), file( ped ) from plink_GxP output: set file( &quot;*.bed&quot; ), file( &quot;*.bim&quot; ),file( &quot;*.fam&quot; ) into plink_binary script: &quot;&quot;&quot; # convert genotypes into binary format (bed/bim/fam) plink \\ --noweb \\ --file GxP_plink \\ --make-bed \\ --out GxP_plink_binary &quot;&quot;&quot; } Additionally to the genotypes, the phenotypes of the samples are needed. // git 3.13 // import phenotypes Channel .fromPath(&quot;../../metadata/phenotypes.sc&quot;) .set{ phenotypes_raw } As a legacy of the early analysis, we perform a PCA on the phenotypes and report the extended phenotype data including the scoring on PC1 &amp; PC2. // git 3.14 // run PCA on phenotypes process phenotye_pca { label &quot;L_loc_phenotype_pca&quot; publishDir &quot;../../2_analysis/phenotype&quot;, mode: &#39;copy&#39; , pattern: &quot;*.txt&quot; //module &quot;R3.5.2&quot; input: file( sc ) from phenotypes_raw output: file( &quot;phenotypes.txt&quot; ) into phenotype_file file( &quot;phenotype_pca*.pdf&quot; ) into phenotype_pca script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/phenotypes_pca.R ${sc} &quot;&quot;&quot; } We set up all the traits for which we want to perform a GxP association. // git 3.15 // setup GxP traits Channel .from(&quot;Bars&quot;, &quot;Snout&quot;, &quot;Peduncle&quot;) .set{ traits_ch } Then, we combine the reformatted genotypes with the phenotyes and the traits of interest. // git 3.16 // bundle GxP input traits_ch.combine( plink_binary ).combine( phenotype_file ).set{ trait_plink_combo } Having collected all the input, we can now run the GxP. // git 3.17 // actually run the GxP process gemma_run { label &#39;L_32g4h_GxP_run&#39; publishDir &quot;../../2_analysis/GxP/bySNP/&quot;, mode: &#39;copy&#39; //module &quot;R3.5.2&quot; input: set val( pheno ), file( bed ), file( bim ), file( fam ), file( pheno_file ) from trait_plink_combo output: file(&quot;*.GxP.txt.gz&quot;) into gemma_results script: &quot;&quot;&quot; source \\$BASE_DIR/sh/body.sh BASE_NAME=\\$(echo ${fam} | sed &#39;s/.fam//g&#39;) mv ${fam} \\$BASE_NAME-old.fam cp \\${BASE_NAME}-old.fam ${fam} # 1) replace the phenotype values Rscript --vanilla \\$BASE_DIR/R/assign_phenotypes.R ${fam} ${pheno_file} ${pheno} # 2) create relatedness matrix of samples using gemma gemma -bfile \\$BASE_NAME -gk 1 -o ${pheno} # 3) fit linear model using gemma (-lm) gemma -bfile \\$BASE_NAME -lm 4 -miss 0.1 -notsnp -o ${pheno}.lm # 4) fit linear mixed model using gemma (-lmm) gemma -bfile \\$BASE_NAME -k output/${pheno}.cXX.txt -lmm 4 -o ${pheno}.lmm # 5) reformat output sed &#39;s/\\\\trs\\\\t/\\\\tCHROM\\\\tPOS\\\\t/g; s/\\\\([0-2][0-9]\\\\):/\\\\1\\\\t/g&#39; output/${pheno}.lm.assoc.txt | \\ cut -f 2,3,9-14 | body sort -k1,1 -k2,2n | gzip &gt; ${pheno}.lm.GxP.txt.gz sed &#39;s/\\\\trs\\\\t/\\\\tCHROM\\\\tPOS\\\\t/g; s/\\\\([0-2][0-9]\\\\):/\\\\1\\\\t/g&#39; output/${pheno}.lmm.assoc.txt | \\ cut -f 2,3,8-10,13-15 | body sort -k1,1 -k2,2n | gzip &gt; ${pheno}.lmm.GxP.txt.gz &quot;&quot;&quot; } To smooth the GxP results, we initialize two resolutions (50 kb windows with 5 kb increments and 10 kb windows with 1 kb increments). // git 3.18 // setup smoothing levels Channel .from([[50000, 5000], [10000, 1000]]) .set{ gxp_smoothing_levels } The we apply all smoothing levels to the raw GxP output… // git 3.19 // apply all smoothing levels gemma_results.combine( gxp_smoothing_levels ).set{ gxp_smoothing_input } .. and run the smoothing. // git 3.20 // actually run the smoothing process gemma_smooth { label &#39;L_20g2h_GxP_smooth&#39; publishDir &quot;../../2_analysis/GxP/${win}&quot;, mode: &#39;copy&#39; input: set file( lm ), file( lmm ), val( win ), val( step ) from gxp_smoothing_input output: set val( win ), file( &quot;*.lm.*k.txt.gz&quot; ) into gxp_lm_smoothing_output set val( win ), file( &quot;*.lmm.*k.txt.gz&quot; ) into gxp_lmm_smoothing_output script: &quot;&quot;&quot; \\$BASE_DIR/sh/gxp_slider ${lm} ${win} ${step} \\$BASE_DIR/sh/gxp_slider ${lmm} ${win} ${step} &quot;&quot;&quot; } At this step we are done with differentiation and GxP. 4.2.4 FST within species (adaptation scenario) To judge the independence of the individual populations of the hamlets species sampled at several locations, we also compute the \\(F_{ST}\\) among those populations. We start by defining the species-set for which we sampled multiple populations. // Fst within species --------------------------------------------------------- // git 3.21 // define species set Channel .from( &quot;nig&quot;, &quot;pue&quot;, &quot;uni&quot;) .set{ species_ch } Then, we define the sampling locations. // git 3.22 // define location set Channel.from( [[1, &quot;bel&quot;], [2, &quot;hon&quot;], [3, &quot;pan&quot;]]).into{ locations_ch_1;locations_ch_2 } Next, we create all population-pairs and bind it to the genotype file. // git 3.23 // create location pairs locations_ch_1 .combine(locations_ch_2) .filter{ it[0] &lt; it[2] } .map{ it[1,3]} .combine( species_ch ) .combine( vcf_adapt ) .set{ vcf_location_combo_adapt } Last, we compute the pair wise \\(F_{ST}\\) among all populations. // git 3.24 // compute pairwise fsts process fst_run_adapt { label &#39;L_20g4h_fst_run_adapt&#39; publishDir &quot;../../2_analysis/fst/adapt/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.log&quot; input: set val( loc1 ), val( loc2 ), val( spec ), val(vcf_indx), file( vcf ) from vcf_location_combo_adapt output: file( &quot;adapt_${spec}${loc1}-${spec}${loc2}.log&quot; ) into fst_adapt_logs script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | grep ${spec}${loc1} &gt; pop1.txt vcfsamplenames ${vcf[0]} | grep ${spec}${loc2} &gt; pop2.txt vcftools --gzvcf ${vcf[0]} \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --out adapt_${spec}${loc1}-${spec}${loc2} 2&gt; adapt_${spec}${loc1}-${spec}${loc2}.log &quot;&quot;&quot; } "],
["git-4-analysis-ii-dxy-pi.html", "5 (git 4) Analysis II (dXY &amp; pi) 5.1 Summary 5.2 Details of analysis_dxy.nf", " 5 (git 4) Analysis II (dXY &amp; pi) This pipeline can be executed as follows: cd $BASE_DIR/nf/04_analysis_dxy source ../sh/nextflow_alias.sh nf_run_dxy 5.1 Summary The genetic divergence, as well diversity, are computed within the nextflow script analysis_dxy.nf (located under $BASE_DIR/nf/04_analysis_dxy/). It takes the all BP data set and computes \\(d_{XY}\\) and \\(\\pi\\). Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 5.2 Details of analysis_dxy.nf 5.2.1 Data preparation The nextflow script starts by opening the genotype data and feeding it into two different streams (one for dXY and one for \\(\\pi\\)). #!/usr/bin/env nextflow // This pipeline includes the analysis run on the // all callable sites data sheet (dxy). // git 4.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/filterd.allBP.vcf.{gz,gz.tbi}&quot;) .into{ vcf_ch; vcf_pi_ch } The computation of dXY is split by linkage group, so we need to initialize a channel for the LGs. // git 4.2 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .set{ lg_ch } Now, we can subset the data set and convert it to a custom genotype format. // git 4.3 // split by LG and reformat the genotypes process split_allBP { label &#39;L_32g15h_split_allBP&#39; tag &quot;LG${lg}&quot; input: set val( lg ), vcfId, file( vcf ) from lg_ch.combine( vcf_ch ) output: set val( lg ), file( &#39;filterd.allBP.vcf.gz&#39; ), file( &quot;allBP.LG${lg}.geno.gz&quot; ) into geno_ch script: &quot;&quot;&quot; module load openssl1.0.2 vcftools --gzvcf ${vcf[0]} \\ --chr LG${lg} \\ --recode \\ --stdout | bgzip &gt; allBP.LG${lg}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i allBP.LG${lg}.vcf.gz | gzip &gt; allBP.LG${lg}.geno.gz &quot;&quot;&quot; } Since the species composition differs between locations, we need to initialize three separate sets of species. These are going to be used to create the divergence species pairs. // git 4.4 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } For the diversity, we also initialize the full set of populations of the study. // git 4.5 // init all sampled populations (for pi) Channel .from(&#39;indbel&#39;, &#39;maybel&#39;, &#39;nigbel&#39;, &#39;puebel&#39;, &#39;unibel&#39;, &#39;abehon&#39;, &#39;gumhon&#39;, &#39;nighon&#39;, &#39;puehon&#39;, &#39;ranhon&#39;, &#39;unihon&#39;, &#39;nigpan&#39;, &#39;puepan&#39;, &#39;unipan&#39;) .set{spec_dxy} We want to run a sliding window at different resolutions, so we set up a channel for these. // git 4.6 // init slining window resolutions Channel .from( 1, 5 ) .into{ kb_ch; kb_ch2; kb_ch3 } 5.2.2 dXY To prepare all species comparisons used to estimate divergence, we combine each species witch all other species within a location. // git 4.7 // prepare pair wise dxy // ------------------------------ // create all possible species pairs depending on location // and combine with genotype subset (for the respective location) // ------------------------------ // channel content after joining: // set [0:val(loc), 1:file(vcf), 2:file(pop), 3:val(spec1), 4:val(spec2)] // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .combine( bel_spec1_ch ) .combine( bel_spec2_ch ) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .combine( hon_spec1_ch ) .combine(hon_spec2_ch) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .combine( pan_spec1_ch ) .combine(pan_spec2_ch) .filter{ it[1] &lt; it[3] } .map{ it[0,2,4]} Now we attach the genotypes and the resolution level to the species pairs. // git 4.8 // combine species pair with genotypes (and window size) bel_pairs_ch .concat( hon_pairs_ch, pan_pairs_ch ) .combine( geno_ch ) .combine( kb_ch ) .into { all_dxy_pairs_ch; random_dxy_pairs_ch } At this point, we can calculate dXY. // git 4.9 // compute the dxy values process dxy_lg { label &#39;L_G32g15h_dxy_lg&#39; tag &quot;${spec1}${loc}-${spec2}${loc}_LG${lg}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), val( lg ), file( vcf ), file( geno ), val( kb ) from all_dxy_pairs_ch output: set val( &quot;${spec1}${loc}-${spec2}${loc}-${kb}&quot; ), file( &quot;dxy.${spec1}${loc}-${spec2}${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz&quot; ), val( lg ), val( &quot;${spec1}${loc}&quot; ), val( &quot;${spec2}${loc}&quot; ), val( kb ) into dxy_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 module load intel17.0.4 intelmpi17.0.4 zcat ${geno} | \\ head -n 1 | \\ cut -f 3- | \\ sed &#39;s/\\\\t/\\\\n/g&#39; | \\ awk -v OFS=&#39;\\\\t&#39; &#39;{print \\$1, substr( \\$1, length(\\$1) - 5, 6)}&#39; &gt; pop.txt mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 -s ${kb}000 \\ --popsFile pop.txt \\ -p ${spec1}${loc} -p ${spec2}${loc} \\ -g ${geno} \\ -o dxy.${spec1}${loc}-${spec2}${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } Since the calculation was split across LGs, we now need to collect all LGs of a particular species pair… // git 4.10 // collect all LGs for each species pair dxy_lg_ch .groupTuple() .set{ tubbled_dxy } … and merge the results. // git 4.11 // concatenate all LGs for each species pair process receive_tuple { label &#39;L_20g2h_receive_tuple&#39; publishDir &quot;../../2_analysis/dxy/${kb[0]}0k/&quot;, mode: &#39;copy&#39; tag &quot;${pop1[0]}-${pop2[0]}&quot; input: set val( comp ), file( dxy ), val( lg ), val( pop1 ), val( pop2 ), val( kb ) from tubbled_dxy output: file( &quot;dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv.gz&quot; ) into dxy_output_ch script: &quot;&quot;&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG01.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ head -n 1 &gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; for j in {01..24};do echo &quot;-&gt; LG\\$j&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG\\$j.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ awk &#39;NR&gt;1{print}&#39; &gt;&gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; done gzip dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv &quot;&quot;&quot; } For control, we also create a “random” dXY run, where we take the most diverged species pair and randomize the population assignment of the samples. Therefore, we first pick the most diverged species pair. // git 4.12 // collect a species pair to randomize Channel .from( [[&#39;bel&#39;, &#39;ind&#39;, &#39;may&#39;]] ) .set{ random_run_ch } Then we set the sliding window resolution. // git 4.13 // setup channel content for random channel Channel .from( 1 ) .combine( random_run_ch ) .combine( kb_ch2 ) .filter{ it[4] == 5 } .set{ random_sets_ch } Now, we randomize the population assignment of the samples and calculate differentiation. // git 4.14 // permute the population assignment (the randomization) process randomize_samples { label &#39;L_20g15h_randomize_samples&#39; publishDir &quot;../../2_analysis/fst/${kb}0k/random&quot;, mode: &#39;copy&#39; , pattern: &quot;*_windowed.weir.fst.gz&quot; module &quot;R3.5.2&quot; input: set val( random_set ), val( loc ), val(spec1), val(spec2), val( kb ) from random_sets_ch output: set random_set, file( &quot;random_pop.txt&quot; ) into random_pops_ch file( &quot;*_windowed.weir.fst.gz&quot;) into random_fst_out script: &quot;&quot;&quot; cut -f 2,3 \\$BASE_DIR/metadata/sample_info.txt | \\ grep &quot;${loc}&quot; | \\ grep &quot;${spec1}\\\\|${spec2}&quot; &gt; pop_prep.tsv Rscript --vanilla \\$BASE_DIR/R/randomize_pops.R grep A random_pop.txt | cut -f 1 &gt; pop1.txt grep B random_pop.txt | cut -f 1 &gt; pop2.txt vcftools \\ --gzvcf \\$BASE_DIR/1_genotyping/3_gatk_filtered/filterd_bi-allelic.allBP.vcf.gz \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --fst-window-step ${kb}0000 \\ --fst-window-size ${kb}0000 \\ --stdout | gzip &gt; ${loc}-aaa-bbb.${kb}0k.random_${spec1}_${spec2}_windowed.weir.fst.gz &quot;&quot;&quot; } We set up another channel to prepare the random dXY… // git 4.15 // pick random pair of interest random_dxy_pairs_ch .filter{ it[0] == &#39;bel&#39; &amp;&amp; it[1] == &#39;ind&#39; &amp;&amp; it[2] == &#39;may&#39; &amp;&amp; it[6] == 5 } .combine( random_pops_ch ) .set{ random_assigned_ch } .. and compute the divergence. // git 4.16 // compute the dxy values process dxy_lg_random { label &#39;L_G32g15h_dxy_lg_random&#39; tag &quot;aaa${loc}-bbb${loc}_LG${lg}&quot; module &quot;R3.5.2&quot; input: set val( loc ), val( spec1 ), val( spec2 ), val( lg ), file( vcf ), file( geno ), val( kb ), val( random_set ), file( pop_file ) from random_assigned_ch output: set val( &quot;aaa${loc}-bbb${loc}-${kb}0kb&quot; ), file( &quot;dxy.aaa${loc}-bbb${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz&quot; ), val( lg ), val( &quot;aaa${loc}&quot; ), val( &quot;bbb${loc}&quot; ), val( kb ) into dxy_random_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 module load intel17.0.4 intelmpi17.0.4 mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/popgenWindows.py \\ -w ${kb}0000 -s ${kb}000 \\ --popsFile ${pop_file} \\ -p A -p B \\ -g ${geno} \\ -o dxy.aaa${loc}-bbb${loc}.LG${lg}.${kb}0kb-${kb}kb.txt.gz \\ -f phased \\ --writeFailedWindows \\ -T 1 &quot;&quot;&quot; } Again, we collect the output of the individual LGs…. // git 4.17 // collect all LGs of random run dxy_random_lg_ch .groupTuple() .set{ tubbled_random_dxy } … and we merge them // git 4.18 // concatinate all LGs of random run process receive_random_tuple { label &#39;L_20g2h_receive_random_tuple&#39; publishDir &quot;../../2_analysis/dxy/random/&quot;, mode: &#39;copy&#39; input: set val( comp ), file( dxy ), val( lg ), val( pop1 ), val( pop2 ), val( kb ) from tubbled_random_dxy output: file( &quot;dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv.gz&quot; ) into dxy_random_output_ch script: &quot;&quot;&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG01.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ head -n 1 &gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; for j in {01..24};do echo &quot;-&gt; LG\\$j&quot; zcat dxy.${pop1[0]}-${pop2[0]}.LG\\$j.${kb[0]}0kb-${kb[0]}kb.txt.gz | \\ awk &#39;NR&gt;1{print}&#39; &gt;&gt; dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv; done gzip dxy.${pop1[0]}-${pop2[0]}.${kb[0]}0kb-${kb[0]}kb.tsv &quot;&quot;&quot; } 5.2.3 \\(\\pi\\) Estimating the diversity is quite straight forward: We take the prepared population identifier, the data set and the window resolutions and run VCFtools on each combination. // --------------------------------------------------------------- // The pi part need to be run AFTER the global fst outlier // windows were selected (REMEMBER TO CHECK FST OUTLIER DIRECTORY) // --------------------------------------------------------------- // git 4.19 // calculate pi per species process pi_per_spec { label &#39;L_32g15h_pi&#39; tag &quot;${spec}&quot; publishDir &quot;../../2_analysis/pi/${kb}0k&quot;, mode: &#39;copy&#39; input: set val( spec ), vcfId, file( vcf ), val( kb ) from spec_dxy.combine( vcf_pi_ch ).combine( kb_ch3 ) output: file( &quot;*.${kb}0k.windowed.pi.gz&quot; ) into pi_50k script: &quot;&quot;&quot; module load openssl1.0.2 vcfsamplenames ${vcf[0]} | \\ grep ${spec} &gt; pop.txt vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --window-pi ${kb}0000 \\ --window-pi-step ${kb}000 \\ --out ${spec}.${kb}0k 2&gt; ${spec}.pi.log gzip ${spec}.${kb}0k.windowed.pi tail -n +2 \\$BASE_DIR/2_analysis/summaries/fst_outliers_998.tsv | \\ cut -f 2,3,4 &gt; outlier.bed vcftools --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --exclude-bed outlier.bed \\ --window-pi ${kb}0000 \\ --window-pi-step ${kb}000\\ --out ${spec}_no_outlier.${kb}0k 2&gt; ${spec}_${kb}0k_no_outllier.pi.log gzip ${spec}_no_outlier.${kb}0k.windowed.pi &quot;&quot;&quot; } At this step we are done with divergence and diversity. "],
["git-5-analysis-iii-topology-weighting.html", "6 (git 5) Analysis III (topology weighting) 6.1 Summary 6.2 Details of analysis_fasttree_twisst.nf", " 6 (git 5) Analysis III (topology weighting) This pipeline can be executed as follows: cd $BASE_DIR/nf/05_analysis_fasttree_twisst source ../sh/nextflow_alias.sh nf_run_phylo 6.1 Summary A superseded version of the whole genome phylogeny and the topology weighting are prepared within the nextflow script analysis_fasttree_twisst.nf (located under $BASE_DIR/nf/05_analysis_fasttree_twisst/), which runs on the SNPs only data set. Below is an overview of the steps involved in the process. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 6.2 Details of analysis_fasttree_twisst.nf 6.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into two different streams (one for the phylogeny and one for topology weighting). #!/usr/bin/env nextflow // git 5.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_fasttree_whg; vcf_locations } 6.2.2 Whole genome phylogeny During data exploration, various subsets of the data set were investigated, including the different sampling locations, whole genome vs. non-outlier regions and all samples vs. hamlets only. (Now, most of the options have been muted by commenting these options out.) Here, we set up a channel managing the subset by location. // git 5.2 // setting the sampling location // (the script is set up to run on diffferent subsets of samples) Channel .from( &quot;all&quot; ) //, &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot; ) .set{ locations4_ch } This channel toggles the inclusion of \\(F_{ST}\\) outlier regions. // git 5.3 // setting loci restrictions // (keep vs remove outlier regions) Channel .from( &quot;whg&quot; ) //, &quot;no_outl&quot; ) .set{ whg_modes } This channel toggles the inclusion of the non-hamlet outgroup. // git 5.4 // setting the sampling mode // (the script is set up to run on diffferent subsets of samples) Channel .from( &quot;no_outgroups&quot; ) //, &quot;all&quot; ) .into{ sample_modes } We combine the different selector channels to create all possible combinations of the settings. // git 5.5 // compile the config settings and add data file locations4_ch .combine( vcf_fasttree_whg ) .combine( whg_modes ) .combine( sample_modes ) .set{ vcf_fasttree_whg_location_combo } To prepare the input for the phylogeny, the fine tuned selection is applied to subset the genotypes. // git 5.6 // apply sample filter, subset and convert genotypes process subset_vcf_by_location_whg { label &quot;L_28g5h_subset_vcf_whg&quot; input: set val( loc ), vcfId, file( vcf ), val( mode ), val( sample_mode ) from vcf_fasttree_whg_location_combo output: set val( mode ), val( loc ), val( sample_mode ), file( &quot;${loc}.${mode}.${sample_mode}.whg.geno.gz&quot; ) into snp_geno_tree_whg script: &quot;&quot;&quot; DROP_CHRS=&quot; &quot; # check if samples need to be dropped based on location if [ &quot;${loc}&quot; == &quot;all&quot; ];then vcfsamplenames ${vcf[0]} &gt; prep.pop else vcfsamplenames ${vcf[0]} | \\ grep ${loc} &gt; prep.pop fi # check if outgroups need to be dropped if [ &quot;${sample_mode}&quot; == &quot;all&quot; ];then mv prep.pop ${loc}.pop else cat prep.pop | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop fi # check if diverged LGs need to be dropped if [ &quot;${mode}&quot; == &quot;no_outl&quot; ];then DROP_CHRS=&quot;--not-chr LG04 --not-chr LG07 --not-chr LG08 --not-chr LG09 --not-chr LG12 --not-chr LG17 --not-chr LG23&quot; fi vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ \\$DROP_CHRS \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.${mode}.${sample_mode}.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i ${loc}.${mode}.${sample_mode}.vcf.gz | gzip &gt; ${loc}.${mode}.${sample_mode}.whg.geno.gz &quot;&quot;&quot; } The subset is converted to fasta format, creating two whole genome pseudo haplotypes per sample. // git 5.7 // convert genotypes to fasta process fasttree_whg_prep { label &#39;L_190g4h_fasttree_whg_prep&#39; tag &quot;${mode} - ${loc} - ${sample_mode}&quot; input: set val( mode ), val( loc ), val( sample_mode ), file( geno ) from snp_geno_tree_whg output: set val( mode ), val( loc ), val( sample_mode ), file( &quot;all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa&quot; ) into ( fasttree_whg_prep_ch ) script: &quot;&quot;&quot; python \\$SFTWR/genomics_general/genoToSeq.py -g ${geno} \\ -s all_samples.${loc}.${mode}.${sample_mode}.whg.SNP.fa \\ -f fasta \\ --splitPhased &quot;&quot;&quot; } Then, the phylogeny is reconstructed based on the converted geneotypes. // git 5.8 // create phylogeny process fasttree_whg_run { label &#39;L_300g30h_fasttree_run&#39; tag &quot;${mode} - ${loc} - ${sample_mode}&quot; publishDir &quot;../../2_analysis/fasttree/&quot;, mode: &#39;copy&#39; input: set val( mode ), val( loc ), val( sample_mode ), file( fa ) from fasttree_whg_prep_ch output: file( &quot;${sample_mode}.${loc}.${mode}.SNP.tree&quot; ) into ( fasttree_whg_output ) script: &quot;&quot;&quot; fasttree -nt ${fa} &gt; ${sample_mode}.${loc}.${mode}.SNP.tree &quot;&quot;&quot; } 6.2.3 Topology weighting The complexity of topology weighting increases non-linearely with the number of included populations as the number of possible unrooted topologies skyrockets. The maximum number of possible populations allowed within twisst is eight, so we are running the topology weighting for Belize and Honduras independently (for the three species at Panama only one unrooted topology is possible, so we don’t run twisst here). We start by setting up a channel for the sampling location. // git 5.9 // initialize the locations for topology weighting Channel .from( &quot;bel&quot;, &quot;hon&quot; ) .set{ locations_ch } Then, we attach the genotypes to the location. // git 5.10 locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } The analysis is split by linkage group, so we need to initialize the LGs. // git 5.11 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .set{ lg_twisst } The data is subset to include only the samples of the respective location. // git 5.12 // subset the genotypes by location process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.pop&quot; ) into ( vcf_loc_twisst ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.vcf.gz &quot;&quot;&quot; } While running twisst, we ran into an issues regarding incompatibilities of the used scheduling system used on our computing cluster and the threading within the python scripts used in the preparation of twisst. Unfortunately, this prevented us running the preparation in place (as part of the nextflow script). Instead, we rand the preparation separately on a local computer and clumsily plugged the results into the nextflow process. Below we include the originally intended workflow which we muted so that the script is runnable using the plug-in approach. Still, the the original workflow describes the steps executed locally and conveys the intermediate steps more clearly. Also, on a different computer cluster, the original script should work alright. The next step is to attach the LGs to the genotype subsets. // --------------------------------------------------------------- // Unfortunately the twisst preparation did not work on the cluster // (&#39;in place&#39;), so I had to setup the files locally and then plug // them into this workflow. // Below is the originally intended clean workflow (commented out), // while the plugin version picks up at git 5.19. // --------------------------------------------------------------- /* MUTE: // git 5.13 // add the lg channel to the genotype subset vcf_loc_twisst .combine( lg_twisst ) .set{ vcf_loc_lg_twisst } */ Based on the LGs the genotypes are split. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.14 // subset genotypes by LG process vcf2geno_loc { label &#39;L_20g15h_vcf2geno&#39; input: set val( loc ), file( vcf ), file( pop ), val( lg ) from vcf_loc_lg_twisst output: set val( loc ), val( lg ), file( &quot;${loc}.${lg}.geno.gz&quot; ), file( pop ) into snp_geno_twisst script: &quot;&quot;&quot; vcftools \\ --gzvcf ${vcf} \\ --chr ${lg} \\ --recode \\ --stdout | gzip &gt; intermediate.vcf.gz python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py \\ -i intermediate.vcf.gz | gzip &gt; ${loc}.${lg}.geno.gz &quot;&quot;&quot; } */ /* MUTE: python thread conflict - run locally and feed into ressources/plugin We initialize the window size size (as SNPs) used for the topology weighting… // git 5.15 // initialize SNP window size Channel.from( 50, 200 ).set{ twisst_window_types } */ …and attach them to the genotypes. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.16 // add the SNP window size to the genotype subset snp_geno_twisst.combine( twisst_window_types ).set{ twisst_input_ch } */ To conduct topology weighting, we need some underlying phylogenies, so we run PhyML along the sliding window. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.17 // create the phylogenies along the sliding window process twisst_prep { label &#39;L_G120g40h_prep_twisst&#39; publishDir &quot;../../2_analysis/twisst/positions/${loc}/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ) from twisst_input_ch.filter { it[0] != &#39;pan&#39; } output: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( &quot;*.trees.gz&quot; ), file( &quot;*.data.tsv&quot; ) into twisst_prep_ch script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/genomics_general/phylo/phyml_sliding_windows.py \\ -g ${geno} \\ --windType sites \\ -w ${twisst_w} \\ --prefix ${loc}.${lg}.w${twisst_w}.phyml_bionj \\ --model HKY85 \\ --optimise n \\ --threads 1 &quot;&quot;&quot; } */ The last step then is to run twisst on the prepared phylogenies. /* MUTE: python thread conflict - run locally and feed into ressources/plugin // git 5.18 // run the topology weighting on the phylogenies process twisst_run { label &#39;L_G120g40h_run_twisst&#39; publishDir &quot;../../2_analysis/twisst/weights/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( lg ), file( geno ), file( pop ), val( twisst_w ), file( tree ), file( data ) from twisst_prep_ch output: set val( loc ), val( lg ), val( twisst_w ), file( &quot;*.weights.tsv.gz&quot; ), file( &quot;*.data.tsv&quot; ) into ( twisst_output ) script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; ${pop} | \\ sed &#39;s/\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; | \\ cut -f 1,3 | \\ awk &#39;{print \\$1&quot;_A\\\\t&quot;\\$2&quot;\\\\n&quot;\\$1&quot;_B\\\\t&quot;\\$2}&#39; &gt; ${loc}.${lg}.twisst_pop.txt TWISST_POPS=\\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d&#39;,&#39; | sed &#39;s/,/ -g /g; s/^/-g /&#39; ) mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/twisst/twisst.py \\ --method complete \\ -t ${tree} \\ -T 1 \\ \\$TWISST_POPS \\ --groupsFile ${loc}.${lg}.twisst_pop.txt | \\ gzip &gt; ${loc}.${lg}.w${twisst_w}.phyml_bionj.weights.tsv.gz &quot;&quot;&quot; } */ Here, the plug in approach picks up. At this point we have run PhyML locally and deposited the results under $BASE_DIR/ressources/plugin/trees. To restart, we need to emulate the settings needed for the twisst process. // git 5.19 // emulate setting Channel .from(50, 200) .combine( vcf_loc_twisst ) .combine( lg_twisst ) .set{ twisst_modes } Then, we feed the phylogenies from an external directory into twisst. // git 5.20 // run the topology weighting on the phylogenies process twisst_plugin { label &#39;L_G120g40h_twisst_plugin&#39; publishDir &quot;../../2_analysis/twisst/weights/&quot;, mode: &#39;copy&#39; tag &quot;${loc}-${lg}-${mode}&quot; input: set val( mode ), val( loc ), file( vcf ), file( pop ), val( lg ) from twisst_modes output: set val( loc ), val( lg ), file( &quot;*.weights.tsv.gz&quot; ) into ( twisst_output ) script: &quot;&quot;&quot; module load intel17.0.4 intelmpi17.0.4 awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; ${pop} | \\ sed &#39;s/\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; | \\ cut -f 1,3 | \\ awk &#39;{print \\$1&quot;_A\\\\t&quot;\\$2&quot;\\\\n&quot;\\$1&quot;_B\\\\t&quot;\\$2}&#39; &gt; ${loc}.${lg}.twisst_pop.txt TWISST_POPS=\\$( cut -f 2 ${loc}.${lg}.twisst_pop.txt | sort | uniq | paste -s -d&#39;,&#39; | sed &#39;s/,/ -g /g; s/^/-g /&#39; ) mpirun \\$NQSII_MPIOPTS -np 1 \\ python \\$SFTWR/twisst/twisst.py \\ --method complete \\ -t \\$BASE_DIR/ressources/plugin/trees/${loc}/${loc}.${lg}.w${mode}.phyml_bionj.trees.gz \\ \\$TWISST_POPS \\ --groupsFile ${loc}.${lg}.twisst_pop.txt | \\ gzip &gt; ${loc}.${lg}.w${mode}.phyml_bionj.weights.tsv.gz &quot;&quot;&quot; } At this step we are done with phylogeny and the topology weighting. "],
["git-6-analysis-iv-rho.html", "7 (git 6) Analysis IV (rho) 7.1 Summary 7.2 Details of analysis_recombination.nf", " 7 (git 6) Analysis IV (rho) This pipeline can be executed as follows: cd $BASE_DIR/nf/06_analysis_recombination source ../sh/nextflow_alias.sh nf_run_recombination 7.1 Summary The population recombination rate is estimated within the nextflow script analysis_recombination.nf (located under $BASE_DIR/nf/06_analysis_recombination/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 7.2 Details of analysis_recombination.nf 7.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // This pipeline includes the recombination anlysis // git 6.1 // load genotypes Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } The estimation of the population recombination rate using FastEPRR happens in three steps. The first step is run independently for all linkage groups, so we set up a channel for the LGs. // git 6.2 // initialize LGs Channel .from( 1..24 ) .map{ it.toString().padLeft(2, &quot;0&quot;) } .set{ lg_ch } To prepare the input, the genotypes are split by LG. // git 6.3 // split genotypes by LG process split_allBP { label &#39;L_20g2h_split_by_lg&#39; tag &quot;LG${lg}&quot; input: set val( lg ), vcfId, file( vcf ) from lg_ch.combine( vcf_ch ) output: set val( lg ), file( &quot;phased_mac2.LG${lg}.vcf.gz&quot; ) into vcf_by_lg_ch script: &quot;&quot;&quot; module load openssl1.0.2 vcftools --gzvcf ${vcf[0]} \\ --chr LG${lg} \\ --recode \\ --stdout | bgzip &gt; phased_mac2.LG${lg}.vcf.gz &quot;&quot;&quot; } The prepared data is then fed to the first step of FastEPRR. // git 6.4 // run fasteprr step 1 process fasteprr_s1 { label &#39;L_20g2h_fasteprr_s1&#39; tag &quot;LG${lg}&quot; module &quot;R3.5.2&quot; input: set val( lg ), file( vcf ) from vcf_by_lg_ch output: file( &quot;step1_LG${lg}&quot; ) into step_1_out_ch script: &quot;&quot;&quot; mkdir step1_LG${lg} Rscript --vanilla \\$BASE_DIR/R/fasteprr_step1.R ./${vcf} step1_LG${lg} LG${lg} 50 &quot;&quot;&quot; } Since nextflow manages the results of its processes in a complex file structure, we need to collect all results of step 1 and bundle them before proceeding. // git 6.5 // collect step 1 output process fasteprr_s1_summary { label &#39;L_loc_fasteprr_s1_summmary&#39; input: file( step1 ) from step_1_out_ch.collect() output: file( &quot;step1&quot; ) into ( step1_ch1, step1_ch2 ) script: &quot;&quot;&quot; mkdir step1 cp step1_LG*/* step1/ &quot;&quot;&quot; } The second step of FastEPRR is parallelized over an arbitrary number of sub-processes. Here, we initialize 250 parallel processes and combine the parallelization index with the results from step 1. // git 6.6 // initialize fasteperr subprocesses and attach them to step 1 output Channel .from( 1..250 ) .map{ it.toString().padLeft(3, &quot;0&quot;) } .combine( step1_ch1 ) .set{ step_2_run_ch } Taking this prepared bundle, we now can start the second step of FastEPRR. // git 6.7 // run fasteprr step 2 process fasteprr_s2 { label &#39;L_long_loc_fasteprr_s2&#39; tag &quot;run_${idx}&quot; module &quot;R3.5.2&quot; input: set val( idx ), file( step1 ) from step_2_run_ch output: set val( idx ), file( &quot;step2_run${idx}&quot; ) into ( step_2_indxs, step_2_files ) script: &quot;&quot;&quot; mkdir -p step2_run${idx} Rscript --vanilla \\$BASE_DIR/R/fasteprr_step2.R ${step1} step2_run${idx} ${idx} &quot;&quot;&quot; } We collect both clones of the step 2 results and bundle the results in a single directory. // git 6.8 // collect step 2 output process fasteprr_s2_summary { label &#39;L_loc_fasteprr_s2_summmary&#39; input: val( idx ) from step_2_indxs.map{ it[0] }.collect() file( files ) from step_2_files.map{ it[1] }.collect() output: file( &quot;step2&quot; ) into ( step2_ch ) script: &quot;&quot;&quot; mkdir step2 for k in \\$( echo ${idx} | sed &#39;s/\\\\[//g; s/\\\\]//g; s/,//g&#39;); do cp -r step2_run\\$k/* step2/ done &quot;&quot;&quot; } Then we feed the bundled results into the third step of FastEPRR. // git 6.9 // run fasteprr step 3 process fasteprr_s3 { label &#39;L_32g4h_fasteprr_s3&#39; module &quot;R3.5.2&quot; input: set file( step1 ), file( step2 ) from step1_ch2.combine( step2_ch ) output: file( &quot;step3&quot; ) into step_3_out_ch script: &quot;&quot;&quot; mkdir step3 Rscript --vanilla \\$BASE_DIR/R/fasteprr_step3.R ${step1} ${step2} step3 &quot;&quot;&quot; } To ease the usage of the FastEPRR results downstream, we reformat them and compile a tidy table. // git 6.10 // reformat overall fasteprr output process fasteprr_s3_summary { label &#39;L_loc_fasteprr_s3_summmary&#39; publishDir &quot;../../2_analysis/fasteprr&quot;, mode: &#39;copy&#39; input: file( step3 ) from step_3_out_ch output: file( &quot;step4/fasteprr.all.rho.txt.gz&quot; ) into ( step3_ch ) script: &quot;&quot;&quot; mkdir step4 # ------ rewriting the fasteprr output into tidy format -------- for k in {01..24};do j=&quot;LG&quot;\\$k; echo \\$j; \\$BASE_DIR/sh/fasteprr_trans.sh step3/chr_\\$j \\$j step4/fasteprr.\\$j done # --------- combining all LGs into a single data set ----------- cd step4 head -n 1 fasteprr.LG01.rho.txt &gt; fasteprr.all.rho.txt for k in {01..24}; do echo &quot;LG&quot;\\$k awk &#39;NR&gt;1{print \\$0}&#39; fasteprr.LG\\$k.rho.txt &gt;&gt; fasteprr.all.rho.txt done gzip fasteprr.all.rho.txt cd .. &quot;&quot;&quot; } Finally, we are done with preparing the recombination rate for plotting. "],
["git-7-analysis-v-principal-component-analysis.html", "8 (git 7) Analysis V (Principal Component Analysis) 8.1 Summary 8.2 Details of analysis_pca.nf", " 8 (git 7) Analysis V (Principal Component Analysis) This pipeline can be executed as follows: cd $BASE_DIR/nf/07_analysis_pca source ../sh/nextflow_alias.sh nf_run_pca 8.1 Summary The PCAs are produced within the nextflow script analysis_pca.nf (located under $BASE_DIR/nf/07_analysis_pca/). It takes the SNPs only data set and runs the PCAs (for all samples and within locations) both for the whole genome and for the highly differentiated regions excluded. Below is an overview of the steps involved in the analysis. (The green dots indicate the input files, red dots depict output that is exported for further use.) 8.2 Details of analysis_pca.nf 8.2.1 Setup The nextflow script starts by setting the channel for data subset options. #!/usr/bin/env nextflow // git 7.1 // prepare subset modes (whole genome vs non-diverged regions) Channel .from( &quot;whg&quot;, &quot;subset_non_diverged&quot;) .set{ subset_type_ch } Also, the file with the genomic coordinates of the outlier regions is loaded. // git 7.2 // load table with differentiation outlier regions Channel .fromPath( &quot;../../2_analysis/summaries/fst_outliers_998.tsv&quot; ) .set{ outlier_tab } Then it combines the genotype file with the subset type and the outlier location file. // git 7.3 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .combine( outlier_tab ) .combine( subset_type_ch ) .set{ vcf_ch } Depending on subset type, the outlier regions are excluded from the genotypes. // git 7.4 // depending on subset mode, subset vcf process subset_vcf_divergence_based { label &quot;L_20g2h_subset_divergence&quot; input: set vcfId, file( vcf ), file( outlier_tab ), val( subset_type ) from vcf_ch output: set file( &quot;${subset_type}.vcf.gz&quot; ), file( &quot;${subset_type}.vcf.gz.tbi&quot; ), val( subset_type ) into ( vcf_locations, vcf_all_samples_pca ) script: &quot;&quot;&quot; if [ &quot;${subset_type}&quot; == &quot;subset_non_diverged&quot; ];then awk -v OFS=&quot;\\\\t&quot; &#39;{print \\$2,\\$3,\\$4}&#39; ${outlier_tab} &gt; diverged_regions.bed SUBSET=&quot;--exclude-bed diverged_regions.bed&quot; else SUBSET=&quot;&quot; fi vcftools --gzvcf ${vcf[0]} \\ \\$SUBSET \\ --recode \\ --stdout | bgzip &gt; ${subset_type}.vcf.gz tabix ${subset_type}.vcf.gz &quot;&quot;&quot; } The locations channel is set… // git 7.5 // prepare location channel for separate pcas Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } …and combined with the genotypes. // git 7.6 // attach genotypes to location channel locations_ch .combine( vcf_locations ) .set{ vcf_location_combo } Then, the genotypes are subset by location. // git 7.7 // subset vcf by location process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), file( vcf ), file( vcfidx ), val( subset_type ) from vcf_location_combo output: set val( loc ), file( &quot;*.vcf.gz&quot; ), file( &quot;*.pop&quot; ), val( subset_type ) into ( vcf_loc_pca ) script: &quot;&quot;&quot; vcfsamplenames ${vcf} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.${subset_type}.pop vcftools --gzvcf ${vcf} \\ --keep ${loc}.${subset_type}.pop \\ --mac 3 \\ --recode \\ --stdout | gzip &gt; ${loc}.${subset_type}.vcf.gz &quot;&quot;&quot; } Finally, the PCAs are run within the different locations… // PCA section // ----------- // git 7.8 // run pca by location process pca_location { label &quot;L_20g15h_pca_location&quot; publishDir &quot;../../figures/pca&quot;, mode: &#39;copy&#39; , pattern: &quot;*.pdf&quot; publishDir &quot;../../2_analysis/pca&quot;, mode: &#39;copy&#39; , pattern: &quot;*.gz&quot; input: set val( loc ), file( vcf ), file( pop ), val( subset_type ) from vcf_loc_pca output: set file( &quot;*.prime_pca.pdf&quot; ), file( &quot;*.pca.pdf&quot; ), file( &quot;*.exp_var.txt.gz&quot; ), file( &quot;*.scores.txt.gz&quot; ) into pca_loc_out script: &quot;&quot;&quot; awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; ${loc}.${subset_type}.pop | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; ${loc}.${subset_type}.pop.txt Rscript --vanilla \\$BASE_DIR/R/vcf2pca.R ${vcf} ${loc}.${subset_type}.pop.txt 6 &quot;&quot;&quot; } …and for the entire data set. // git 7.9 // run pca for global data set process pca_all { label &quot;L_20g15h_pca_all&quot; publishDir &quot;../../figures/pca&quot;, mode: &#39;copy&#39; , pattern: &quot;*.pdf&quot; publishDir &quot;../../2_analysis/pca&quot;, mode: &#39;copy&#39; , pattern: &quot;*.txt.gz&quot; publishDir &quot;../../1_genotyping/4_phased/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.vcf.gz&quot; input: set file( vcf ), file( vcfidx ), val( subset_type ) from vcf_all_samples_pca output: set file( &quot;*.prime_pca.pdf&quot; ), file( &quot;*.pca.pdf&quot; ), file( &quot;*.exp_var.txt.gz&quot; ), file( &quot;*.scores.txt.gz&quot; ) into pca_all_out file( &quot;hamlets_only.${subset_type}.vcf.gz&quot; ) into vcf_hamlets_only set file( &quot;hamlets_only.${subset_type}.vcf.gz&quot; ), file( &quot;hamlets_only.${subset_type}.pop.txt&quot; ) into vcf_multi_fst script: &quot;&quot;&quot; # complete PCA, all samples ------------ vcfsamplenames ${vcf} | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; all.${subset_type}.pop.txt Rscript --vanilla \\$BASE_DIR/R/vcf2pca.R ${vcf} all.${subset_type}.pop.txt 6 # PCA without outgroups --------------- vcfsamplenames ${vcf} | \\ grep -v &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni\\\\|flo&quot; &gt; outgroup.${subset_type}.pop vcfsamplenames ${vcf} | \\ grep &quot;abe\\\\|gum\\\\|ind\\\\|may\\\\|nig\\\\|pue\\\\|ran\\\\|uni\\\\|flo&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; hamlets_only.${subset_type}.pop.txt vcftools \\ --gzvcf ${vcf} \\ --remove outgroup.${subset_type}.pop \\ --recode \\ --stdout | gzip &gt; hamlets_only.${subset_type}.vcf.gz Rscript --vanilla \\$BASE_DIR/R/vcf2pca.R hamlets_only.${subset_type}.vcf.gz hamlets_only.${subset_type}.pop.txt 6 &quot;&quot;&quot; } "],
["git-8-analysis-vi-demographic-history.html", "9 (git 8) Analysis VI (Demographic History) 9.1 Summary 9.2 Details of analysis_msmc.nf", " 9 (git 8) Analysis VI (Demographic History) This pipeline can be executed as follows: cd $BASE_DIR/nf/08_analysis_msmc source ../sh/nextflow_alias.sh nf_run_msmc 9.1 Summary The demographic history rate is inferred within the nextflow script analysis_msmc.nf (located under $BASE_DIR/nf/08_analysis_msmc/), which runs on the SNPs only data set. Below is an overview of the steps involved in the inference. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 9.2 Details of analysis_msmc.nf 9.2.1 Data preparation The first part of the scripts includes a large block of preparation work. In this initial block, the data masks are being generated based on the samples coverage statistics combined with the locations of idels and the reference genomes mappability. The whole script is opened by a creating a channel for the linkage groups since the coverage statistics are being created on a linkage group basis. #!/usr/bin/env nextflow // git 8.1 // create channel of linkage groups Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{ &quot;LG&quot; + it } .into{ lg_ch1; lg_ch2; lg_ch3 } Then the phased genotype data is opened (for later use in msmc). // git 8.2 // open phased genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_msmc } To extract the sequencing depth for each individual, the unphased genotypes are opened as well (as this information is lost during phasing). // git 8.3 // open unphased genotype data to extract depth information Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/filterd_bi-allelic.vcf.{gz,gz.tbi}&quot;) .set{ vcf_depth } The outgroups are removed from the data set and the depth is reported for each individual. // git 8.4 // gather depth per individual process gather_depth { label &#39;L_20g2h_split_by_sample&#39; publishDir &quot;../../metadata&quot;, mode: &#39;copy&#39; input: set vcfID, file( vcf ) from vcf_depth output: file( &quot;depth_by_sample.txt&quot; ) into depth_ch script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --depth \\ --stdout &gt; depth_by_sample.txt &quot;&quot;&quot; } The depth information is fed into a channel so that the information is accessible for nextflow. // git 8.5 // create channel out of sequencing depth table depth_ch .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ id:row.INDV, sites:row.N_SITES, depth:row.MEAN_DEPTH] } .map{ [it.id, it.sites, it.depth] } .set { depth_by_sample_ch } Next, a channel is created from all the original .bam files from the mapped sequences (git 1.6). // git 8.6 // create channel from bam files and add sample id Channel .fromPath( &#39;../../1_genotyping/0_dedup_bams/*.bam&#39; ) .map{ file -&gt; def key = file.name.toString().tokenize(&#39;.&#39;).get(0) return tuple(key, file)} .set{ sample_bams } The previously created depth information is attached to the bam channel… // git 8.7 // combine sample bams and sequencing depth sample_bams .join( depth_by_sample_ch ) .set{ sample_bam_and_depth } … and the genotype data and individual linkage groups are added. // git 8.8 // multiply the sample channel by the linkage groups sample_bam_and_depth .combine( vcf_msmc ) .combine( lg_ch1 ) .set{ samples_msmc } Now, the data is split by individual and all the additional information is passed on to the masking (git 8.10) as well as to the production of the the individuals segregating sites (git 8.11). // git 8.9 // split vcf by individual process split_vcf_by_individual { label &#39;L_20g15m_split_by_vcf&#39; input: set val( id ), file( bam ), val( sites ), val( depth ), val( vcf_id ), file( vcf ), val( lg ) from samples_msmc output: set val( id ), val( lg ), file( bam ), val( depth ), file( &quot;phased_mac2.${id}.${lg}.vcf.gz&quot; ) into ( sample_vcf, sample_vcf2 ) script: &quot;&quot;&quot; gatk --java-options &quot;-Xmx10G&quot; \\ SelectVariants \\ -R \\$REF_GENOME \\ -V ${vcf[0]} \\ -sn ${id} \\ -L ${lg}\\ -O phased_mac2.${id}.${lg}.vcf.gz &quot;&quot;&quot; } The individual coverage statistics are then being queried using the individuals average depth to create the coverage mask for each individual. // git 8.10 // create coverage mask from original mapped sequences process bam_caller { label &#39;L_36g47h_bam_caller&#39; publishDir &quot;../../ressources/coverage_masks&quot;, mode: &#39;copy&#39; , pattern: &quot;*.coverage_mask.bed.gz&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: set val( id ), val( lg ), file( bam ), val( depth ), file( vcf ) from sample_vcf output: set val( id ), val( lg ), file( &quot;*.bam_caller.vcf.gz&quot; ), file( &quot;*.coverage_mask.bed.gz&quot; ) into coverage_by_sample_lg script: &quot;&quot;&quot; module load openssl1.0.2 samtools index ${bam} samtools mpileup -q 25 -Q 20 -C 50 -u -r ${lg} -f \\$REF_GENOME ${bam} | \\ bcftools call -c -V indels | \\ \\$BASE_DIR/py/bamHamletCaller.py ${depth} ${id}.${lg}.coverage_mask.bed.gz | \\ gzip -c &gt; ${id}.${lg}.bam_caller.vcf.gz &quot;&quot;&quot; } For each individual, the segregating sites are created. // git 8.11 // create segsites file process generate_segsites { label &quot;L_20g15m_msmc_generate_segsites&quot; publishDir &quot;../../2_analysis/msmc/segsites&quot;, mode: &#39;copy&#39; , pattern: &quot;*.segsites.vcf.gz&quot; input: set val( id ), val( lg ), file( bam ), val( depth ), file( vcf ) from sample_vcf2 output: set val( id ), val( lg ), file( &quot;*.segsites.vcf.gz&quot; ), file( &quot;*.covered_sites.bed.txt.gz&quot; ) into segsites_by_sample_lg script: &quot;&quot;&quot; zcat ${vcf} | \\ vcfAllSiteParser.py ${id} ${id}.${lg}.covered_sites.bed.txt.gz | \\ gzip -c &gt; ${id}.${lg}.segsites.vcf.gz &quot;&quot;&quot; } 9.2.2 Grouping of individuals At this point, the data masks are prepared and the samples can be assigned to their respective groups for their demographic history and and cross-coalescence rate inference. // git 8.12 // assign samples randomly across MSMC and cross coalescence runs process msmc_sample_grouping { label &quot;L_loc_msmc_grouping&quot; publishDir &quot;../../2_analysis/msmc/setup&quot;, mode: &#39;copy&#39; module &quot;R3.5.2&quot; output: file( &quot;msmc_grouping.txt&quot; ) into msmc_grouping file( &quot;msmc_cc_grouping.txt&quot; ) into cc_grouping script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/sample_assignment_msmc.R \\ \\$BASE_DIR/R/distribute_samples_msmc_and_cc.R \\ \\$BASE_DIR/R/cross_cc.R \\ \\$BASE_DIR/metadata/sample_info.txt \\ msmc &quot;&quot;&quot; } The results of the random assignment are being fed into a channel. // git 8.13 // read grouping into a channel msmc_grouping .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ run:row.msmc_run, spec:row.spec, geo:row.geo, group_nr:row.group_nr, group_size:row.group_size, samples:row.samples ] } .set { msmc_runs } Since this script uses an unorthodox way of making the results of the data preparation available to all following processes by exporting them back to the root folder, the following dummy process is installed to wait for the data preparation to finish before proceeding with the workflow. // git 8.14 // wait for bam_caller and generate_segsites to finish: /*this &#39;.collect&#39; is only meant to wait until the channel is done, files are being redirected via publishDir*/ coverage_by_sample_lg.collect().map{ &quot;coverage done!&quot; }.into{ coverage_done; coverage_cc } segsites_by_sample_lg.collect().map{ &quot;segsites done!&quot; }.into{ segsites_done; segsites_cc } To set up the msmc2 runs, the sample grouping is waiting for the dummy process to finish. // git 8.15 // attach masks to MSMC group assignment lg_ch2 .combine( msmc_runs ) .combine( coverage_done ) .combine( segsites_done ) .map{[it[0], it[1].run, it[1]]} .set{ msmc_grouping_after_segsites } Then, the specific msmc2 input files are compiled from the combined masks of the involved samples (for each linkage group individually). // git 8.16 // generating MSMC input files (4 or 3 inds per species) process generate_multihetsep { label &quot;L_120g40h_msmc_generate_multihetsep&quot; publishDir &quot;../../2_analysis/msmc/input/run_${run}&quot;, mode: &#39;copy&#39; , pattern: &quot;*.multihetsep.txt&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: /* content msmc_gr: val( msmc_run ), val( spec ), val( geo ), val( group_nr ), val( group_size ), val( samples ) */ /*[LG20, [msmc_run:45, spec:uni, geo:pan, group_nr:4, group_size:3, samples:ind1, ind2, ind3], coverage done!, segsites done!]*/ set val( lg ), val( run ), msmc_gr from msmc_grouping_after_segsites output: set val( run ), val( lg ), val( msmc_gr.spec ), val( msmc_gr.geo ), val( msmc_gr.group_size ), file( &quot;msmc_run.*.multihetsep.txt&quot; ) into msmc_input_lg script: &quot;&quot;&quot; COVDIR=&quot;\\$BASE_DIR/ressources/coverage_masks/&quot; SMP=\\$(echo ${msmc_gr.samples} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SEGDIR=&quot;\\$BASE_DIR/2_analysis/msmc/segsites/&quot; SEG=\\$(echo ${msmc_gr.samples} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) generate_multihetsep.py \\ \\$SMP \\ --mask=\\$BASE_DIR/ressources/mappability_masks/${lg}.mapmask.bed.txt.gz \\ --negative_mask=\\$BASE_DIR/ressources/indel_masks/indel_mask.${lg}.bed.gz \\ \\$SEG &gt; msmc_run.${msmc_gr.run}.${msmc_gr.spec}.${msmc_gr.geo}.${lg}.multihetsep.txt &quot;&quot;&quot; } The input files of all linkage group are collected for each sample grouping. // git 8.17 // collect all linkage groups for each run msmc_input_lg .groupTuple() .set {msmc_input} And finally msmc2 is executed to infer the demographic history of the involved samples. // git 8.18 // run msmc process msmc_run { label &quot;L_190g100h_msmc_run&quot; publishDir &quot;../../2_analysis/msmc/output/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.final.txt&quot; publishDir &quot;../../2_analysis/msmc/loops/&quot;, mode: &#39;copy&#39; , pattern: &quot;*.loop.txt&quot; input: set msmc_run, lg , spec, geo, group_size, file( hetsep ) from msmc_input output: file(&quot;*.msmc2.*.txt&quot;) into msmc_output script: &quot;&quot;&quot; NHAP=\\$(echo \\$(seq 0 \\$((${group_size[0]}*2-1))) | sed &#39;s/ /,/g&#39; ) INFILES=\\$( echo ${hetsep} ) msmc2 \\ -m 0.00254966 -t 8 \\ -p 1*2+25*1+1*2+1*3 \\ -o run${msmc_run}.${spec[0]}.${geo[0]}.msmc2 \\ -I \\${NHAP} \\ \\${INFILES} &quot;&quot;&quot; } The process cross-coalescence rate is similar (with git 8.19 being the equivalent of git 8.13). So, again the grouping information in fed into a channel. // git 8.19 // generate MSMC cross coalescence input files (2 inds x 2 species) cc_grouping .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ run:row.run_nr, geo:row.geo, spec_1:row.spec_1, spec_2:row.spec_2, contrast_nr:row.contrast_nr, samples_1:row.samples_1, samples_2:row.samples_2 ] } .set { cc_runs } The groups wait for the data preparation to finish. // git 8.20 // attach masks to cross coalescence group assignment lg_ch3 .combine( cc_runs ) .combine( coverage_cc ) .combine( segsites_cc ) .map{[it[0], it[1].run, it[1]]} .set{ cc_grouping_after_segsites } The msmc2 input files are being compiled based on the involved samples (for each linkage group). // git 8.21 // create multihetsep files (combination off all 4 individuals) process generate_multihetsep_cc { label &quot;L_105g30h_cc_generate_multihetsep&quot; publishDir &quot;../../2_analysis/cross_coalescence/input/run_${run}&quot;, mode: &#39;copy&#39; , pattern: &quot;*.multihetsep.txt&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: /* content cc_gr: val( run_nr ), val( geo ), val( spec_1 ), val( spec_2 ), val( contrast_nr ), val( samples_1 ), val( samples_2 ) */ set val( lg ), val( run ), cc_gr from cc_grouping_after_segsites output: set val( cc_gr.run ), val( lg ), val( cc_gr.spec_1 ), val( cc_gr.spec_2 ), val( cc_gr.geo ), val( cc_gr.contrast_nr ), val( cc_gr.samples_1 ), val( cc_gr.samples_2 ), file( &quot;cc_run.*.multihetsep.txt&quot; ) into cc_input_lg script: &quot;&quot;&quot; COVDIR=&quot;\\$BASE_DIR/ressources/coverage_masks/&quot; SMP1=\\$(echo ${cc_gr.samples_1} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SMP2=\\$(echo ${cc_gr.samples_2} | \\ sed &quot;s|, |\\\\n--mask=\\${COVDIR}|g; s|^|--mask=\\${COVDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.coverage_mask.bed.gz/g&quot; | \\ echo \\$( cat ) ) SEGDIR=&quot;\\$BASE_DIR/2_analysis/msmc/segsites/&quot; SEG1=\\$(echo ${cc_gr.samples_1} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) SEG2=\\$(echo ${cc_gr.samples_2} | \\ sed &quot;s|, |\\\\n\\${SEGDIR}|g; s|^|\\${SEGDIR}|g&quot; | \\ sed &quot;s/\\$/.${lg}.segsites.vcf.gz/g&quot; | \\ echo \\$( cat ) ) generate_multihetsep.py \\ \\${SMP1} \\ \\${SMP2} \\ --mask=\\$BASE_DIR/ressources/mappability_masks/${lg}.mapmask.bed.txt.gz \\ --negative_mask=\\$BASE_DIR/ressources/indel_masks/indel_mask.${lg}.bed.gz \\ \\${SEG1} \\ \\${SEG2} \\ &gt; cc_run.${run}.${cc_gr.spec_1}-${cc_gr.spec_2}.${cc_gr.contrast_nr}.${cc_gr.geo}.${lg}.multihetsep.txt &quot;&quot;&quot; } The input files of all linkage group are collected for each sample grouping. // git 8.22 // collect all linkage groups for each run cc_input_lg .groupTuple() .set {cc_input} And msmc2 is executed to infer the cross-coalescence rate of the involved samples. // git 8.23 // run cross coalescence process cc_run { label &quot;L_190g10ht24_cc_run&quot; publishDir &quot;../../2_analysis/cross_coalescence/output/&quot;, mode: &#39;copy&#39; tag &quot;${cc_run}-${geo[0]}:${spec1[0]}/${spec2[0]}&quot; conda &quot;$HOME/miniconda2/envs/py3&quot; input: set cc_run, lg , spec1, spec2, geo, contr_nr, samples_1, samples_2, file( hetsep ) from cc_input output: file(&quot;cc_run.*.final.txt.gz&quot;) into cc_output script: &quot;&quot;&quot; INFILES=\\$( echo ${hetsep} ) POP1=\\$( echo &quot;${samples_1}&quot; | sed &#39;s/\\\\[//g; s/, /,/g; s/\\\\]//g&#39; ) POP2=\\$( echo &quot;${samples_2}&quot; | sed &#39;s/\\\\[//g; s/, /,/g; s/\\\\]//g&#39; ) msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.${spec1[0]}.msmc \\ -I 0,1,2,3 \\ \\${INFILES} msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.${spec2[0]}.msmc \\ -I 4,5,6,7 \\ \\${INFILES} msmc2 \\ -m 0.00255863 -t 24 \\ -p 1*2+25*1+1*2+1*3 \\ -o cc_run.${cc_run}.cross.msmc \\ -I 0,1,2,3,4,5,6,7 \\ -P 0,0,0,0,1,1,1,1 \\ \\${INFILES} combineCrossCoal.py \\ cc_run.${cc_run}.cross.msmc.final.txt \\ cc_run.${cc_run}.${spec1[0]}.msmc.final.txt \\ cc_run.${cc_run}.${spec2[0]}.msmc.final.txt | \\ gzip &gt; cc_run.${cc_run}.final.txt.gz &quot;&quot;&quot; } Finally, we are done with the inference of the demographic history. "],
["git-9-analysis-vii-hybridization.html", "10 (git 9) Analysis VII (hybridization) 10.1 Summary 10.2 Details of analysis_hybridization.nf", " 10 (git 9) Analysis VII (hybridization) This pipeline can be executed as follows: cd $BASE_DIR/nf/09_analysis_hybridization source ../sh/nextflow_alias.sh nf_run_hybrid 10.1 Summary The hybridization classes are assigned within the nextflow script analysis_hybridization.nf (located under $BASE_DIR/nf/09_analysis_hybridization/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 10.2 Details of analysis_hybridization.nf 10.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 9.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_loc1; vcf_loc2; vcf_loc3 } Since we are going to work on the three sampling locations independently, we create channel for the locations. // git 9.2 // initialize location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } Next, we define the species sets sampled at the individual locations. // git 9.3 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } For each location, we create all possible species pairs and then merge the channels of the different locations. // git 9.4 // prepare pairwise new_hybrids // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .combine( vcf_loc1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .combine( vcf_loc2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .combine( vcf_loc3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[3] &lt; it[5] } .map{ it[0,1,2,4,6]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } We are going to run newhybrids only on a subset of the most diverged SNPs for each species pair. For this we first need to compute the \\(F_{ST}\\) on a SNP level for each species pair. // git 9.5 // comute pairwise fsts for SNP filtering process fst_run { label &#39;L_20g45m_fst_run&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( vcfidx ), file( vcf ), val( spec1 ), val( spec2 ) from all_fst_pairs_ch output: set val( loc ), val( spec1 ), val( spec2 ), file( &quot;${vcf[0]}&quot; ), file( &quot;*.fst.tsv.gz&quot; ), file( &quot;${spec1}${loc}.pop&quot;), file( &quot;${spec2}${loc}.pop&quot;) into fst_SNPS script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | grep ${spec1}${loc} &gt; ${spec1}${loc}.pop vcfsamplenames ${vcf[0]} | grep ${spec2}${loc} &gt; ${spec2}${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --weir-fst-pop ${spec1}${loc}.pop \\ --weir-fst-pop ${spec2}${loc}.pop \\ --stdout | gzip &gt; ${spec1}${loc}-${spec2}${loc}.fst.tsv.gz &quot;&quot;&quot; } For each species pair, the 800 most diverged SNPs for this particular pair are selected. // git 9.6 // select the 800 most differentiated SNPs for each population pair process filter_fst { label &#39;L_8g15m_filter_fst&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( fst ), file( pop1 ), file( pop2 ) from fst_SNPS output: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( pop1 ), file( pop2 ), file( &quot;*SNPs.snps&quot; ) into filter_SNPs script: &quot;&quot;&quot; Rscript --vanilla \\$BASE_DIR/R/filter_snps.R ${fst} 800 ${spec1}${loc}-${spec2}${loc} &quot;&quot;&quot; } This pre-selection is then filtered to guarantee a minimum distance of 5 kb between all SNPs. Of this filtered SNP set, 80 SNPs are randomly sampled and formateted for the newhybrid analysis. // git 9.7 // filter the SNP set by min distance (5kb), than randomly pick 80 SNPs // then reformat newhybrid input process prep_nh_input { label &#39;L_8g15m_prep_nh&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; input: set val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( pop1 ), file( pop2 ), file( snps ) from filter_SNPs output: set val( loc ), val( spec1 ), val( spec2 ), file( &quot;*_individuals.txt&quot; ), file( &quot;*.80SNPs.txt&quot;) into newhybrids_input script: &quot;&quot;&quot; vcftools \\ --gzvcf ${vcf} \\ --keep ${pop1} \\ --keep ${pop2} \\ --thin 5000 \\ --out newHyb.${spec1}${loc}-${spec2}${loc} \\ --positions ${snps} \\ --recode grep &#39;#&#39; newHyb.${spec1}${loc}-${spec2}${loc}.recode.vcf &gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf grep -v &#39;#&#39; newHyb.${spec1}${loc}-${spec2}${loc}.recode.vcf | \\ shuf -n 80 | \\ sort -k 1 -k2 &gt;&gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf grep &#39;#CHROM&#39; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf | \\ cut -f 10- | \\ sed &#39;s/\\\\t/\\\\n/g&#39; &gt; newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs_individuals.txt /usr/bin/java -Xmx1024m -Xms512M \\ -jar \\$SFTWR/PGDSpider/PGDSpider2-cli.jar \\ -inputfile newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.vcf \\ -inputformat VCF \\ -outputfile newHyb.${spec1}${loc}-${spec2}${loc}.80SNPs.txt \\ -outputformat NEWHYBRIDS \\ -spid \\$BASE_DIR/ressources/vcf2nh.spid &quot;&quot;&quot; } Using a prepared R sript we then can run newhybrids on the SNP selection. // git 9.8 // Run new hybrids // (copy of nh_input is needed because nh can&#39;t read links) process run_nh { label &#39;L_20g15h4x_run_nh&#39; tag &quot;${spec1}${loc}-${spec2}${loc}&quot; publishDir &quot;../../2_analysis/newhyb/&quot;, mode: &#39;copy&#39; input: set val( loc ), val( spec1 ), val( spec2 ), file( inds ), file( snps ) from newhybrids_input output: set file( &quot;nh_input/NH.Results/newHyb.*/*_individuals.txt&quot; ), file( &quot;nh_input/NH.Results/newHyb.*/*_PofZ.txt&quot; ) into newhybrids_output script: &quot;&quot;&quot; mkdir -p nh_input cp ${snps} nh_input/${snps} cp ${inds} nh_input/${inds} Rscript --vanilla \\$BASE_DIR/R/run_newhybrids.R &quot;&quot;&quot; } Finally, we are done with the hybridization analysis. "],
["git-10-analysis-viii-admixture.html", "11 (git 10) Analysis VIII (admixture) 11.1 Summary 11.2 Details of analysis_admixture.nf", " 11 (git 10) Analysis VIII (admixture) This pipeline can be executed as follows: cd $BASE_DIR/nf/10_analysis_admixture source ../sh/nextflow_alias.sh nf_run_admixture 11.1 Summary The degree of admixture is estimated within the nextflow script analysis_admixture.nf (located under $BASE_DIR/nf/10_analysis_admixture/), which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 11.2 Details of analysis_admixture.nf 11.2.1 Data preparation The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 10.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } Next, we select the range k values that we we want to explore within the admixture analysis. // git 10.2 // Set different k values for the admixture analysis Channel .from( 2..15 ) .set{ k_ch } Then, we open the \\(F_{ST}\\) outlier regions within which we want to run the admixture analysis. // git 10.3 // load Fst outlier regions Channel .fromPath(&quot;../../ressources/plugin/poptrees/outlier.bed&quot;) .splitCsv(header:true, sep:&quot;\\t&quot;) .map{ row -&gt; [ chrom:row.chrom, start:row.start, end:row.end, gid:row.gid ] } .combine( vcf_ch ) .set{ vcf_admx } Then, we subset the genotypes to each outlier regions of interest respectively and reformat them to the plink genotype format. // git 10.4 // subset genotypes to the outlier region and reformat process plink12 { label &#39;L_20g2h_plink12&#39; tag &quot;${grouping.gid}&quot; input: set val( grouping ), val( vcfidx ), file( vcf ) from vcf_admx output: set val( grouping ), file( &quot;hapmap.*.ped&quot; ), file( &quot;hapmap.*.map&quot; ), file( &quot;hapmap.*.nosex&quot; ), file( &quot;pop.txt&quot; ) into admx_plink script: &quot;&quot;&quot; echo -e &quot;CHROM\\\\tSTART\\\\tEND&quot; &gt; outl.bed echo -e &quot;${grouping.chrom}\\\\t${grouping.start}\\\\t${grouping.end}&quot; &gt;&gt; outl.bed vcfsamplenames ${vcf[0]} | \\ grep -v &quot;tor\\\\|tab\\\\|flo&quot; | \\ awk &#39;{print \\$1&quot;\\\\t&quot;\\$1}&#39; | \\ sed &#39;s/\\\\t.*\\\\(...\\\\)\\\\(...\\\\)\\$/\\\\t\\\\1\\\\t\\\\2/g&#39; &gt; pop.txt vcftools \\ --gzvcf ${vcf[0]} \\ --keep pop.txt \\ --bed outl.bed \\ --plink \\ --out admx_plink plink \\ --file admx_plink \\ --recode12 \\ --out hapmap.${grouping.gid} &quot;&quot;&quot; } Then, we combine the reformatted genotypes with the k values… // git 10.5 // combine genoutype subsets with k values admx_prep = k_ch.combine( admx_plink ) … and run admixture. // git 10.6 // run admixture process admixture_all { label &#39;L_20g4h_admixture_all&#39; publishDir &quot;../../2_analysis/admixture/&quot;, mode: &#39;copy&#39; tag &quot;${grouping.gid}.${k}&quot; input: set val( k ), val( grouping ), file( ped ), file( map ), file( nosex ), file( pop ) from admx_prep output: set val( &quot;dummy&quot; ), file( &quot;*.out&quot; ), file( &quot;*.Q&quot; ), file( &quot;*.txt&quot; ) into admx_log script: &quot;&quot;&quot; mv ${pop} pop.${grouping.gid}.${k}.txt admixture --cv ${ped} ${k} | tee log.${grouping.gid}.${k}.out &quot;&quot;&quot; } "],
["git-11-analysis-ix-allele-age.html", "12 (git 11) Analysis IX (Allele Age) 12.1 Summary 12.2 Details of analysis_allele_age.nf", " 12 (git 11) Analysis IX (Allele Age) This pipeline can be executed as follows: cd $BASE_DIR/nf/11_analysis_allele_age source ../sh/nextflow_alias.sh nf_run_aa 12.1 Summary The allele age is estimated within the nextflow script analysis_allele_age.nf (located under $BASE_DIR/nf/11_analysis_allele_age/) which runs on the SNPs only data set. Below is an overview of the steps involved in the analysis. (The green dot indicates the genotype input, red arrows depict output that is exported for further use.) 12.2 Details of analysis_allele_age.nf 12.2.1 Setup The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 11.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .set{ vcf_ch } As the data is going to be split by linkage group, we create a channel for the individual LGs. // git 11.2 // initialize LGs Channel .from( (&#39;01&#39;..&#39;09&#39;) + (&#39;10&#39;..&#39;19&#39;) + (&#39;20&#39;..&#39;24&#39;) ) .map{&quot;LG&quot; + it} .combine( vcf_ch ) .set{ lg_ch } Several things are happening in the next step: First the genotypes are split by LG and a new info field is added to the vcf file to store information about the ancestral state of each SNP. The second step checks for each SNP if it is invariant across all Serranid outgroup samples (in that case the outgroup allele is considered ancestral). Then, allele frequencies are computed as a fallback clue - if a SNP is variant in the outgroup, the major allele is then set as ancestral allele. Lastly, the ancestral state information is added to the genotype file. // git 11.3 // subset the genotypes by LG // and add ancestral allele annotation process prepare_vcf { label &quot;L_20g2h_prepare_vcf&quot; input: set val( lg ), val( vcfidx ), file( vcf ) from lg_ch output: set val( lg ), file( &quot;${lg}_integer.vcf&quot; ) into ( vcf_prep_ch ) script: &quot;&quot;&quot; # subset by LG and add AC info field vcftools \\ --gzvcf ${vcf[0]} \\ --chr ${lg} \\ --recode \\ --stdout | \\ sed &#39;s/\\\\(##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=&quot;Phased Genotype&quot;&gt;\\\\)/\\\\1\\\\n##INFO=&lt;ID=AC,Number=A,Type=Integer,Description=&quot;Allele count in genotypes, for each ALT allele, in the same order as listed&quot;&gt;/&#39; | \\ bgzip &gt; ${lg}.vcf.gz # determine ancestral state based on invariant sites in outgoup zcat ${lg}.vcf.gz | \\ grep -v &quot;^#&quot; | \\ awk -v OFS=&quot;\\\\t&quot; \\ &#39;BEGIN{print &quot;#CHROM&quot;,&quot;FROM&quot;,&quot;TO&quot;,&quot;AA&quot;} {o1=substr(\\$107, 1, 1); o2=substr(\\$107, 3, 3); o3=substr(\\$167, 1, 1); o4=substr(\\$167, 3, 3); o5=substr(\\$179, 1, 1); o6=substr(\\$179, 3, 3); if (o1 == o2 &amp;&amp; o3 == o4 &amp;&amp; o5 == o6 &amp;&amp; o1 == o3 &amp;&amp; o1 == o5){ aa = \\$(4+o1)} else {aa = &quot;.&quot;}; print \\$1,\\$2,\\$2,aa}&#39; &gt; ${lg}_annotations.bed # determine allele frquencies vcftools \\ --gzvcf ${lg}.vcf.gz \\ --freq \\ --stdout | \\ sed &#39;s/{ALLELE:FREQ}/ALLELE1\\\\tALLELE2/&#39; &gt; ${lg}_allele_counts.tsv # determine ancestral state for variant sites in outgoup based on allele freq Rscript --vanilla \\$BASE_DIR/R/major_allele.R ${lg}_allele_counts.tsv ${lg}_annotations.bed bgzip ${lg}_annotations_maj.bed tabix -s 1 -b 2 -e 3 ${lg}_annotations_maj.bed.gz # add ancestral state annotation zcat ${lg}.vcf.gz | \\ vcf-annotate -a ${lg}_annotations_maj.bed.gz \\ -d key=INFO,ID=AA,Number=1,Type=String,Description=&#39;Ancestral Allele&#39; \\ -c CHROM,FROM,TO,INFO/AA | \\ sed &#39;s/LG//g&#39; \\ &gt; ${lg}_integer.vcf &quot;&quot;&quot; } Based on the ancestral state information, the genotypes are recoded such that the ancestral allele is set as the new reference allele of the vcf file. // git 11.4 // re-write ancestral state in vcf process set_ancestral_states { label &#39;L_2g15m_ancestral_states&#39; publishDir &quot;../../1_genotyping/5_ancestral_allele&quot;, mode: &#39;copy&#39; input: set val( lg ), file( vcf ) from ( vcf_prep_ch ) output: set val( lg ), file( &quot;${lg}_aa.vcf.gz&quot; ) into ( vcf_aa_ch ) script: &quot;&quot;&quot; java -jar \\$SFTWR/jvarkit/dist/vcffilterjdk.jar \\ -f \\$BASE_DIR/js/script.js ${vcf} | \\ bgzip &gt; ${lg}_aa.vcf.gz &quot;&quot;&quot; } After this, the outgroup samples are removed from the data and the remaining data set is filtered to remove sites that are invariant within the hamlets. // git 11.5 // filter vcf to remove invariant sites in hamlets process create_positions { label &#39;L_20g2h_create_positions&#39; publishDir &quot;../../2_analysis/sliding_phylo/&quot;, mode: &#39;copy&#39; input: set val( lg ), file( vcf ) from ( vcf_aa_ch ) output: set val( lg ), file( &quot;${lg}_aa_h_variant.vcf.gz&quot; ), file( &quot;${lg}_positions.txt&quot; ) into ( positions_ch ) script: &quot;&quot;&quot; echo -e &quot;20478tabhon\\\\n28393torpan\\\\ns_tort_3torpan&quot; &gt; outgr.pop # keeping only sites that are variant within hamlets vcftools \\ --gzvcf ${vcf} \\ --remove outgr.pop \\ --recode \\ --stdout | \\ vcftools \\ --gzvcf - \\ --mac 1 \\ --recode \\ --stdout | \\ bgzip &gt; ${lg}_aa_no_outgroup.vcf.gz zcat ${lg}_aa_no_outgroup.vcf.gz | \\ grep -v &quot;^#&quot; | \\ cut -f 1,2 | \\ head -n -1 &gt; ${lg}_positions_prep.txt vcftools \\ --gzvcf ${vcf} \\ --positions ${lg}_positions_prep.txt \\ --recode \\ --stdout | \\ bgzip &gt; ${lg}_aa_h_variant.vcf.gz cut -f 2 ${lg}_positions_prep.txt &gt; ${lg}_positions.txt &quot;&quot;&quot; } Since a single GEVA run can take quite some time, the data is split further into chunks of 25k SNPs // git 11.5 // prepare the age estimation by splitting the vcf // (all in one takes too long...) process pre_split { label &#39;L_2g2h_pre_split&#39; publishDir &quot;../../2_analysis/geva/&quot;, mode: &#39;copy&#39; input: set val( lg ), file( vcf ), file( pos ) from ( positions_ch ) output: set val( lg ), file( &quot;pre_positions/pre_*&quot; ), file( &quot;*.bin&quot; ), file( &quot;*.marker.txt&quot; ), file( &quot;*.sample.txt&quot; ) into ( geva_setup_ch ) set val( lg ), file( &quot;inner_pos.txt&quot; ), file( vcf ) into ( ccf_vcf_ch ) script: &quot;&quot;&quot; mkdir -p pre_positions head -n -1 ${pos} | \\ tail -n +2 &gt; inner_pos.txt split inner_pos.txt -a 4 -l 25000 -d pre_positions/pre_ r=\\$(awk -v k=${lg} &#39;\\$1 == k {print \\$4}&#39; \\$BASE_DIR/ressources/avg_rho_by_LG.tsv) geva_v1beta \\ --vcf ${vcf} --rec \\$r --out ${lg} &quot;&quot;&quot; } Then GEVA is run on those genotype subsets to actually estimate the allele ages. // git 11.6 // run geva on vcf subsets process run_geva { label &#39;L_30g15h6x_run_geva&#39; input: set val( lg ), file( pos ), file( bin ), file( marker ), file( sample ) from geva_setup_ch.transpose() output: set val( lg ), file( &quot;*.sites.txt.gz&quot; ), file( &quot;*.pairs.txt.gz&quot; ) into ( output_split_ch ) script: &quot;&quot;&quot; pref=\\$(echo &quot;${pos}&quot; | sed &#39;s=^.*/A==; s=pre_positions/pre_==&#39;) mkdir -p sub_positions sub_results split ${pos} -a 4 -l 250 -d sub_positions/sub_pos_\\${pref}_ r=\\$(awk -v k=${lg} &#39;\\$1 == k {print \\$4}&#39; \\$BASE_DIR/ressources/avg_rho_by_LG.tsv) for sp in \\$(ls sub_positions/sub_pos_\\${pref}_*); do run_id=\\$(echo \\$sp | sed &quot;s=sub_positions/sub_pos_\\${pref}_==&quot;) geva_v1beta \\ -t 6 \\ -i ${bin} \\ -o sub_results/${lg}_\\${pref}_\\${run_id}\\ --positions \\$sp \\ --Ne 30000 \\ --mut 3.7e-08 \\ --hmm \\$SFTWR/geva/hmm/hmm_initial_probs.txt \\$SFTWR/geva/hmm/hmm_emission_probs.txt tail -n +2 sub_results/${lg}_\\${pref}_\\${run_id}.sites.txt &gt;&gt; ${lg}_\\${pref}.sites.txt tail -n +2 sub_results/${lg}_\\${pref}_\\${run_id}.pairs.txt &gt;&gt; ${lg}_\\${pref}.pairs.txt done gzip ${lg}_\\${pref}.sites.txt gzip ${lg}_\\${pref}.pairs.txt &quot;&quot;&quot; } And finally, the results of the separate chunks are gathered and compiled into a single output file. // git 11.7 // collect results by lg process collect_by_lg { label &#39;L_2g2h_collect&#39; publishDir &quot;../../2_analysis/geva/&quot;, mode: &#39;copy&#39; input: set val( lg ), file( sites ), file( pairs ) from output_split_ch.groupTuple() output: set val( lg ), file( &quot;*.sites.txt.gz&quot; ), file( &quot;*.pairs.txt.gz&quot; ) into ( output_lg_ch ) script: &quot;&quot;&quot; echo &quot;MarkerID Clock Filtered N_Concordant N_Discordant PostMean PostMode PostMedian&quot; &gt; ${lg}.sites.txt echo &quot;MarkerID Clock SampleID0 Chr0 SampleID1 Chr1 Shared Pass SegmentLHS SegmentRHS Shape Rate&quot; &gt; ${lg}.pairs.txt zcat ${sites} &gt;&gt; ${lg}.sites.txt zcat ${pairs} &gt;&gt; ${lg}.pairs.txt gzip ${lg}.sites.txt gzip ${lg}.pairs.txt &quot;&quot;&quot; } "],
["git-12-analysis-x-fst-permutation-test.html", "13 (git 12) Analysis X (FST Permutation Test) 13.1 Summary 13.2 Details of analysis_fst_sign.nf", " 13 (git 12) Analysis X (FST Permutation Test) This pipeline can be executed as follows: cd $BASE_DIR/nf/12_analysis_fst_signif source ../sh/nextflow_alias.sh nf_run_fstsig 13.1 Summary \\(F_{ST}\\) significance is assesed within the nextflow script analysis_fst_sign.nf (located under $BASE_DIR/nf/12_analysis_fst_signif/). Below is an overview of the steps involved in the analysis. (The green dots indicates the input files, red dots depict output that is exported for further use.) 13.2 Details of analysis_fst_sign.nf 13.2.1 Setup The nextflow script starts by opening the genotype data. #!/usr/bin/env nextflow // git 12.1 // open genotype data Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_locations; vcf_adapt } As population pairs of different locations are going to be tested independently, we prepare a location channel. // git 12.2 // prepare location channel Channel .from( &quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) .set{ locations_ch } The permutation test is going to be run once for the full genotype set and once for a subset excluding the highly differentiated regions of the genome. For this, we create a channel to toggle between the subset types. // git 12.3 // prepare subset modes (whole genome vs non-diverged regions) Channel .from( &quot;whg&quot;, &quot;subset_non_diverged&quot;) .into{ subset_type_ch; subset_type_ch2 } We also load a reference table with the genomic coordinates of the highly differentiated regions. // git 12.4 // load table with differentiation outlier regions Channel .fromPath( &quot;../../2_analysis/summaries/fst_outliers_998.tsv&quot; ) .into{ outlier_tab; outlier_tab2 } Then we combine the information about the focal location with the genotype data and the outlier file. // git 12.5 // attach genotypes to location locations_ch .combine( vcf_locations ) .combine( outlier_tab ) .set{ vcf_location_combo } At this point, the genotypes are subset by location. // git 12.6 // subset vcf by location process subset_vcf_by_location { label &quot;L_20g2h_subset_vcf&quot; input: set val( loc ), vcfId, file( vcf ), file( outlier_tab ) from vcf_location_combo output: set val( loc ), file( &quot;${loc}.vcf.gz&quot; ), file( &quot;${loc}.vcf.gz.tbi&quot; ), file( &quot;${loc}.pop&quot; ), file( outlier_tab ) into ( vcf_loc_pair1, vcf_loc_pair2, vcf_loc_pair3 ) script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${loc} | \\ grep -v tor | \\ grep -v tab &gt; ${loc}.pop vcftools --gzvcf ${vcf[0]} \\ --keep ${loc}.pop \\ --mac 3 \\ --recode \\ --stdout | bgzip &gt; ${loc}.vcf.gz tabix ${loc}.vcf.gz &quot;&quot;&quot; } Now, for each location, we respectively list the specific set… // git 12.7 // define location specific sepcies set Channel.from( [[1, &quot;ind&quot;], [2, &quot;may&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;uni&quot;]] ).into{ bel_spec1_ch; bel_spec2_ch } Channel.from( [[1, &quot;abe&quot;], [2, &quot;gum&quot;], [3, &quot;nig&quot;], [4, &quot;pue&quot;], [5, &quot;ran&quot;], [6, &quot;uni&quot;]] ).into{ hon_spec1_ch; hon_spec2_ch } Channel.from( [[1, &quot;nig&quot;], [2, &quot;pue&quot;], [3, &quot;uni&quot;]] ).into{ pan_spec1_ch; pan_spec2_ch } …and create all possible species pairs. // git 12.8 // prepare pairwise fsts // ------------------------------ /* (create all possible species pairs depending on location and combine with genotype subset (for the respective location))*/ // ------------------------------ /* channel content after joinig: set [0:val(loc), 1:file(vcf), 2:file( vcfidx ), 3:file(pop), 4:file( outlier_tab ), 5:val(spec1), 6:val(spec2)]*/ // ------------------------------ bel_pairs_ch = Channel.from( &quot;bel&quot; ) .join( vcf_loc_pair1 ) .combine(bel_spec1_ch) .combine(bel_spec2_ch) .filter{ it[5] &lt; it[7] } .map{ it[0,1,2,3,4,6,8]} hon_pairs_ch = Channel.from( &quot;hon&quot; ) .join( vcf_loc_pair2 ) .combine(hon_spec1_ch) .combine(hon_spec2_ch) .filter{ it[5] &lt; it[7] } .map{ it[0,1,2,3,4,6,8]} pan_pairs_ch = Channel.from( &quot;pan&quot; ) .join( vcf_loc_pair3 ) .combine(pan_spec1_ch) .combine(pan_spec2_ch) .filter{ it[5] &lt; it[7] } .map{ it[0,1,2,3,4,6,8]} bel_pairs_ch.concat( hon_pairs_ch, pan_pairs_ch ).set { all_fst_pairs_ch } Before the actual permutations, a little preparation is necessary: first, the differentiated regions are excluded from the data if needed then, population assignment files are created (pop1.txt and pop2.txt) next, differentiation for the actual population assignment are computed the resulting genome wide FST is used to start populating a results table *_random_fst_a00.tsv finally files that are not needed anymore are cleaned up and the list of involved samples is exported // git 12.9 // run fst on actual populations process fst_run { label &#39;L_32g1h_fst_run&#39; input: set val( loc ), file( vcf ), file( vcfidx ), file( pop ), file( outlier_tab ), val( spec1 ), val( spec2 ), val( subset_type ) from all_fst_pairs_ch.combine( subset_type_ch ) output: set val( &quot;${spec1}${loc}-${spec2}${loc}_${subset_type}&quot; ), file( &quot;*_random_fst_a00.tsv&quot; ) into rand_header_ch set val( &quot;${spec1}${loc}-${spec2}${loc}_${subset_type}&quot; ), val( loc ), val( spec1 ), val( spec2 ), file( &quot;${loc}.${subset_type}.vcf.gz&quot; ), file( &quot;col1.pop&quot; ), file( &quot;prep.pop&quot; ) into rand_body_ch script: &quot;&quot;&quot; if [ &quot;${subset_type}&quot; == &quot;subset_non_diverged&quot; ];then awk -v OFS=&quot;\\\\t&quot; &#39;{print \\$2,\\$3,\\$4}&#39; ${outlier_tab} &gt; diverged_regions.bed SUBSET=&quot;--exclude-bed diverged_regions.bed&quot; else SUBSET=&quot;&quot; fi vcftools --gzvcf ${vcf[0]} \\ \\$SUBSET \\ --recode \\ --stdout | bgzip &gt; ${loc}.${subset_type}.vcf.gz tabix ${loc}.${subset_type}.vcf.gz echo -e &quot;0000\\treal_pop&quot; &gt; idx.txt vcfsamplenames ${loc}.${subset_type}.vcf.gz | \\ awk &#39;{print \\$1&quot;\\\\t&quot;substr(\\$1, length(\\$1)-5, length(\\$1))}&#39; &gt; prep.pop grep ${spec1} ${pop} &gt; pop1.txt grep ${spec2} ${pop} &gt; pop2.txt vcftools --gzvcf ${loc}.${subset_type}.vcf.gz \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --stdout 2&gt; fst.log 1&gt; tmp.txt grep &quot;^Weir&quot; fst.log | sed &#39;s/.* //&#39; | paste - - &gt; fst.tsv echo -e &quot;idx\\\\ttype\\\\tmean_fst\\\\tweighted_fst&quot; &gt; ${spec1}${loc}-${spec2}${loc}_${subset_type}_random_fst_a00.tsv paste idx.txt fst.tsv &gt;&gt; ${spec1}${loc}-${spec2}${loc}_${subset_type}_random_fst_a00.tsv rm fst.tsv fst.log pop1.txt pop2.txt tmp.txt idx.txt awk &#39;{print \\$1}&#39; prep.pop &gt; col1.pop &quot;&quot;&quot; } We are going to perform 10,000 permutations for each population pair. Theses are going to be split over 100 batches of 100 permutations each. For this we now create a channel with the indices for those batches ranging from 00 - 99. // git 12.10 // create indexes for permutation itteration Channel .from( (&#39;0&#39;..&#39;9&#39;)) .into{ singles_ch; tens_ch } singles_ch .combine(tens_ch) .map{ it[0]+it[1] } .toSortedList() .flatten() .into{ sub_pre_ch; sub_pre_ch2 } Then within each batch, 100 permutations of the population assignments are conducted, the genome wide FST is computed and the results are exported. // git 12.11 // for each itteration run fst on 100 // permutations of population assignment process random_bodies { label &#39;L_32g6h_fst_run&#39; input: set val( run ), val( loc ), val( spec1 ), val( spec2 ), file( vcf ), file( col1 ), file( prepop ), val( pre ) from rand_body_ch.combine(sub_pre_ch) output: set val( run ), file(&quot;*_random_fst_b${pre}.tsv&quot;) into rand_body_out_ch script: &quot;&quot;&quot; for k in {00..99}; do echo &quot;Iteration_&quot;\\$k echo -e &quot;${pre}\\$k\\trandom&quot; &gt; idx.txt awk &#39;{print \\$2}&#39; ${prepop} | shuf &gt; col2.pop # premutation happens here paste ${col1} col2.pop &gt; rand.pop grep &quot;${spec1}${loc}\\$&quot; rand.pop &gt; r_pop1.pop grep &quot;${spec2}${loc}\\$&quot; rand.pop &gt; r_pop2.pop vcftools --gzvcf ${vcf} \\ --weir-fst-pop r_pop1.pop \\ --weir-fst-pop r_pop2.pop \\ --stdout 2&gt; fst.log 1&gt; tmp.txt grep &quot;^Weir&quot; fst.log | sed &#39;s/.* //&#39; | paste - - &gt; fst.tsv paste idx.txt fst.tsv &gt;&gt; ${run}_random_fst_b${pre}.tsv rm fst.tsv fst.log rand.pop col2.pop r_pop1.pop r_pop2.pop tmp.txt done &quot;&quot;&quot; } Finally, the results of all batches are collected and a single output file is compiled per species pair. // git 12.12 // collect all itterations and compile // output for each population pair process compile_random_results { label &#39;L_20g2h_compile_rand&#39; publishDir &quot;../../2_analysis/fst_signif/random&quot;, mode: &#39;copy&#39; input: set val( run ), file( body ), file( head ) from rand_body_out_ch.groupTuple().join(rand_header_ch, remainder: true) output: file(&quot;${run}_random_fst.tsv.gz&quot;) into random_lists_result script: &quot;&quot;&quot; cat ${head} &gt; ${run}_random_fst.tsv cat ${body} &gt;&gt; ${run}_random_fst.tsv gzip ${run}_random_fst.tsv &quot;&quot;&quot; } The same general approach is used for the permutation test for the allopatric populations of the H. nigricans, H. puella and H. unicolor. First, a channel for the three species is created. // ----------------------------------------- // repeat the same procedure for adaptation // (permuting location within species) // git 12.13 // prepare species channel Channel .from( &quot;nig&quot;, &quot;pue&quot;, &quot;uni&quot;) .set{ species_ch } Then two instances of a location channel are created… // git 12.14 // define location set Channel.from( [[1, &quot;bel&quot;], [2, &quot;hon&quot;], [3, &quot;pan&quot;]]).into{ locations_ch_1;locations_ch_2 } …to produce every possible location combination. // git 12.15 // create location pairs locations_ch_1 .combine(locations_ch_2) .filter{ it[0] &lt; it[2] } .map{ it[1,3]} .combine( species_ch ) .combine( vcf_adapt ) .combine( outlier_tab2 ) .combine( subset_type_ch2 ) .set{ vcf_location_combo_adapt } Then, as a preparation of the permutation, genome wide FST for the actual sample configuration is computed (s. git 12.9). // git 12.16 // collapsed analog to git 12.6 &amp; 9 // subset vcf by species and // run fst on actual populations process fst_run_adapt { label &#39;L_32g1h_fst_run&#39; input: set val( loc1 ), val( loc2 ), val( spec ), val( vcf_indx) , file( vcf ), file( outlier_tab ), val( subset_type ) from vcf_location_combo_adapt output: set val( &quot;${spec}${loc1}-${spec}${loc2}_${subset_type}&quot; ), file( &quot;*_random_fst_a00.tsv&quot; ) into rand_header_adapt_ch set val( &quot;${spec}${loc1}-${spec}${loc2}_${subset_type}&quot; ), val( spec ), val( loc1 ), val( loc2 ), file( &quot;${spec}.${subset_type}.vcf.gz&quot; ), file( &quot;col1.pop&quot; ), file( &quot;prep.pop&quot; ) into rand_body_adapt_ch script: &quot;&quot;&quot; vcfsamplenames ${vcf[0]} | \\ grep ${spec} &gt; ${spec}.pop if [ &quot;${subset_type}&quot; == &quot;subset_non_diverged&quot; ];then awk -v OFS=&quot;\\\\t&quot; &#39;{print \\$2,\\$3,\\$4}&#39; ${outlier_tab} &gt; diverged_regions.bed SUBSET=&quot;--exclude-bed diverged_regions.bed&quot; else SUBSET=&quot;&quot; fi vcftools --gzvcf ${vcf[0]} \\ \\$SUBSET \\ --keep ${spec}.pop \\ --mac 3 \\ --recode \\ --stdout | bgzip &gt; ${spec}.${subset_type}.vcf.gz tabix ${spec}.${subset_type}.vcf.gz echo -e &quot;0000\\treal_pop&quot; &gt; idx.txt vcfsamplenames ${spec}.${subset_type}.vcf.gz | \\ awk &#39;{print \\$1&quot;\\\\t&quot;substr(\\$1, length(\\$1)-5, length(\\$1))}&#39; &gt; prep.pop grep ${loc1} ${spec}.pop &gt; pop1.txt grep ${loc2} ${spec}.pop &gt; pop2.txt vcftools --gzvcf ${spec}.${subset_type}.vcf.gz \\ --weir-fst-pop pop1.txt \\ --weir-fst-pop pop2.txt \\ --stdout 2&gt; fst.log 1&gt; tmp.txt grep &quot;^Weir&quot; fst.log | sed &#39;s/.* //&#39; | paste - - &gt; fst.tsv echo -e &quot;idx\\\\ttype\\\\tmean_fst\\\\tweighted_fst&quot; &gt; ${spec}${loc1}-${spec}${loc2}_${subset_type}_random_fst_a00.tsv paste idx.txt fst.tsv &gt;&gt; ${spec}${loc1}-${spec}${loc2}_${subset_type}_random_fst_a00.tsv rm fst.tsv fst.log pop1.txt pop2.txt tmp.txt idx.txt awk &#39;{print \\$1}&#39; prep.pop &gt; col1.pop &quot;&quot;&quot; } Now, 100 batches of 100 permutations each are run for every location pair (s. git 12.11). // git 12.17 // for each itteration run fst on 100 // permutations of location assignment process random_bodies_adapt { label &#39;L_32g6h_fst_run&#39; input: set val( run ), val( spec ), val( loc1 ), val( loc2 ), file( vcf ), file( col1 ), file( prepop ), val( pre ) from rand_body_adapt_ch.combine(sub_pre_ch2) output: set val( run ), file(&quot;*_random_fst_b${pre}.tsv&quot;) into rand_body_out_adapt_ch script: &quot;&quot;&quot; for k in {00..99}; do echo &quot;Iteration_&quot;\\$k echo -e &quot;${pre}\\$k\\trandom&quot; &gt; idx.txt awk &#39;{print \\$2}&#39; ${prepop} | shuf &gt; col2.pop # premutation happens here paste ${col1} col2.pop &gt; rand.pop grep &quot;${spec}${loc1}\\$&quot; rand.pop &gt; r_pop1.pop grep &quot;${spec}${loc2}\\$&quot; rand.pop &gt; r_pop2.pop vcftools --gzvcf ${vcf} \\ --weir-fst-pop r_pop1.pop \\ --weir-fst-pop r_pop2.pop \\ --stdout 2&gt; fst.log 1&gt; tmp.txt grep &quot;^Weir&quot; fst.log | sed &#39;s/.* //&#39; | paste - - &gt; fst.tsv paste idx.txt fst.tsv &gt;&gt; ${run}_random_fst_b${pre}.tsv rm fst.tsv fst.log rand.pop col2.pop r_pop1.pop r_pop2.pop tmp.txt done &quot;&quot;&quot; } Finally, the results are collected and a single output file is compiled. // git 12.18 // collect all itterations and compile // output for each location pair process compile_random_results_adapt { label &#39;L_20g2h_compile_rand&#39; publishDir &quot;../../2_analysis/fst_signif/random/adapt&quot;, mode: &#39;copy&#39; input: set val( run ), file( body ), file( head ) from rand_body_out_adapt_ch.groupTuple().join(rand_header_adapt_ch, remainder: true) output: file(&quot;${run}_random_fst.tsv.gz&quot;) into random_lists_adapt_result script: &quot;&quot;&quot; cat ${head} &gt; ${run}_random_fst.tsv cat ${body} &gt;&gt; ${run}_random_fst.tsv gzip ${run}_random_fst.tsv &quot;&quot;&quot; } "],
["git-13-analysis-xi-whole-genome-phylogenies.html", "14 (git 13) Analysis XI (Whole Genome Phylogenies) 14.1 Summary 14.2 Details of analysis_phylo_whg.nf", " 14 (git 13) Analysis XI (Whole Genome Phylogenies) This pipeline can be executed as follows: cd $BASE_DIR/nf/13_analysis_phylo_whg nextflow run analysis_phylo_whg.nf 14.1 Summary The whole genome phylogenies can be reconstructed within the nextflow script analysis_phylo_whg.nf (located under $BASE_DIR/nf/analysis_phylo_whg/). 14.2 Details of analysis_phylo_whg.nf This part of the analysis was actually manged manually and not via nextflow. We still report the analysis as a .nf script as we believe this is a cleaner and more concise report of the conducted analysis. 14.2.1 Setup The nextflow script starts by opening the genotype data and feeding it into two different streams. #!/usr/bin/env nextflow // ----------------------- DISCLAIMER ---------------------- // this pipeline was not actually run using nexflow, // but managed manually // --------------------------------------------------------- // Hamlet phylogeny // ---------------- // git 13.1 // open the SNP data set Channel .fromFilePairs(&quot;../../1_genotyping/4_phased/phased_mac2.vcf.{gz,gz.tbi}&quot;) .into{ vcf_hypo_whg_ch; vcf_serr_whg_ch } Next, also a file containing the sample IDs excluding the samples identified as hybrids in git 9 is loaded. // RAxML analysis, Serranus-rooted // ------------------------------- // git 13.2 // open the sample-list (excluding hybrid samples) Channel .fromPath(&quot;../../ressources/samples_hybrids.txt&quot;) .set{ hybrids_file } As a preparation for running raxml, the genotype file is subset to exclude the hybrids. Then, heterozygous sites are masked and the data is filtered for minimal allele count and physical distance thresholds. Finally, the genotypes a indirectly converted to fasta format. // git 13.3 // subset data and convert to fasta for raxml process serr_whg_genotypes { input: set vcfId, file( vcf ), file( hybrids ) from vcf_serr_whg_ch.combine( hybrids_file ) output: file( &quot;hyS_n_0.33_mac4_5kb.fas&quot; ) into raxml_serr_genotypes_ch script: &quot;&quot;&quot; # Remove hybrids from genotype data (SNPs only) vcftools \\ --gzvcf ${vcf[0]} \\ --remove ${hybrids} \\ --recode \\ --stdout | \\ gzip &gt; hyS.vcf.gz # Mask heterozygous genotypes as unknown zcat &lt; hyS.vcf.gz | \\ sed -e s/&quot;1|0&quot;/&quot;.|.&quot;/g -e s/&quot;0|1&quot;/&quot;.|.&quot;/g | \\ gzip &gt; hyS_n.vcf.gz # Apply missingness, allele count and distance filters vcftools \\ --gzvcf hyS_n.vcf.gz \\ --max-missing 0.33 \\ --mac 4 \\ --thin 5000 \\ --recode \\ --out hyS_n_0.33_mac4_5kb # Convert to fasta format (Perl script available at https://github.com/JinfengChen/vcf-tab-to-fasta) wget https://raw.githubusercontent.com/JinfengChen/vcf-tab-to-fasta/master/vcf_tab_to_fasta_alignment.pl vcf-to-tab &lt; hyS_n_0.33_mac4_5kb.vcf &gt; hyS_n_0.33_mac4_5kb.tab perl ~/apps/vcf-tab-to-fasta/vcf_tab_to_fasta_alignment.pl -i hyS_n_0.33_mac4_5kb.tab &gt; hyS_n_0.33_mac4_5kb.fas &quot;&quot;&quot; } Then, raxml can be run on the fasta formated genotypes. // git 13.4 // run raxml (Serranus-rooted) process serr_whg_raxml { publishDir &quot;../../2_analysis/raxml/&quot;, mode: &#39;copy&#39; input: file( fas ) from raxml_serr_genotypes_ch output: file( &quot;hyS_n_0.33_mac4_5kb.raxml.support&quot; ) into raxml_serr_whg_ch script: &quot;&quot;&quot; # Reconstruct phylogeny # Note: number of invariant sites for Felsenstein correction was calculated as number of # variant sites in alignment (109,660) / genome-wide proportion of variant sites # (0.05) * genome-wide proportion of invariant sites (0.95) raxml-NG --all \\ --msa hyS_n_0.33_mac4_5kb.fas \\ --model GTR+G+ASC_FELS{2083540} \\ --tree pars{20},rand{20} \\ --bs-trees 100 \\ --threads 24 \\ --worker 4 \\ --seed 123 \\ --prefix hyS_n_0.33_mac4_5kb &quot;&quot;&quot; } The same general aproach is used for the phylogeny excluding the Serranus outgroup samples. For this, a different sample list (also excluding the outgroup samples) is loaded. // RAxML analysis, floridae-rooted // ------------------------------- // git 13.5 // open the sample-list (excluding hybrid and Serranus samples) Channel .fromPath(&quot;../../ressources/samples_155.txt&quot;) .set{ hamlet_file } Like in git 13.3, the genotypes are subset and converted to fasta format. // git 13.6 // subset data and convert to fasta for raxml process hypo_whg_genotypes { input: set vcfId, file( vcf ), file( hamlets ) from vcf_hypo_whg_ch.combine(hamlet_file) output: file( &quot;hyp155_n_0.33_mac4_5kb.fas&quot; ) into raxml_hypo_genotypes_ch script: &quot;&quot;&quot; # Remove hybrid and Serranus samples from genotype data (SNPs only) vcftools \\ --gzvcf ${vcf[0]} \\ --remove ${hamlets} \\ --recode \\ --stdout | \\ gzip &gt; hyp155.vcf.gz # Mask heterozygous genotypes as unknown zcat &lt; hyp155.vcf.gz | \\ sed -e s/&quot;1|0&quot;/&quot;.|.&quot;/g -e s/&quot;0|1&quot;/&quot;.|.&quot;/g | \\ gzip &gt; hyp155_n.vcf.gz # Apply missingness, allele count and distance filters vcftools \\ --gzvcf hyp155_n.vcf.gz \\ --max-missing 0.33 \\ --mac 4 \\ --thin 5000 \\ --recode \\ --out hyp155_n_0.33_mac4_5kb # Convert to fasta format (Perl script available at https://github.com/JinfengChen/vcf-tab-to-fasta) wget https://raw.githubusercontent.com/JinfengChen/vcf-tab-to-fasta/master/vcf_tab_to_fasta_alignment.pl vcf-to-tab &lt; hyp155_n_0.33_mac4_5kb.vcf &gt; hyp155_n_0.33_mac4_5kb.tab perl ./vcf_tab_to_fasta_alignment.pl -i hyp155_n_0.33_mac4_5kb.tab &gt; hyp155_n_0.33_mac4_5kb.fas &quot;&quot;&quot; } Finally, again raxml is run (equivalent to git 13.4). // git 13.7 // run raxml (floridae-rooted) process hypo_whg_raxml { publishDir &quot;../../2_analysis/raxml/&quot;, mode: &#39;copy&#39; input: file( fas ) from raxml_hypo_genotypes_ch output: file( &quot;hyp155_n_0.33_mac4_5kb.raxml.support&quot; ) into raxml_hypo_whg_ch script: &quot;&quot;&quot; # Infer phylogeny # Note: number of invariant sites for Felsenstein correction was calculated as number of # variant sites in alignment (105,043) / genome-wide proportion of variant sites # (0.05) * genome-wide proportion of invariant sites (0.95) raxml-NG --all \\ --msa ${fas} \\ --model GTR+G+ASC_FELS{1995817} \\ --tree pars{20},rand{20} \\ --bs-trees 100 \\ --threads 24 \\ --worker 8 \\ --seed 123 \\ --prefix hyp155_n_0.33_mac4_5kb &quot;&quot;&quot; } "],
["git-14-analysis-xii-outlier-region-phylogenies.html", "15 (git 14) Analysis XII (Outlier Region Phylogenies) 15.1 Summary 15.2 Details of analysis_phylo_regions.nf", " 15 (git 14) Analysis XII (Outlier Region Phylogenies) This pipeline can be executed as follows: cd $BASE_DIR/nf/14_analysis_phylo_regions nextflow run analysis_phylo_regions.nf 15.1 Summary The phylogenies specific to particular differentiation outlier regions are reconstructed within the nextflow script analysis_phylo_regions.nf (located under $BASE_DIR/nf/14_analysis_phylo_regions/). This includes both the sample-level as well as the population-level phylogenies. 15.2 Details of analysis_phylo_regions.nf This part of the analysis was actually manged manually and not via nextflow. We still report the analysis as a .nf script as we believe this is a cleaner and more concise report of the conducted analysis. 15.2.1 Setup The nextflow script starts by opening the two specific linkage groups of the all_bp genotype data set and binding it to differentiation outlier IDs as well as to a reference table containing the genomic coordinates of all differentiation outlier regions. #!/usr/bin/env nextflow // ----------------------- DISCLAIMER ---------------------- // this pipeline was not actually run using nexflow, // but managed manually // --------------------------------------------------------- // Region-specific phylogenies // --------------------------- // git 14.1 // bundle allBP files and outlier table Channel .fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/byLG/filterd.allBP.LG04.vcf.{gz,gz.tbi}&quot;) .concat(Channel.fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/byLG/filterd.allBP.LG12.vcf.{gz,gz.tbi}&quot;)) .concat(Channel.fromFilePairs(&quot;../../1_genotyping/3_gatk_filtered/byLG/filterd.allBP.LG12.vcf.{gz,gz.tbi}&quot;)) .merge(Channel.from(&quot;LG04_1&quot;, &quot;LG12_3&quot;, &quot;LG12_4&quot;)) .combine(Channel.fromPath(&quot;../../ressources/focal_outlier.tsv&quot;)) .set{ vcf_lg_ch } Then, two different sample lists (both excluding hybrid samples, one with and one without Serranus outgroup samples) are loaded and bound to a sample mode identifier. // git 14.2 // toggle sample modes (with / without Serranus outgroup) Channel.fromPath(&quot;../../ressources/samples_155.txt&quot;) .concat(Channel.fromPath(&quot;../../ressources/samples_hybrids.txt&quot;)) .merge(Channel.from(&quot;155&quot;, &quot;hyS&quot;)) .set{ sample_mode_ch } Next, for each outlier region, the genotype data is subset to the respective outlier region. // git 14.3 // subset genotypes to outlier region process extract_regions { input: set val( vcfIdx ), file( vcf ), val( outlierId ), file( outlier_file ), file( sample_file ), val( sample_mode ) from vcf_lg_ch.combine( sample_mode_ch ) output: set file( &quot;*_${sample_mode}.vcf&quot; ), val( outlierId ), val( sample_mode ) into ( vcf_raxml_ch, vcf_pomo_ch ) script: &quot;&quot;&quot; # Extract regions of interest from genotype data (allBP), # remove hybrid / Serranus samples and indels; simplify headers head -n 1 ${outlier_file} | cut -f 1-3 &gt; outlier.bed grep ${outlierId} ${outlier_file} | cut -f 1-3 &gt;&gt; outlier.bed OUT_ALT=\\$(echo ${outlierId} | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39; | sed &#39;s/_/./&#39;) vcftools --gzvcf \\ ${vcf[0]} \\ --bed outlier.bed \\ --remove-indels \\ --remove ${sample_file} \\ --recode \\ --stdout | \\ grep -v &#39;##&#39; &gt; \\${OUT_ALT}_${sample_mode}.vcf &quot;&quot;&quot; } Then, for the population-level phylogenies, the genotypes are first converted to fasta format and then to a allele frequency format (.cf). At that point, iqtree2 is run to create the population-level phylogenies. // git 14.4 // run iqtree under pomo model process run_pomo { publishDir &quot;../../2_analysis/revPoMo/outlier_regions/&quot;, mode: &#39;copy&#39; input: set file( vcf ), val( outlierId ), val( sample_mode ) from vcf_raxml_ch output: file( &quot;*_pop.cf.treefile&quot; ) into pomo_results_ch script: &quot;&quot;&quot; OUT_ALT=\\$(echo ${outlierId} | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39; | sed &#39;s/_/./&#39;) # Convert to fasta format (Python scripts available at https://github.com/simonhmartin/genomics_general), picked up from 6.1.1 output python \\$SFTWR/genomics_general/VCF_processing/parseVCF.py -i ${vcf} &gt; \\${OUT_ALT}_${sample_mode}.geno python \\$SFTWR/genomics_general/genoToSeq.py \\ -g \\${OUT_ALT}_${sample_mode}.geno \\ -s \\${OUT_ALT}_${sample_mode}.fas \\ -f fasta \\ --splitPhased # Reformat sample ids to provide population prefixes for cflib sed -e &#39;s/-/_/g&#39; -e &#39;s/&gt;\\(.*\\)\\([a-z]\\{6\\}\\)_\\([AB]\\)/&gt;\\2-\\1_\\3/g&#39; \\${OUT_ALT}_${sample_mode}.fas &gt; \\${OUT_ALT}_${sample_mode}_p.fas # Convert to allele frequency format (cflib library available at https://github.com/pomo-dev/cflib) \\$SFTWR/cflib/FastaToCounts.py \\${OUT_ALT}_${sample_mode}_p.fas \\${OUT_ALT}_${sample_mode}_pop.cf # IQTREE analysis under PoMo model iqtree2 \\ -nt 16 \\ -s \\${OUT_ALT}_${sample_mode}_pop.cf \\ -m HKY+F+P+N9+G4 \\ -b 100 &quot;&quot;&quot; } For the sample-level phylogenies, the genotypes are also converted to fasta format. // git 14.5 // convert genotypes to fasta for raxml process conversion_raxml { input: set file( vcf ), val( outlierId ), val( sample_mode ) from vcf_pomo_ch output: set val( outlierId ), val( sample_mode ), file( &quot;*N.fas&quot; ) into outlier_regions_ch script: &quot;&quot;&quot; OUT_ALT=\\$(echo ${outlierId} | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39; | sed &#39;s/_/./&#39;) # Replace unknown character states and asterisks (deletions as encoded by GATK) with &quot;N&quot; vcf-to-tab &lt; ${vcf} | sed -e &#39;s/\\\\.\\\\/\\\\./N\\\\/N/g&#39; -e &#39;s/[ACGTN\\\\*]\\\\/\\\\*/N\\\\/N/g&#39; &gt; \\${OUT_ALT}_${sample_mode}N.tab # Convert to fasta format (Perl script available at https://github.com/JinfengChen/vcf-tab-to-fasta) wget https://raw.githubusercontent.com/JinfengChen/vcf-tab-to-fasta/master/vcf_tab_to_fasta_alignment.pl perl ~/apps/vcf-tab-to-fasta/vcf_tab_to_fasta_alignment.pl -i \\${OUT_ALT}_${sample_mode}N.tab &gt; \\${OUT_ALT}_${sample_mode}N.fas &quot;&quot;&quot; } Then, raxml is run directly on the fasta files. // git 14.6 // run raxml process run_raxml { publishDir &quot;../../2_analysis/raxml/&quot;, mode: &#39;copy&#39; input: set val( outlierId ), val( sample_mode ), file( fas ) from outlier_regions_ch output: file( &quot;*.raxml.support&quot; ) into outlier_results_ch script: &quot;&quot;&quot; OUT_ALT=\\$(echo ${outlierId} | tr &#39;[:upper:]&#39; &#39;[:lower:]&#39; | sed &#39;s/_/./&#39;) # Reconstruct phylogenies raxml-NG --all \\ --msa ${fas} \\ --model GTR+G \\ --tree pars{10},rand{10} \\ --bs-trees 100 \\ --threads 24 \\ --worker 8 \\ --seed 123 \\ --prefix \\${OUT_ALT}_${sample_mode}N &quot;&quot;&quot; } "],
["git-15-data-visualization.html", "16 (git 15) Data Visualization", " 16 (git 15) Data Visualization After all nextflow pipelines are successfully run to completion, each Figure (and Suppl. Figure) of the manuscript can be re-created with its respective R script located under R/fig. These are executable R scripts that can be launched from the base directory; Rscript --vanilla R/fig/plot_Fxyz.R input1 input2 ... For convenience, there also exists a bash script that can be used to re-create all Figures in one go (git 15): cd $BASE_DIR bash sh/create_figures.sh After running create_figures.sh, Figures 1 - 6 and Suppl. Figures 1 - 16 should be created withing the folder figures/. In the remaining documentation, the individual Visualization scripts are going to discussed in detail. Below is the bash code that is executed when running create_figures.sh: #/usr/bin/bash # git 15 # Main Figures Rscript --vanilla R/fig/plot_F1.R \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/summaries/fst_permutation_summary.tsv Rscript --vanilla R/fig/plot_F2.R \\ 2_analysis/msmc/output/ \\ 2_analysis/cross_coalescence/output/ \\ 2_analysis/msmc/setup/msmc_grouping.txt \\ 2_analysis/msmc/setup/msmc_cc_grouping.txt \\ 2_analysis/summaries/fst_globals.txt Rscript --vanilla R/fig/plot_F3.R \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_globals.txt Rscript --vanilla R/fig/plot_F4.R 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/multi_fst.50k.tsv.gz 2_analysis/GxP/50000/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz \\ 2_analysis/summaries/fst_globals.txt Rscript --vanilla R/fig/plot_F5.R \\ 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/summaries/fst_outliers_998.tsv 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/GxP/50000/ 200 5 2_analysis/revPoMo/outlier_regions/ Rscript --vanilla R/fig/plot_F6.R \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/geva/ 2_analysis/GxP/bySNP/ # Suppl. Figures Rscript --vanilla R/fig/plot_SF1.R \\ 2_analysis/pca/ Rscript --vanilla R/fig/plot_SF2.R \\ 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_globals.txt Rscript --vanilla R/fig/plot_SF3.R \\ 2_analysis/dxy/50k/ Rscript --vanilla R/fig/plot_SF4.R \\ 2_analysis/newhyb/nh_input/NH.Results/ Rscript --vanilla R/fig/plot_SF5.R 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/summaries/fst_globals.txt Rscript --vanilla R/fig/plot_SF6.R \\ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/fst/50k/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz Rscript --vanilla R/fig/plot_SF7.R \\ 2_analysis/dxy/50k/ Rscript --vanilla R/fig/plot_SF8.R \\ 2_analysis/pi/50k/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz Rscript --vanilla R/fig/plot_SF9.R \\ 2_analysis/raxml/hyp155_n_0.33_mac4_5kb.raxml.support.bs-tbe \\ 2_analysis/raxml/RAxML_bipartitions.hypS-h_n_0.33_mac6_10kb Rscript --vanilla R/fig/plot_SF10.R \\ 2_analysis/pi/50k/ Rscript --vanilla R/fig/plot_SF11.R \\ 2_analysis/raxml/lg04.1_155N.raxml.support \\ 2_analysis/raxml/lg12.3_155N.raxml.support \\ 2_analysis/raxml/lg12.4_155N.raxml.support Rscript --vanilla R/fig/plot_SF12.R \\ 2_analysis/raxml/lg04.1_hySN.raxml.support \\ 2_analysis/raxml/lg12.3_hySN.raxml.support \\ 2_analysis/raxml/lg12.4_hySN.raxml.support Rscript --vanilla R/fig/plot_SF13.R \\ 2_analysis/admixture/ \\ metadata/phenotypes.sc Rscript --vanilla R/fig/plot_SF14.R \\ 2_analysis/GxP/50000/ Rscript --vanilla R/fig/plot_SF15.R \\ 2_analysis/fst_signif/random/ Rscript --vanilla R/fig/plot_SF16.R \\ 2_analysis/raxml/hyS_n_0.33_mac4_5kb.raxml.support # ================== "],
["figure-1.html", "17 Figure 1 17.1 Summary 17.2 Details of plot_F1.R", " 17 Figure 1 17.1 Summary This is the accessory documentation of Figure 1. Note that this script depends on results that are created within the script R/figs/plot_SF15.R (specifically 2_analysis/summaries/fst_permutation_summary.ts) and thus needs to be run after this script has finished. The Figure can be recreated by running the R script plot_F1.R from a (bash terminal): cd $BASE_DIR Rscript --vanilla R/fig/plot_F1.R 2_analysis/dxy/50k/ 2_analysis/fst/50k/ 17.2 Details of plot_F1.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, BAMMtools and on the package hypoimg. 17.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute interactively or debug the script: #!/usr/bin/env Rscript # # Context: this script depends on the input file 2_analysis/summaries/fst_permutation_summary.tsv # which is created by R/figs/plot_SF15.R # # run from terminal: # Rscript --vanilla R/fig/plot_F1.R \\ # 2_analysis/fst/50k/ 2_analysis/summaries/fst_globals.txt 2_analysis/summaries/fst_permutation_summary.tsv # =============================================================== # This script produces Figure 1 of the study &quot;Ancestral variation, hybridization and modularity # fuel a marine radiation&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/fst/50k/&#39;, &#39;2_analysis/summaries/fst_globals.txt&#39;, &#39;2_analysis/summaries/fst_permutation_summary.tsv&#39;) # script_name &lt;- &quot;R/fig/plot_F1.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(patchwork) library(ape) library(ggraph) library(tidygraph) library(stringr) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F1.R ────────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/fst/50k/ #&gt; ★ 2: 2_analysis/summaries/fst_globals.txt #&gt; ★ 3: 2_analysis/summaries/fst_permutation_summary.tsv #&gt; ─────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we set a few parameters for the plot layout: # config ----------------------- fst_dir &lt;- as.character(args[1]) fst_globals &lt;- as.character(args[2]) fst_permutation_file &lt;- as.character(args[3]) wdh &lt;- .3 # The width of the boxplots scaler &lt;- 20 # the ratio of the Fst and the dxy axis (legacy - not really needed anymore) clr_sec &lt;- &#39;gray&#39; # the color of the secondary axis (dxy) 17.2.2 Fish Tree Of Life Subtree The Figure 1 is assembled from five panels (a-e) which are prepared independently and combined at the end of the R script. The first panel (a) contains ar re-drawing of a of subset of the data from (Rabosky et al. 2019) and also uses builds upon the R code from the script scripts/main figures/Figure3.R of their Dryad repository. The code of (Rabosky et al. 2019) relies on the packages {BAMMtools} and {geiger} which are loaded additionally to those loaded during the start-up (this is to keep the modified code as close to the original as possible). # start script ------------------- # === fig 1. panel a: modified plugin from ressources/Rabosky_etal_2018/scripts/main figures/Figure3.R ============= library(BAMMtools) library(geiger) To be able to handle the tree-plot within a ggplot framework, we also need the package {ggplotify} to turn the plot into a grid object. library(ggplotify) Then, the path to the downloaded Dryad repository is specified and the original helper script PlottingFunctions.R from (Rabosky et al. 2019) is sourced. basepath &lt;- &#39;ressources/Rabosky_etal_2018/&#39; source(paste0(basepath, &quot;scripts/supporting_fxns/PlottingFunctions.R&quot;)) Next, the paths of Fish Tree Of Life (FToL) data are specified and the tree is loaded and prepared according to the original script by (Rabosky et al. 2019). eventdata_vr &lt;- paste0(basepath, &quot;dataFiles/bamm_results/12k_tv1/event_data_thinned.csv&quot;) eventdata_cr &lt;- paste0(basepath, &quot;dataFiles/bamm_results/12k_tc1/event_data_thinned.csv&quot;) treefile &lt;- paste0(basepath, &quot;dataFiles/bamm_results/12k_tv1/bigfish_no_outgroup.tre&quot;) fspdata &lt;- paste0(basepath, &quot;dataFiles/rate_lat_stats_by_sp_fixed0.5.csv&quot;) anadromous &lt;- FALSE vx &lt;- read.tree(treefile) # node rotations for plotting: rset &lt;- c(15447, 15708:15719) for (i in 1:length(rset)){ vx &lt;- rotate(vx, node = rset[i]) } vx &lt;- read.tree(text = write.tree(vx)) spdata &lt;- read.csv(fspdata, stringsAsFactors = F) rownames(spdata) &lt;- spdata$sp # --------------------------- latvals &lt;- abs(spdata$lat_centroid) names(latvals) &lt;- spdata$sp inboth &lt;- intersect(spdata$sp, vx$tip.label) # 1 species is in tree, but was dropped because is synonym to another species in tree according to fishbase (Gadus_ogac matches to Gadus_macrocephalus) inboth &lt;- intersect(inboth, spdata$sp[which(!is.na(spdata$tv.lambda))]) latvals &lt;- latvals[inboth] edvr &lt;- getEventData(vx, eventdata_vr, burnin=0) At this point we diverge from the original script and define our own subset of interest, which covers all available species of the Serraninae subfamily. serranids &lt;- c(&quot;Hypoplectrus_gemma&quot;, &quot;Hypoplectrus_unicolor&quot;, &quot;Hypoplectrus_gummigutta&quot;, &quot;Hypoplectrus_chlorurus&quot;, &quot;Hypoplectrus_aberrans&quot;, &quot;Hypoplectrus_nigricans&quot;, &quot;Hypoplectrus_guttavarius&quot;, &quot;Hypoplectrus_indigo&quot;, &quot;Hypoplectrus_puella&quot;, &quot;Serranus_tortugarum&quot;, &quot;Serranus_tabacarius&quot;, &quot;Schultzea_beta&quot;, &quot;Diplectrum_formosum&quot;, &quot;Diplectrum_bivittatum&quot;, &quot;Diplectrum_pacificum&quot;, &quot;Diplectrum_maximum&quot;, &quot;Serranus_notospilus&quot;, &quot;Serranus_phoebe&quot;, &quot;Serranus_psittacinus&quot;, &quot;Serranus_baldwini&quot;, &quot;Serranus_tigrinus&quot;, &quot;Paralabrax_albomaculatus&quot;, &quot;Paralabrax_dewegeri&quot;, &quot;Paralabrax_callaensis&quot;, &quot;Paralabrax_loro&quot;, &quot;Paralabrax_auroguttatus&quot;, &quot;Paralabrax_clathratus&quot;, &quot;Paralabrax_humeralis&quot;, &quot;Paralabrax_nebulifer&quot;, &quot;Paralabrax_maculatofasciatus&quot;, &quot;Zalanthias_kelloggi&quot;, &quot;Serranus_cabrilla&quot;, &quot;Serranus_atricauda&quot;, &quot;Serranus_scriba&quot;, &quot;Serranus_hepatus&quot;, &quot;Serranus_accraensis&quot;, &quot;Centropristis_striata&quot;, &quot;Chelidoperca_occipitalis&quot;, &quot;Chelidoperca_investigatoris&quot;, &quot;Chelidoperca_pleurospilus&quot;) Then, the data is subset based on the selection. edvr_serr &lt;- edvr %&gt;% subtreeBAMM(tips = serranids) Next, the species labels are specified and formatted (for those species that have would have an ambiguous family abbreviation based on the first letter). label_two_chars &lt;- c(`italic(S.~&#39;beta&#39;)` = &quot;italic(Sc.~&#39;beta&#39;)&quot;, `italic(S.~notospilus)` = &quot;italic(Se.~notospilus)&quot;, `italic(S.~phoebe)` = &quot;italic(Se.~phoebe)&quot;, `italic(S.~psittacinus)` = &quot;italic(Se.~psittacinus)&quot;, `italic(S.~baldwini)` = &quot;italic(Se.~baldwini)&quot;, `italic(S.~tigrinus)` = &quot;italic(Se.~tigrinus)&quot;, `italic(S.~cabrilla)` = &quot;italic(Se.~cabrilla)&quot;, `italic(S.~atricauda)` = &quot;italic(Se.~atricauda)&quot;, `italic(S.~scriba)` = &quot;italic(Se.~scriba)&quot;, `italic(S.~hepatus)` = &quot;italic(Se.~hepatus)&quot;, `italic(S.~accraensis)` = &quot;italic(Se.~accraensis)&quot;, `italic(C.~striata)` = &quot;italic(Cp.~striata)&quot;, `italic(C.~occipitalis)` = &quot;italic(Ch.~occipitalis)&quot;, `italic(C.~investigatoris)` = &quot;italic(Ch.~investigatoris)&quot;, `italic(C.~pleurospilus)` = &quot;italic(Ch.~pleurospilus)&quot;, `italic(S.~tabacarius)` = &quot;italic(Se.~tabacarius)&quot;, `italic(S.~tortugarum)` = &quot;italic(Se.~tortugarum)&quot;) Then, the labels of the data set are actually re-formatted. edvr_serr_short &lt;- edvr_serr edvr_serr_short$tip.label &lt;- edvr_serr$tip.label %&gt;% str_replace(pattern = &quot;([A-Z])[a-z]*_([a-z]*)&quot;, &quot;italic(\\\\1.~\\\\2)&quot;)%&gt;% str_replace(pattern = &quot;beta&quot;, &quot;&#39;beta&#39;&quot;) %&gt;% ifelse(. %in% names(label_two_chars), label_two_chars[.], .) %&gt;% ggplot2:::parse_safe() Now, the color scheme for the tree coloration is defined. clr_tree &lt;- scico::scico(6, palette = &quot;berlin&quot;) %&gt;% prismatic::clr_desaturate(shift = .4) %&gt;% prismatic::clr_darken(shift = .2) At this stage, the basic tree is drawn using a modified version of BAMMtools::plot.bammdata() (which allows for different colors for the tip labels). p1 &lt;- as.grob(function(){ par(mar = c(0,0,0,0)) bammplot_k(x = edvr_serr_short, labels = T, lwd = .8, #legend = TRUE, cex = .3, pal = clr_tree, labelcolor = c(rep(&quot;black&quot;, 9), rep(&quot;darkgray&quot;, 31))) leg_shift_x &lt;- 0 lines(x = c(0,25) + leg_shift_x, y = c(1.5, 1.5), col = &quot;darkgray&quot;) text(x = 12.5 + leg_shift_x, y = .5, labels = &quot;25 MYR&quot;, cex = .4, col = &quot;darkgray&quot;) } ) Now that the basic tree is finished, we take care of the annotations starting with the color gradient that is going to highlight the Hypoplectrus family. c1 &lt;- &quot;transparent&quot; c2 &lt;- rgb(0, 0, 0, .1) c3 &lt;- rgb(0, 0, 0, .2) grad_mat &lt;- c(c1, c2, c3, c3) dim(grad_mat) &lt;- c(1, length(grad_mat)) grob_grad &lt;- rasterGrob(grad_mat, width = unit(1, &quot;npc&quot;), height = unit(1, &quot;npc&quot;), interpolate = TRUE) Then, we create a black hamlet to add to the annotation. blank_hamlet &lt;- hypoimg::hypo_outline %&gt;% ggplot()+ coord_equal()+ geom_polygon(aes(x, y), color = rgb(0, 0, 0, .5), fill = rgb(1, 1, 1, .3), size = .1)+ theme_void() At this point we have all pieces to assemble panel a of the figure. p_tree &lt;- ggplot() + geom_point(data = tibble(v = c(.056, 2.4)), x = .5, y = .5, aes(color = v), alpha = 0)+ scale_color_gradientn(colours = clr_tree, limits = c(.056, 2.4))+ annotation_custom(grob = grob_grad, ymin = 0.01, ymax = .2335, xmin = .4, xmax = .96)+ annotation_custom(grob = ggplotGrob(blank_hamlet), xmin = 0.55, xmax = .75, ymin = 0.015, ymax = .14)+ annotation_custom(grob = p1, xmin = -.16, xmax = 1.05, ymin = -.22, ymax = 1) + coord_cartesian(xlim = c(0, 1), ylim = c(0, 1), expand = 0)+ guides(color = guide_colorbar(title = &quot;Speciation Rate&quot;, title.position = &quot;top&quot;, direction = &quot;horizontal&quot;, barheight = unit(3, &quot;pt&quot;), barwidth = unit(57, &quot;pt&quot;), ticks.colour = &quot;white&quot;)) + theme_void()+ theme(legend.position = c(.01, .08), legend.justification = c(0, 0)) 17.2.3 Genetic differentiation boxplots For panel b, the data import starts with collecting the paths to all files containing the \\(F_{ST}\\) data (dir()), then iterating the import function over all files (map(summarize_fst)) and finally combining the outputs into a single tibble (bind_rows()). # ================================================================================================= # import Fst fst_files &lt;- dir(fst_dir, pattern = &#39;.50k.windowed.weir.fst.gz&#39;) fst_data &lt;- str_c(fst_dir, fst_files) %&gt;% purrr::map(summarize_fst) %&gt;% bind_rows() We use the genome wide average \\(F_{ST}\\) to rank the individual pair wise comparisons. # determine fst ranking fst_order &lt;- fst_data %&gt;% select(run, `mean_weighted-fst`) %&gt;% mutate(run = fct_reorder(run, `mean_weighted-fst`)) Then, we transform the \\(F_{ST}\\) data and do quite a bit of data wrangling to prepare the placement of the boxplots. (This is overly complicated by now, as this is mostly legacy code that was create for a previous version of the plot that combined re-scaled \\(F_{ST}\\) and \\(d_{XY}\\) boxplots.) fst_data_gather &lt;- fst_data %&gt;% gather(key = &#39;stat&#39;, value = &#39;val&#39;, -run) %&gt;% # sumstat contains the values needed to plot the boxplots (quartiles, etc) separate(stat, into = c(&#39;sumstat&#39;, &#39;popstat&#39;), sep = &#39;_&#39;) %&gt;% # duplicate dxy values scaled to fst range mutate(val_scaled = ifelse(popstat == &#39;dxy&#39;, val * scaler , val)) %&gt;% unite(temp, val, val_scaled) %&gt;% # separate th eoriginal values from the scales ons (scaled = secondary axis) spread(., key = &#39;sumstat&#39;, value = &#39;temp&#39;) %&gt;% separate(mean, into = c(&#39;mean&#39;,&#39;mean_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(median, into = c(&#39;median&#39;,&#39;median_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(sd, into = c(&#39;sd&#39;,&#39;sd_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(lower, into = c(&#39;lower&#39;,&#39;lower_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(upper, into = c(&#39;upper&#39;,&#39;upper_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(lowpoint, into = c(&#39;lowpoint&#39;,&#39;lowpoint_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(highpoint, into = c(&#39;highpoint&#39;,&#39;highpoint_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% # include &quot;dodge&quot;-positions for side-by-side plotting (secondary axis) mutate(loc = str_sub(run,4,6), run = factor(run, levels = levels(fst_order$run)), x = as.numeric(run) , x_dodge = ifelse(popstat == &#39;dxy&#39;, x + .25, x - .25), x_start_dodge = x_dodge - wdh/2, x_end_dodge = x_dodge + wdh/2, popstat_loc = str_c(popstat,&#39;[&#39;,loc,&#39;]&#39;)) Then we re-order the species pairs according to their average wheighted \\(F_{ST}\\). # sort run by average genome wide Fst run_ord &lt;- tibble(run = levels(fst_data_gather$run), run_ord = 1:length(levels(fst_data_gather$run))) As the last information that is needed for the \\(F_{ST}\\) boxplots, we load the results from the permutation tests (this is the part that depends on R/figs/plot_SF15.R). # load fst permutation results fst_sig_attach &lt;- read_tsv(fst_permutation_file) %&gt;% mutate(loc = str_sub(run, -3, -1)) %&gt;% group_by(loc) %&gt;% mutate(loc_n = 28,#length(loc), fdr_correction_factor = sum(1 / 1:length(loc)), fdr_alpha = .05 / fdr_correction_factor, is_sig = p_perm &gt; fdr_alpha) %&gt;% ungroup() At this point, the data is ready for the boxplots. But first, we are going to prepare the networks of pairwise comparisons that are used as annotation of that panel. For this we create a tibble of the species pairs with their respective rank. Then, we prepare a config table with one row per location, storing the parameters needed for the layout function for the networks. We need to define the location, the number of species at the location, the short three letter ID of those species and a weight parameter that is shifting the comparison label on the link within the networks. # create network annotation # underlying structure for the network plots networx &lt;- tibble( loc = c(&#39;bel&#39;,&#39;hon&#39;, &#39;pan&#39;), n = c(5, 6, 3), label = list(str_c(c(&#39;ind&#39;,&#39;may&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;),&#39;bel&#39;), str_c(c(&#39;abe&#39;,&#39;gum&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;ran&#39;,&#39;uni&#39;),&#39;hon&#39;), str_c(c(&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;),&#39;pan&#39;)), weight = c(1,1.45,1)) %&gt;% purrr::pmap_dfr(network_layout) %&gt;% mutate(edges = map(edges, function(x){x %&gt;% left_join(fst_data_gather %&gt;% filter(popstat == &quot;weighted-fst&quot;) %&gt;% select(run, median, mean)) })) Next, we create one network plot per location… # plot the individual networks by location plot_list &lt;- networx %&gt;% purrr::pmap(plot_network, node_lab_shift = .2) …and assemble them into a single grid object. # combine the networks into a single grob p_net &lt;- cowplot::plot_grid( plot_list[[1]] + theme(legend.position = &quot;none&quot;), plot_list[[2]] + theme(legend.position = &quot;none&quot;), plot_list[[3]] + theme(legend.position = &quot;none&quot;), ncol = 3) %&gt;% cowplot::as_grob() At this point we can create panel b. # assemble panel b p2 &lt;- fst_data_gather %&gt;% filter(popstat == &quot;weighted-fst&quot;) %&gt;% left_join(fst_sig_attach) %&gt;% mutate(loc = str_sub(run, -3, -1)) %&gt;% ggplot(aes(color = loc)) + annotation_custom(p_net, ymin = .15, xmax = 25)+ geom_segment(aes(x = x, xend = x, y = lowpoint, yend = highpoint), lwd = plot_lwd)+ geom_rect(aes(xmin = x - wdh, xmax = x + wdh, ymin = lower, ymax = upper), fill = &#39;white&#39;, size = plot_lwd)+ geom_segment(aes(x = x - wdh, xend = x + wdh, y = median, yend = median), lwd = plot_lwd)+ geom_point(aes(x = x, y = mean, shape = is_sig, fill = after_scale(color)), size = .8)+ scale_x_continuous(name = &quot;Pair of sympatric species&quot;, breaks = 1:28) + scale_y_continuous(name = expression(italic(F[ST])))+ scale_color_manual(values = c(make_faint_clr(&#39;bel&#39;), make_faint_clr(&#39;hon&#39;), make_faint_clr(&#39;pan&#39;))[c(2, 4, 6)])+ scale_shape_manual(values = c(`TRUE` = 1, `FALSE` = 21)) + coord_cartesian(xlim = c(0,29), expand = c(0,0))+ theme_minimal()+ theme(text = element_text(size = plot_text_size), legend.position = &#39;none&#39;, strip.placement = &#39;outside&#39;, strip.text = element_text(size = 12), panel.grid.major.x = element_blank(), panel.grid.minor.y = element_blank(), axis.text.y.right = element_text(color = clr_sec), axis.title.y.right = element_text(color = clr_sec)) 17.2.4 Principal Component Analyses Most of the plotting functions for the PCAs are provided in {GenomicOriginsScripts} package, so that the largest part of the work for the PCAs actually concerns the annotations. Fist, we set an alternative color scale and define the size of the hamlet images. clr_alt &lt;- clr clr_alt[[&quot;uni&quot;]] &lt;- &quot;lightgray&quot; pca_fish_scale &lt;- 1.15 The we create a configuration tibble that holds the desired positions of the hamlet annotations on the PCAs. pca_fish_pos &lt;- tibble(pop = GenomicOriginsScripts::pop_levels, short = str_sub(pop, 1, 3), loc = str_sub(pop, 4, 6), width = c(bel = .08, hon =.08, pan = .09)[loc] * pca_fish_scale, height = c(bel = .08, hon =.08, pan = .09)[loc] * pca_fish_scale) %&gt;% arrange(loc) %&gt;% mutate(x = c(-.18, -.01, .03, -.03, .075, -.04, -.2, -.02, -.01, -.02, -.01, -.15, 0, .05), y = c(.02, .27, -.13, -.03, .05, -.1, .05, 0, .075, -.23, .2, .06, -.2, .2)) %&gt;% select(-pop) %&gt;% group_by(loc) %&gt;% nest() Then we create the individual PCA plots using the function GenomicOriginsScripts::pca_plot(). pcas &lt;- c(&quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) %&gt;% map(pca_plot) 17.2.5 Legend and Final Figure To create the figure legend we define a set of hamlet species (all sequenced species except the outgroups) with their relative horizontal spacing. fish_tib &lt;- tibble(short = names(clr)[!names(clr) %in% c(&quot;flo&quot;, &quot;tab&quot;, &quot;tor&quot;)], x = c(0.5, 3.5, 7, 9.7, 12.25, 15.25, 18, 21.5)) Then, we set the color-patch size and create a custom ggplot that is going to function as legend. key_sz &lt;- .75 p_leg &lt;- fish_tib %&gt;% ggplot() + coord_equal(xlim = c(-.05, 24), expand = 0) + geom_tile(aes(x = x, y = 0, fill = short, color = after_scale(prismatic::clr_darken(fill, .25))), width = key_sz, height = key_sz, size = .3) + geom_text(aes(x = x + .6, y = 0, label = str_c(&quot;H. &quot;, sp_names[short])), hjust = 0, fontface = &quot;italic&quot;, size = plot_text_size / ggplot2:::.pt) + pmap(fish_tib, plot_fish_lwd, width = 1, height = 1, y = 0) + scale_fill_manual(values = clr, guide = FALSE) + theme_void() After this, first the fice panels are combined… p_combined &lt;- ((wrap_elements(plot = p_tree + theme(axis.title = element_blank(), text = element_text(size = plot_text_size)), clip = FALSE) + p2 ) / (pcas %&gt;% wrap_plots()) + plot_layout(heights = c(1,.75)) + plot_annotation(tag_levels = &#39;a&#39;) &amp; theme(text = element_text(size = plot_text_size), plot.background = element_rect(fill = &quot;transparent&quot;, color = &quot;transparent&quot;))) .. and then the legend is attached. p_done &lt;- cowplot::plot_grid(p_combined, p_leg, ncol = 1, rel_heights = c(1,.06)) Finally, we can export Figure 1. scl &lt;- .75 hypo_save(p_done, filename = &#39;figures/F1.pdf&#39;, width = 9 * scl, height = 6 * scl, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["figure-2.html", "18 Figure 2 18.1 Summary 18.2 Details of plot_F2.R", " 18 Figure 2 18.1 Summary This is the accessory documentation of Figure 2. The Figure can be recreated by running the R script plot_F2.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F2.R \\ 2_analysis/msmc/output/ \\ 2_analysis/cross_coalescence/output/ \\ 2_analysis/msmc/setup/msmc_grouping.txt \\ 2_analysis/msmc/setup/msmc_cc_grouping.txt \\ 2_analysis/summaries/fst_globals.txt 18.2 Details of plot_F2.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R packages GenomicOriginsScripts, as well as on the R packages hypoimg and patchwork. 18.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F2.R \\ # 2_analysis/msmc/output/ 2_analysis/cross_coalescence/output/ \\ # 2_analysis/msmc/setup/msmc_grouping.txt 2_analysis/msmc/setup/msmc_cc_grouping.txt \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Figure 2 of the study &quot;Ancestral variation, hybridization and modularity # fuel a marine radiation&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/msmc/output/&#39;, &#39;2_analysis/cross_coalescence/output/&#39;, # &#39;2_analysis/msmc/setup/msmc_grouping.txt&#39;, &#39;2_analysis/msmc/setup/msmc_cc_grouping.txt&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_F2.R&quot; # ---------------------------------------- The next section processes the input from the command line. It stores the arguments in the vector args. The R packages GenomicOriginsScripts, hypoimg and patchwork are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F2.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/msmc/output/ #&gt; ★ 2: 2_analysis/cross_coalescence/output/ #&gt; ★ 3: 2_analysis/msmc/setup/msmc_grouping.txt #&gt; ★ 4: 2_analysis/msmc/setup/msmc_cc_grouping.txt #&gt; ★ 5: 2_analysis/summaries/fst_globals.txt #&gt; ────────────────────────────────────────── /current/working/directory ── The directories for the demographic inference and the cross-coalescence data are received and stored in respective variables. Also, the files containing the groupings for demographic inference and cross-coalescence as well as the reference file for the genome wide \\(F_{ST}\\) values are received. # config ----------------------- msmc_path &lt;- as.character(args[1]) cc_path &lt;- as.character(args[2]) msmc_group_file &lt;- as.character(args[3]) cc_group_file &lt;- as.character(args[4]) fst_globals_file &lt;- as.character(args[5]) The msmc sample groupings are imported and the \\(F_{ST}\\) values loaded. # actual script ========================================================= msmc_groups &lt;- read_tsv(msmc_group_file) cc_groups &lt;- read_tsv(cc_group_file) fst_globals &lt;- vroom::vroom(fst_globals_file,delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run_prep&#39;,&#39;mean_fst&#39;,&#39;weighted_fst&#39;)) %&gt;% separate(run_prep,into = c(&#39;pop1&#39;,&#39;pop2&#39;),sep = &#39;-&#39;) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;,pop2,loc), run = fct_reorder(run,weighted_fst)) Next, the file names of all msmc results are collected. # locate cross-coalescence results msmc_files &lt;- dir(msmc_path, pattern = &#39;.final.txt.gz&#39;) cc_files &lt;- dir(cc_path, pattern = &#39;.final.txt.gz&#39;) Separately, all the demographic inference and cross-coalescence data are read in an compiled into two data sets. # import effective population size data msmc_data &lt;- msmc_files %&gt;% map_dfr(.f = get_msmc, msmc_path = msmc_path) # import cross-coalescence data cc_data &lt;- cc_files %&gt;% map_dfr(get_cc, cc_groups = cc_groups, cc_path = cc_path) %&gt;% mutate( run = factor(run, levels = levels(fst_globals$run))) The default color scheme is adjusted (to keep H. unicolor visible) and the tick color for the plots is defined. # color adjustments for line plots (replace white by gray) clr_alt &lt;- clr clr_alt[&#39;uni&#39;] &lt;- rgb(.8,.8,.8) clr_ticks &lt;- &#39;lightgray&#39; The first panel containing the demographic history is created. p_msmc &lt;- msmc_data %&gt;% # remove the two first and last time segments filter(!time_index %in% c(0:2,29:31)) %&gt;% ggplot( aes(x=YBP, y=Ne, group = run_nr, colour = spec)) + # add guides for the logarithmic axes annotation_logticks(sides=&quot;tl&quot;, color = clr_ticks, size = plot_lwd) + # add the msmc data as lines geom_line(size = .3)+ # set the color scheme scale_color_manual(NULL, values = clr_alt, label = sp_labs) + # format the x axis scale_x_log10(expand = c(0,0), breaks = c(10^3, 10^4, 10^5), position = &#39;top&#39;, labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x))) + # format the y axis scale_y_log10(labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)), breaks = c(10^3,10^4,10^5,10^6)) + # format the color legend guides(colour = guide_legend(title.position = &quot;top&quot;, override.aes = list(alpha = 1, size=1), nrow = 3, keywidth = unit(7, &quot;pt&quot;), byrow = TRUE)) + # set the axis titles labs(x = &quot;Generations Before Present&quot;, y = expression(Effective~Population~Size~(italic(N[e])))) + # set plot range coord_cartesian(xlim = c(250, 5*10^5)) + # tune plot appreance theme_minimal()+ theme(text = element_text(size = plot_text_size), axis.ticks = element_line(colour = clr_ticks), legend.position = c(1.05,-.175), legend.justification = c(1,0), legend.text.align = 0, panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(), title = element_text(face = &#39;bold&#39;), legend.spacing.y = unit(-5,&quot;pt&quot;), legend.spacing.x = unit(3, &quot;pt&quot;), axis.title = element_text(face = &#39;plain&#39;), legend.title = element_text(face = &#39;plain&#39;)) Then, the second panel containing the cross-coalescence plot is created. p_cc &lt;- cc_data %&gt;% # remove the two first and last time segments filter( !time_index %in% c(0:2,29:31)) %&gt;% arrange(run_nr) %&gt;% # attach fst data left_join(fst_globals %&gt;% select(run, weighted_fst)) %&gt;% ggplot(aes(x = YBP, y = Cross_coal, group = run_nr, color = weighted_fst)) + # add guides for the logarithmic axis annotation_logticks(sides=&quot;b&quot;, color = clr_ticks, size = plot_lwd) + # add the msmc data as lines geom_line(alpha = 0.2, size = .3)+ # set the color scheme scale_color_gradientn(name = expression(Global~weighted~italic(F[ST])), colours = hypogen::hypo_clr_LGs[1:24])+ # format the x axis scale_x_log10(expand = c(0,0), labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x))) + # set the axis titles guides(color = guide_colorbar(barheight = unit(3, &#39;pt&#39;), barwidth = unit(110, &#39;pt&#39;), title.position = &#39;top&#39; )) + # set the axis titles labs(x = &quot;Generations Before Present&quot;, y = &#39;Cross-coalescence Rate&#39;) + # set plot range coord_cartesian(xlim = c(250, 5*10^5)) + # tune plot appreance theme_minimal()+ theme(text = element_text(size = plot_text_size), axis.ticks = element_line(colour = clr_ticks), legend.position = c(1,.03), legend.direction = &#39;horizontal&#39;, legend.justification = c(1,0), panel.grid.minor.x = element_blank(), panel.grid.minor.y = element_blank(), title = element_text(face = &#39;bold&#39;), axis.title = element_text(face = &#39;plain&#39;), axis.title.x = element_blank(), axis.text.x = element_blank(), legend.title = element_text(face = &#39;plain&#39;)) Then, the figure is composed from both panels. # combine panels a and b p_done &lt;- p_msmc / p_cc + plot_annotation(tag_levels = c(&#39;a&#39;)) &amp; theme(legend.text = element_text(size = plot_text_size_small), legend.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = &quot;pt&quot;), panel.grid.major = element_line(size = plot_lwd), axis.ticks.x = element_blank(), panel.background = element_blank(), plot.background = element_blank()) Finally, we can export Figure 2. # export figure 2 hypo_save(plot = p_done, filename = &#39;figures/F2.pdf&#39;, width = f_width_half, height = f_width_half * .95, comment = plot_comment, bg = &quot;transparent&quot;, device = cairo_pdf) "],
["figure-3.html", "19 Figure 3 19.1 Summary 19.2 Details of plot_F3.R", " 19 Figure 3 19.1 Summary This is the accessory documentation of Figure 3. It should be possible to recreate the figure by running the R script plot_F3.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F3.R \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_globals.txt 19.2 Details of plot_F3.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R packages GenomicOriginsScripts and on the R packages hypoimg, hypogen, vroom and ggforce. 19.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: The next section processes the input from the command line. It stores the arguments in the vector args. The R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(ggforce) library(hypoimg) library(hypogen) library(vroom) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F3.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/fst/50k/ #&gt; ★ 2: 2_analysis/summaries/fst_globals.txt #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the sliding window \\(F_{ST}\\) data and the and the file with the genome wide average \\(F_{ST}\\) for all the species comparisons are received from the command line input. # config ----------------------- data_dir &lt;- as.character(args[1]) globals_file &lt;- as.character(args[2]) Then, the data folder is scanned for windowed \\(F_{ST}\\) data with an window size of 50 kb. # script ----------------------- # locate data files files &lt;- dir(path = data_dir, pattern = &#39;.50k.windowed.weir.fst.gz&#39;) Next, the genome wide average \\(F_{ST}\\) data for each population pair is loaded. # load genome wide average fst data globals &lt;- vroom::vroom(globals_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% mutate(run = str_c(loc,&#39;-&#39;,run) %&gt;% reformat_run_name() ) The package GenomicOriginsScripts contains the function get_fst_fixed to import \\(F_{ST}\\) data and compute the number, average length and cumulative length of regions exceeding a given \\(F_{ST}\\) threshold. Here, we prepare a table of a series of thresholds and all pair wise species comparisons as a configuration table for the following import with get_fst_fixed. # prepare data import settings within a data table (tibble) import_table &lt;- list(file = str_c(data_dir,files), fst_threshold = c(.5,.4,.3,.2,.1, .05,.02,.01)) %&gt;% cross_df() %&gt;% mutate( run = file %&gt;% str_remove(&#39;^.*/&#39;) %&gt;% str_sub(., 1, 11) %&gt;% reformat_run_name()) Then we define a data import function (this used to live in {GenomicOriginsScript}, but was moved here due to namspace issue). # import dxy data and compute threshold stats get_fst_fixed &lt;- function(file, run, fst_threshold,...){ data &lt;- hypogen::hypo_import_windows(file, ...) %&gt;% mutate(rank = rank(WEIGHTED_FST, ties.method = &quot;random&quot;))%&gt;% mutate(thresh = fst_threshold) %&gt;% mutate(outl = (WEIGHTED_FST &gt; thresh) %&gt;% as.numeric()) %&gt;% filter(outl == 1 ) if(nrow(data) == 0){ return(tibble(run = run, n = 0, avg_length = NA, med_length = NA, min_length = NA, max_length = NA, sd_length = NA, overal_length = NA, threshold_value = fst_threshold)) } else { data %&gt;% # next, we want to collapse overlapping windows group_by(CHROM) %&gt;% # we check for overlap and create &#39;region&#39; IDs mutate(check = 1-(lag(BIN_END,default = 0)&gt;BIN_START), ID = str_c(CHROM,&#39;_&#39;,cumsum(check))) %&gt;% ungroup() %&gt;% # then we collapse the regions by ID group_by(ID) %&gt;% summarise(run = run[1], run = run[1], treshold_value = thresh[1], CHROM = CHROM[1], BIN_START = min(BIN_START), BIN_END = max(BIN_END)) %&gt;% mutate(PEAK_SIZE = BIN_END-BIN_START) %&gt;% summarize(run = run[1], run = run[1], n = length(ID), avg_length = mean(PEAK_SIZE), med_length = median(PEAK_SIZE), min_length = min(PEAK_SIZE), max_length = max(PEAK_SIZE), sd_length = sd(PEAK_SIZE), overal_length = sum(PEAK_SIZE), threshold_value = treshold_value[1]) } } Using the configuration table, the \\(F_{ST}\\) data are loaded, and the threshold-specific stats are computed. # load data and compute statistics based on fixed fst treshold data &lt;- purrr::pmap_dfr(import_table, get_fst_fixed) %&gt;% left_join(globals) %&gt;% mutate(run = fct_reorder(run, weighted)) To simplify the figure, a subset of the original thresholds are selected and some columns are renamed for clean figure labels. # pre-format labels data2 &lt;- data %&gt;% select(threshold_value,weighted,n,avg_length,overal_length) %&gt;% mutate(avg_length = avg_length/1000, overal_length = overal_length/(10^6)) %&gt;% rename(`atop(Number~of,Regions)` = &#39;n&#39;, `atop(Average~Region,Length~(kb))` = &#39;avg_length&#39;, `atop(Cummulative~Region,Length~(Mb))` = &#39;overal_length&#39;) %&gt;% pivot_longer(names_to = &#39;variable&#39;,values_to = &#39;Value&#39;,3:5) %&gt;% mutate(threshold_value = str_c(&#39;italic(F[ST])~threshold:~&#39;, threshold_value), variable = factor(variable, levels = c(&#39;atop(Number~of,Regions)&#39;, &#39;atop(Average~Region,Length~(kb))&#39;, &#39;atop(Cummulative~Region,Length~(Mb))&#39;))) At this point we can create the figure. # set font size base_line_clr &lt;- &quot;black&quot; # compile plot p_done &lt;- data2 %&gt;% # select thresholds of interest filter(!(threshold_value %in% (c(0.02,.1,0.2, 0.3, .4) %&gt;% str_c(&quot;italic(F[ST])~threshold:~&quot;,.)))) %&gt;% ggplot(aes(x = weighted, y = Value#, fill = weighted ) )+ # add red line for genome extent in lowest row geom_hline(data = tibble(variable = factor(c(&#39;atop(Cummulative~Region,Length~(Mb))&#39;, &#39;atop(Average~Region,Length~(kb))&#39;, &#39;atop(Number~of,Regions)&#39;), levels = c(&#39;atop(Number~of,Regions)&#39;, &#39;atop(Average~Region,Length~(kb))&#39;, &#39;atop(Cummulative~Region,Length~(Mb))&#39;)), y = c(559649677/(10^6),NA,NA)), aes(yintercept = y), color = rgb(1,0,0,.25))+ # add data points geom_point(size = plot_size, color = plot_clr#, shape = 21 )+ # define plot stucture facet_grid(variable~threshold_value, scale=&#39;free&#39;, switch = &#39;y&#39;, labeller = label_parsed)+ # configure scales # scale_fill_gradientn(name = expression(weighted~italic(F[ST])), # colours = hypogen::hypo_clr_LGs[1:24] %&gt;% clr_lighten(factor = .3))+ scale_x_continuous(name = expression(Whole-genome~differentiation~(weighted~italic(F[ST]))), breaks = c(0,.05,.1), limits = c(-.00025,.10025), labels = c(&quot;0&quot;, &quot;0.05&quot;, &quot;0.1&quot;))+ # configure legend guides(fill = guide_colorbar(barwidth = unit(150, &quot;pt&quot;), label.position = &quot;top&quot;, barheight = unit(5,&quot;pt&quot;)))+ # tweak plot apperance theme_minimal()+ theme(axis.text = element_text(size = plot_text_size_small, color = rgb(.6,.6,.6)), axis.title.y = element_blank(), axis.text.x = element_text(vjust = .5, angle = 0), axis.title.x = element_text(vjust = -2), panel.background = element_rect(fill = rgb(.95,.95,.95,.5), color = rgb(.9,.9,.9,.5), size = .3), panel.grid.minor = element_blank(), panel.grid.major = element_line(size = plot_lwd), legend.position = &quot;bottom&quot;, strip.text = element_text(size = plot_text_size), legend.direction = &quot;horizontal&quot;, strip.placement = &#39;outside&#39;, axis.title = element_text(size = plot_text_size), legend.title = element_text(size = plot_text_size), strip.background.y = element_blank(), plot.background = element_blank()) Finally, we can export Figure 3. # export figure 3 hypo_save(filename = &#39;figures/F3.pdf&#39;, plot = p_done, width = .52 * f_width, height = .52 * f_width, device = cairo_pdf, comment = plot_comment, bg = &quot;transparent&quot;) "],
["figure-4.html", "20 Figure 4 20.1 Summary 20.2 Details of plot_F4.R", " 20 Figure 4 20.1 Summary This is the accessory documentation of Figure 4. The Figure can be recreated by running the R script plot_F4.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F4.R 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/multi_fst.50k.tsv.gz 2_analysis/GxP/50000/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz \\ 2_analysis/summaries/fst_globals.txt 20.2 Details of plot_F4.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts and on the package hypoimg and hypogen. 20.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F4.R 2_analysis/dxy/50k/ \\ # 2_analysis/fst/50k/multi_fst.50k.tsv.gz 2_analysis/GxP/50000/ \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ # 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ # 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Figure 4 of the study &quot;Ancestral variation, hybridization and modularity # fuel a marine radiation&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/dxy/50k/&#39;,&#39;2_analysis/fst/50k/multi_fst.50k.tsv.gz&#39;, # &#39;2_analysis/GxP/50000/&#39;, &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R&#39;, # &#39;2_analysis/twisst/weights/&#39;, &#39;ressources/plugin/trees/&#39;, # &#39;2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz&#39;, &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_F4.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F4.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ★ 2: 2_analysis/fst/50k/multi_fst.50k.tsv.gz #&gt; ★ 3: 2_analysis/GxP/50000/ #&gt; ★ 4: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 5: https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R #&gt; ★ 6: 2_analysis/twisst/weights/ #&gt; ★ 7: ressources/plugin/trees/ #&gt; ★ 8: 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz #&gt; ★ 9: 2_analysis/summaries/fst_globals.txt #&gt; ────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we source an external r script from the original twisst github repository that we need to import the twisst data: # config ----------------------- dxy_dir &lt;- as.character(args[1]) fst_file &lt;- as.character(args[2]) gxp_dir &lt;- as.character(args[3]) outlier_table &lt;- as.character(args[4]) twisst_script &lt;- as.character(args[5]) w_path &lt;- as.character(args[6]) d_path &lt;- as.character(args[7]) recombination_file &lt;- as.character(args[8]) global_fst_file &lt;- as.character(args[9]) source(twisst_script) 20.2.2 Data import Figure 4 contains quite a lot of different data sets. The main part of this script is just importing and organizing all of this data: In the following we’ll go step by step through the import of: differentiation data (\\(F_{ST}\\)) divergence data (\\(d_{XY}\\), also containing diversity data - \\(\\pi\\)) genotype \\(\\times\\) phenotype association data (\\(p_{Wald}\\)) recombination data (\\(\\rho\\)) topology weighting data We start with the import of the \\(F_{ST}\\) data, specifically the data set containing the genome wide \\(F_{ST}\\) computed for all populations simultaneously (joint \\(F_{ST}\\)). The data file is read, the columns are renamed and the genomic positions are added. Then, only the genomic positions and the \\(F_{ST}\\) columns are selected and a window column is added for faceting in ggplot(). # start script ------------------- # import fst data fst_data &lt;- vroom::vroom(fst_file, delim = &#39;\\t&#39;) %&gt;% select(CHROM, BIN_START, BIN_END, N_VARIANTS, WEIGHTED_FST) %&gt;% setNames(., nm = c(&#39;CHROM&#39;, &#39;BIN_START&#39;, &#39;BIN_END&#39;, &#39;n_snps&#39;, &#39;fst&#39;) ) %&gt;% add_gpos() %&gt;% select(GPOS, fst) %&gt;% setNames(., nm = c(&#39;GPOS&#39;,&#39;value&#39;)) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;a&#39;),&#39;):joint~italic(F[ST])&#39;)) Next, we import the \\(d_{XY}\\) data. Here we are importing all 28 pairwise comparisons, so we first collect all the file paths and the iterate the data import over all files. # locate dxy data files dxy_files &lt;- dir(dxy_dir) # import dxy data dxy_data &lt;- str_c(dxy_dir,dxy_files) %&gt;% purrr::map(get_dxy) %&gt;% bind_rows() %&gt;% select(N_SITES:GPOS, run) %&gt;% mutate(pop1 = str_sub(run,1,6), pop2 = str_sub(run,8,13)) From this data, we compute the divergence difference (\\(\\Delta d_{XY}\\)). # compute delta dxy dxy_summary &lt;- dxy_data %&gt;% group_by(GPOS) %&gt;% summarise(delta_dxy = max(dxy)-min(dxy), sd_dxy = sd(dxy), delt_pi = max(c(max(PI_POP1),max(PI_POP2))) - min(c(min(PI_POP1),min(PI_POP2)))) %&gt;% ungroup() %&gt;% setNames(., nm = c(&#39;GPOS&#39;, str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):\\u0394~italic(d[xy])&#39;), str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):italic(d[xy])~(sd)&#39;), str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):\\u0394~italic(\\u03C0)&#39;))) %&gt;% gather(key = &#39;window&#39;, value = &#39;value&#39;,2:4) %&gt;% filter(window == str_c(&#39;bold(&#39;,project_case(&#39;e&#39;),&#39;):\\u0394~italic(d[xy])&#39;)) Then we import the genotype \\(\\times\\) phenotype association data. For this, we list all the traits we want to include and then iterate the import function over all traits. We combine the data sets and transform the table to long format. # set G x P traits to be imported traits &lt;- c(&quot;Bars.lm.50k.5k.txt.gz&quot;, &quot;Peduncle.lm.50k.5k.txt.gz&quot;, &quot;Snout.lm.50k.5k.txt.gz&quot;) # set trait figure panels trait_panels &lt;- c(Bars = str_c(&#39;bold(&#39;,project_case(&#39;h&#39;),&#39;)&#39;), Peduncle = str_c(&#39;bold(&#39;,project_case(&#39;i&#39;),&#39;)&#39;), Snout = str_c(&#39;bold(&#39;,project_case(&#39;j&#39;),&#39;)&#39;)) # import G x P data gxp_data &lt;- str_c(gxp_dir,traits) %&gt;% purrr::map(get_gxp) %&gt;% join_list() %&gt;% gather(key = &#39;window&#39;, value = &#39;value&#39;,2:4) Then, we import the genome wide \\(F_{ST}\\) summary for all 28 pair wise comparisons to be able to pick a divergence data set of an intermediately differentiated species pair (the species pair of rank 15, close to 14.5 - the median of rank 1 to 28). # import genome wide Fst data summary -------- globals &lt;- vroom::vroom(global_fst_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% mutate(run = str_c(str_sub(run,1,3),loc,&#39;-&#39;,str_sub(run,5,7),loc), run = fct_reorder(run,weighted)) # dxy and pi are only shown for one exemplary population (/pair) # select dxy pair run (15 is one of the two central runs of the 28 pairs) # here, the 15th lowest fst value is identified as &quot;selector&quot; selectors_dxy &lt;- globals %&gt;% arrange(weighted) %&gt;% .$weighted %&gt;% .[15] # the dxy population pair corresponding to the selector is identified select_dxy_runs &lt;- globals %&gt;% filter(weighted %in% selectors_dxy) %&gt;% .$run %&gt;% as.character() # then thne dxy data is subset based on the selector dxy_select &lt;- dxy_data %&gt;% filter(run %in% select_dxy_runs) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;b&#39;),&#39;): italic(d[XY])&#39;)) The \\(d_{XY}\\) data set includes also \\(\\pi\\) of the involved populations. We first extract the diversity data for each population (pop1 &amp; pop2), combine them and compute the statistics needed for ranking the populations based on their diversity. # the pi data is filtered on a similar logic to the dxy data # first a table with the genome wide average pi for each population is compiled # (based on the first populations from the dxy data table # which contains pi for both populations) pi_summary_1 &lt;- dxy_data %&gt;% group_by(pop1,run) %&gt;% summarise(avg_pi = mean(PI_POP1)) %&gt;% ungroup() %&gt;% purrr::set_names(., nm = c(&#39;pop&#39;,&#39;run&#39;,&#39;avg_pi&#39;)) # the mean genome wide average pi is compiled for all the second populations # from the dxy data # then, the average of all the comparisons is computed for each population pi_summary &lt;- dxy_data %&gt;% group_by(pop2,run) %&gt;% summarise(avg_pi = mean(PI_POP2)) %&gt;% ungroup() %&gt;% purrr::set_names(., nm = c(&#39;pop&#39;,&#39;run&#39;,&#39;avg_pi&#39;)) %&gt;% bind_rows(pi_summary_1) %&gt;% group_by(pop) %&gt;% summarise(n = length(pop), mean_pi = mean(avg_pi), min_pi = min(avg_pi), max_pi = max(avg_pi), sd_pi = sd(avg_pi)) %&gt;% arrange(n) Then, we determine an intermediately diverse candidate of our 14 populations (rank 7, again: \\(7 \\approx median(1:14)\\)) and average over the diversities estimated in all pairwise comparisons this population was involved in. # one of the central populations with respect to average genome # wide pi is identified # for this, the 7th lowest pi value of the 14 populations is # determined as &quot;selector&quot; selectors_pi &lt;- pi_summary %&gt;% .$mean_pi %&gt;% sort() %&gt;% .[7] # the respective population is identified select_pi_pops &lt;- pi_summary %&gt;% filter(mean_pi %in% selectors_pi) %&gt;% .$pop %&gt;% as.character() # then the dxy data is subset by that population and the average pi over # all pair-wise runs is calculated for each window pi_data_select &lt;- dxy_data %&gt;% select(GPOS, PI_POP1, pop1 )%&gt;% purrr::set_names(., nm = c(&#39;GPOS&#39;,&#39;pi&#39;,&#39;pop&#39;)) %&gt;% bind_rows(.,dxy_data %&gt;% select(GPOS, PI_POP2, pop2 )%&gt;% purrr::set_names(., nm = c(&#39;GPOS&#39;,&#39;pi&#39;,&#39;pop&#39;))) %&gt;% group_by(GPOS,pop) %&gt;% summarise(n = length(pop), mean_pi = mean(pi), min_pi = min(pi), max_pi = max(pi), sd_pi = sd(pi)) %&gt;% filter(pop %in% select_pi_pops) %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;c&#39;),&#39;):~\\u03C0&#39;)) The import of the recombination data is pretty straight forward: Reading one file, adding genomic position and window column for faceting. # import recombination data recombination_data &lt;- vroom::vroom(recombination_file,delim = &#39;\\t&#39;) %&gt;% add_gpos() %&gt;% mutate(window = str_c(&#39;bold(&#39;,project_case(&#39;d&#39;),&#39;):~\\u03C1&#39;)) Then we import the topology weighting data. This is done once per location, the data sets are combined and specific columns are selected: The gnomic position, the topology number (format: three digits with leading zeros, hence “topo3”), relative topology rank ranging from 0 to 1, the faceting column and the actual weight data. We also create a dummy tibble that contains the null expectation of the topology weight for the two locations (1/n, with n = number of possible topologies - n = 15 for Belize and 105 for Honduras). # import topology weighting data twisst_data &lt;- tibble(loc = c(&#39;bel&#39;,&#39;hon&#39;), panel = c(&#39;f&#39;,&#39;g&#39;) %&gt;% project_case() %&gt;% str_c(&#39;bold(&#39;,.,&#39;)&#39;)) %&gt;% purrr::pmap(match_twisst_files) %&gt;% bind_rows() %&gt;% select(GPOS, topo3,topo_rel,window,weight) # the &quot;null-weighting&quot; is computed for both locations twisst_null &lt;- tibble(window = c(str_c(&#39;bold(&#39;, project_case(&#39;f&#39;),&#39;):~italic(w)[bel]&#39;), str_c(&#39;bold(&#39;, project_case(&#39;g&#39;),&#39;):~italic(w)[hon]&#39;)), weight = c(1/15, 1/105)) We create a single data set for \\(d_{XY}\\), \\(F_{ST}\\) and genotype \\(\\times\\) phenotype data. # combine data types -------- data &lt;- bind_rows(dxy_summary, fst_data, gxp_data) Then we load the positions of the the \\(F_{ST}\\) outlier windows, select the focal outliers that will receive individual labels and create a tibble and two parameters for the label placement within the plot. # import fst outliers outliers &lt;- vroom::vroom(outlier_table, delim = &#39;\\t&#39;) # the focal outlier IDs are set outlier_pick &lt;- c(&#39;LG04_1&#39;, &#39;LG12_3&#39;, &#39;LG12_4&#39;) # the table for the outlier labels is created outlier_label &lt;- outliers %&gt;% filter(gid %in% outlier_pick) %&gt;% mutate(label = letters[row_number()] %&gt;% project_inv_case(), x_shift_label = c(-1,-1.2,1)*10^7, gpos_label = gpos + x_shift_label, gpos_label2 = gpos_label - sign(x_shift_label) *.5*10^7, window = str_c(&#39;bold(&#39;,project_case(&#39;a&#39;),&#39;):joint~italic(F[ST])&#39;)) # the y height of the outlier labels and the corresponding tags is set outlier_y &lt;- .45 outlier_yend &lt;- .475 We prepare a set of trait annotations for the genotype \\(\\times\\) phenotype association panels. # the icons for the traits of the GxP are loaded trait_tibble &lt;- tibble(window = c(&quot;bold(h):italic(p)[Bars]&quot;, &quot;bold(i):italic(p)[Peduncle]&quot;, &quot;bold(j):italic(p)[Snout]&quot;), grob = hypo_trait_img$grob_circle[hypo_trait_img$trait %in% c(&#39;Bars&#39;, &#39;Peduncle&#39;, &#39;Snout&#39;)]) 20.2.3 Plotting Finally it is time to put the pieces together with one giant ggplot(): # finally, the figure is being put together p_done &lt;- ggplot()+ # add gray/white LGs background geom_hypo_LG()+ # the red highlights for the outlier regions are added geom_vline(data = outliers, aes(xintercept = gpos), color = outlr_clr)+ # the tags of the outlier labels are added geom_segment(data = outlier_label, aes(x = gpos, xend = gpos_label2, y = outlier_y, yend = outlier_yend), color = alpha(outlr_clr,1),size = .2)+ # the outlier labels are added geom_text(data = outlier_label, aes(x = gpos_label, y = outlier_yend, label = label), color = alpha(outlr_clr,1), fontface = &#39;bold&#39;)+ # the fst, delta dxy and gxp data is plotted geom_point(data = data, aes(x = GPOS, y = value),size = plot_size, color = plot_clr) + # the dxy data is plotted geom_point(data = dxy_select,aes(x = GPOS, y = dxy),size = plot_size, color = plot_clr)+ # the pi data is plotted geom_point(data = pi_data_select, aes(x = GPOS, y = mean_pi),size = plot_size, color = plot_clr) + # the roh data is plotted geom_point(data = recombination_data, aes(x = GPOS, y = RHO),size = plot_size, color = plot_clr) + # the smoothed rho is plotted geom_smooth(data = recombination_data, aes(x = GPOS, y = RHO, group = CHROM), color = &#39;red&#39;, se = FALSE, size = .7) + # the topology weighting data is plotted geom_line(data = twisst_data, aes(x = GPOS, y = weight, color = topo_rel), size = .4) + # the null weighting is added geom_hline(data = twisst_null, aes(yintercept = weight), color = rgb(1, 1, 1, .5), size = .4) + # the trait icons are added geom_hypo_grob(data = trait_tibble, aes(grob = grob, angle = 0, height = .65), inherit.aes = FALSE, x = .95, y = 0.65)+ # setting the scales scale_fill_hypo_LG_bg() + scale_x_hypo_LG()+ scale_color_gradient( low = &quot;#f0a830ff&quot;, high = &quot;#084082ff&quot;, guide = FALSE)+ # organizing the plot across panels facet_grid(window~.,scales = &#39;free&#39;,switch = &#39;y&#39;, labeller = label_parsed)+ # tweak plot appreance theme_hypo()+ theme(text = element_text(size = plot_text_size), legend.position = &#39;bottom&#39;, axis.title = element_blank(), strip.text = element_text(size = plot_text_size), strip.background = element_blank(), strip.placement = &#39;outside&#39;) The final figure is then exported using hypo_save(). # export figure 4 hypo_save(p_done, filename = &#39;figures/F4.png&#39;, width = f_width, height = f_width * .9, type = &quot;cairo&quot;, comment = plot_comment) "],
["figure-5.html", "21 Figure 5 21.1 Summary 21.2 Details of plot_F5.R", " 21 Figure 5 21.1 Summary This is the accessory documentation of Figure 5. The Figure can be recreated by running the R script plot_F5.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F5.R \\ 2_analysis/twisst/weights/ \\ ressources/plugin/trees/ \\ https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/dxy/50k/ \\ 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/GxP/50000/ \\ 200 \\ 5 \\ 2_analysis/revPoMo/outlier_regions/ 21.2 Details of plot_F5.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen, furrr, ggtext, ape, ggtree, patchwork, phangorn and igraph. 21.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F5.R \\ # 2_analysis/twisst/weights/ ressources/plugin/trees/ \\ # https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R \\ # 2_analysis/summaries/fst_outliers_998.tsv 2_analysis/dxy/50k/ \\ # 2_analysis/fst/50k/ 2_analysis/summaries/fst_globals.txt \\ # 2_analysis/GxP/50000/ 200 5 2_analysis/revPoMo/outlier_regions/ # =============================================================== # This script produces Figure 5 of the study &quot;Ancestral variation, hybridization and modularity # fuel a marine radiation&quot; by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/twisst/weights/&#39;, &#39;ressources/plugin/trees/&#39;, # &#39;https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R&#39;, # &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;2_analysis/dxy/50k/&#39;, &#39;2_analysis/fst/50k/&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;, # &#39;2_analysis/GxP/50000/&#39;, 200, 5, # &quot;2_analysis/revPoMo/outlier_regions/&quot;) # script_name &lt;- &quot;R/fig/plot_F5.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(furrr) library(ggtext) library(ape) library(ggtree) library(patchwork) library(phangorn) library(igraph) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_F5.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/twisst/weights/ #&gt; ★ 2: ressources/plugin/trees/ #&gt; ★ 3: https://raw.githubusercontent.com/simonhmartin/twisst/master/plot_twisst.R #&gt; ★ 4: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 5: 2_analysis/dxy/50k/ #&gt; ★ 6: 2_analysis/fst/50k/ #&gt; ★ 7: 2_analysis/summaries/fst_globals.txt #&gt; ★ 8: 2_analysis/GxP/50000/ #&gt; ★ 9: 200 #&gt; ★ 10: 5 #&gt; ★ 11: 2_analysis/revPoMo/outlier_regions/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directories for the different data types are received and stored in respective variables. Also, we source an external r script from the original twisst github repository that we need to import the twisst data: # config ----------------------- w_path &lt;- as.character(args[1]) d_path &lt;- as.character(args[2]) twisst_functions &lt;- as.character(args[3]) out_table &lt;- as.character(args[4]) dxy_dir &lt;- as.character(args[5]) fst_dir &lt;- as.character(args[6]) fst_globals &lt;- as.character(args[7]) gxp_dir &lt;- as.character(args[8]) twisst_size &lt;- as.numeric(args[9]) resolution &lt;- as.numeric(args[10]) pomo_path &lt;- as.character(args[11]) pomo_trees &lt;- dir(pomo_path, pattern = &quot;155_pop&quot;) source(twisst_functions, local = TRUE) Then, we define a buffer width. This is the space left and right of the \\(F_{ST}\\) outlier windows. plan(multiprocess) window_buffer &lt;- 2.5*10^5 21.2.2 Data import Then, we start with the data import. For the figure we are going to need: \\(d_{XY}\\) data genotype \\(\\times\\) phenotype data \\(F_{ST}\\) data topology weighing data the positions of the genome annotations the positions of the \\(F_{ST}\\) outlier windows the group-level phylogeny data (revPoMo) We start by importing \\(d_{XY}\\) by first listing all \\(d_{XY}\\) data files and then iterating the \\(d_{XY}\\) import function over the files. # actual script ========================================================= # locate dxy data files dxy_files &lt;- dir(dxy_dir, pattern = str_c(&#39;dxy.*[a-z]{3}.*.&#39;, resolution ,&#39;0kb-&#39;, resolution ,&#39;kb.tsv.gz&#39;)) # import dxy data dxy_data &lt;- tibble(file = str_c(dxy_dir, dxy_files)) %&gt;% purrr::pmap_dfr(get_dxy, kb = str_c(resolution, &#39;0kb&#39;)) Next we iterate the genotype \\(\\times\\) phenotype import function over the trait names Bars, Snout and Peduncle. # set traits of interest for GxP gxp_traits &lt;- c(&#39;Bars&#39;, &#39;Snout&#39;, &#39;Peduncle&#39;) # import GxP data gxp_data &lt;- str_c(gxp_dir,gxp_traits,&#39;.lm.&#39;, resolution ,&#39;0k.&#39;, resolution ,&#39;k.txt.gz&#39;) %&gt;% future_map_dfr(get_gxp_long, kb = 50) Then, we define two sets of colors - one for the topology highlighting schemes and one for the traits of the genotype \\(\\times\\) phenotype association. # set topology weighting color scheme twisst_clr &lt;- c(Blue = &quot;#0140E5&quot;, Bars = &quot;#E32210&quot;, Butter = &quot;#E4E42E&quot;) # set GxP color scheme gxp_clr &lt;- c(Bars = &quot;#79009f&quot;, Snout = &quot;#E48A00&quot;, Peduncle = &quot;#5B9E2D&quot;) %&gt;% darken(factor = .95) %&gt;% set_names(., nm = gxp_traits) Next, we compute the average genome wide \\(d_{XY}\\) and load the average genome wide \\(F_{ST}\\) values for all 28 pair wise species comparisons. # compute genome wide average dxy dxy_globals &lt;- dxy_data %&gt;% filter(BIN_START %% ( resolution * 10000 ) == 1 ) %&gt;% group_by( run ) %&gt;% summarise(mean_global_dxy = sum(dxy*N_SITES)/sum(N_SITES)) %&gt;% mutate(run = fct_reorder(run,mean_global_dxy)) # import genome wide average fst # and order population pairs by genomewide average fst fst_globals &lt;- vroom::vroom(fst_globals,delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run_prep&#39;,&#39;mean_fst&#39;,&#39;weighted_fst&#39;)) %&gt;% separate(run_prep,into = c(&#39;pop1&#39;,&#39;pop2&#39;),sep = &#39;-&#39;) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;, pop2, loc), run = fct_reorder(run,weighted_fst)) After this, we import the \\(F_{ST}\\) data by first listing all \\(F_{ST}\\) data files and then iterating the \\(F_{ST}\\) import function over the files. # locate fst data files fst_files &lt;- dir(fst_dir, pattern = str_c(&#39;.&#39;, resolution ,&#39;0k.windowed.weir.fst.gz&#39;)) # import fst data fst_data &lt;- str_c(fst_dir, fst_files) %&gt;% future_map_dfr(get_fst, kb = str_c(resolution, &#39;0k&#39;)) %&gt;% left_join(dxy_globals) %&gt;% left_join(fst_globals) %&gt;% mutate(run = refactor(., fst_globals), BIN_MID = (BIN_START+BIN_END)/2) We then add the genome wide averages of \\(F_{ST}\\) and \\(d_{XY}\\) as new columns to the \\(d_{XY}\\) data. This will be used later for coloring the \\(d_{XY}\\) panel. # order dxy population pairs by genomewide average fst dxy_data &lt;- dxy_data %&gt;% left_join(dxy_globals) %&gt;% left_join(fst_globals) %&gt;% mutate(run = refactor(dxy_data, fst_globals), window = &#39;bold(italic(d[xy]))&#39;) Then, we summarize the \\(d_{XY}\\) data to compute \\(\\Delta d_{XY}\\). # compute delta dxy data_dxy_summary &lt;- dxy_data %&gt;% group_by(GPOS) %&gt;% summarise(scaffold = CHROM[1], start = BIN_START[1], end = BIN_END[1], mid = BIN_MID[1], min_dxy = min(dxy), max_dxy = max(dxy), mean_dxy = mean(dxy), median_dxy = median(dxy), sd_dxy = sd(dxy), delta_dxy = max(dxy)-min(dxy)) To only load the relevant twisst data, we first load the positions of the \\(F_{ST}\\) outlier regions. We also define a set of outliers of interest. # load fst outlier regions outlier_table &lt;- vroom::vroom(out_table, delim = &#39;\\t&#39;) %&gt;% setNames(., nm = c(&quot;outlier_id&quot;,&quot;lg&quot;, &quot;start&quot;, &quot;end&quot;, &quot;gstart&quot;,&quot;gend&quot;,&quot;gpos&quot;)) # set outlier regions of interest outlier_pick = c(&#39;LG04_1&#39;, &#39;LG12_3&#39;, &#39;LG12_4&#39;) Then we define a set of genes of interest. These are the ones, that later will be labeled in the annotation panel. # select genes to label cool_genes &lt;- c(&#39;arl3&#39;,&#39;kif16b&#39;,&#39;cdx1&#39;,&#39;hmcn2&#39;, &#39;sox10&#39;,&#39;smarca4&#39;, &#39;rorb&#39;, &#39;alox12b&#39;,&#39;egr1&#39;, &#39;ube4b&#39;,&#39;casz1&#39;, &#39;hoxc8a&#39;,&#39;hoxc9&#39;,&#39;hoxc10a&#39;, &#39;hoxc13a&#39;,&#39;rarga&#39;,&#39;rarg&#39;, &#39;snai1&#39;,&#39;fam83d&#39;,&#39;mafb&#39;,&#39;sws2abeta&#39;,&#39;sws2aalpha&#39;,&#39;sws2b&#39;,&#39;lws&#39;,&#39;grm8&#39;) Next, we load the twisst data for both locations and list all species from Belize (This will be needed to calculate their pair wise distances for the topology highlighting). # load twisst data data_tables &lt;- list(bel = prep_data(loc = &#39;bel&#39;), hon = prep_data(loc = &#39;hon&#39;)) # set species sampled in belize pops_bel &lt;- c(&#39;ind&#39;, &#39;may&#39;, &#39;nig&#39;, &#39;pue&#39;, &#39;uni&#39;) Next, we import the group-level phylogenetic trees for the three focal outlier regions. # load group-level phylogeny data pomo_data &lt;- str_c(pomo_path, pomo_trees) %&gt;% purrr::map_dfr(import_tree) %&gt;% mutate(rooted_tree = map2(.x = tree,.y = type, .f = root_hamlets)) 21.2.3 Plotting Before the actual plotting, we are defining a list of outliers to be included within the final plots. # set outlier region label region_label_tibbles &lt;- tibble(outlier_id = outlier_pick, label = c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;)) We adjust the trait images to show the depicted trait in the same color it is also plotted within figure. # load and recolor trait icons trait_grob &lt;- tibble(svg = hypoimg::hypo_trait_img$grob_circle[hypoimg::hypo_trait_img$trait %in% gxp_traits], layer = c(4,3,7), color = gxp_clr[gxp_traits %&gt;% sort()])%&gt;% pmap(.f = hypo_recolor_svg) %&gt;% set_names(nm = gxp_traits %&gt;% sort()) # recolor second bars-layer trait_grob[[&quot;Bars&quot;]] &lt;- trait_grob[[&quot;Bars&quot;]] %&gt;% hypo_recolor_svg(layer = 7, color = gxp_clr[[&quot;Bars&quot;]]) The actual plotting starts with the visualizations of the group-level phylogenies which are plotted one by one: p_pomo1 &lt;- pomo_data$tree[[1]] %&gt;% midpoint() %&gt;% ggtree::ggtree(layout = &quot;circular&quot;) %&gt;% rotate(node = 18) %&gt;% .$data %&gt;% dplyr::mutate(support = as.numeric(label), support_class = cut(support, c(0, 50, 70, 90, 100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;))) %&gt;% plot_tree(higl_node = 21, angle_in = 168, color = twisst_clr[[&quot;Blue&quot;]], xlim = c(-.015, .1), open_angle = 168) p_pomo2 &lt;- pomo_data$tree[[2]] %&gt;% midpoint() %&gt;% ggtree::ggtree(layout = &quot;circular&quot;) %&gt;% .$data %&gt;% dplyr::mutate(support = as.numeric(label) /100, support_class = cut(support, c(0,.5,.7,.9,1)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,0.5]&quot;, &quot;(0.5,0.7]&quot;, &quot;(0.7,0.9]&quot;, &quot;(0.9,1]&quot;))) %&gt;% plot_tree(higl_node = 27, color = twisst_clr[[&quot;Bars&quot;]], xlim = c(-.015,.065), angle_in = 168, open_angle = 168) p_pomo3 &lt;- pomo_data$tree[[3]] %&gt;% midpoint() %&gt;% ggtree::ggtree(layout = &quot;circular&quot;) %&gt;% .$data %&gt;% dplyr::mutate(support = as.numeric(label) /100, support_class = cut(support, c(0,.5,.7,.9,1)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,0.5]&quot;, &quot;(0.5,0.7]&quot;, &quot;(0.7,0.9]&quot;, &quot;(0.9,1]&quot;))) %&gt;% plot_tree(higl_node = 28, color = twisst_clr[[&quot;Butter&quot;]], xlim = c(-.01, .053), angle_in = 168, open_angle = 168) The three phylogenies are combined into a single list. plot_list &lt;- list(p_pomo1, p_pomo2, p_pomo3) Then, we iterate the main plotting function over all selected \\(F_{ST}\\) outlier windows merge the resulting plot list with the prepared phylogenies and combine the resulting plots into a multi-panel plot. # compose base figure p_single &lt;- outlier_table %&gt;% filter(outlier_id %in% outlier_pick) %&gt;% left_join(region_label_tibbles) %&gt;% mutate(outlier_nr = row_number(), text = ifelse(outlier_nr == 1, TRUE, FALSE), trait = c(&#39;Snout&#39;, &#39;Bars&#39;, &#39;Peduncle&#39;)) %&gt;% pmap(plot_curtain, cool_genes = cool_genes, data_tables = data_tables) %&gt;% c(., plot_list) %&gt;% cowplot::plot_grid(plotlist = ., nrow = 2, rel_heights = c(1, .18), labels = letters[1:length(outlier_pick)] %&gt;% project_case(), label_size = plot_text_size) At this point all that we miss is the figure legend. So, for the \\(F_{ST}\\), \\(d_{XY}\\) and genotype \\(\\times\\) phenotype color schemes we create two dummy plots from where we can export the legends. We combine those two classical color legends with a legend for the phylogeny support values into what will become the left column of the legend. # compile legend # dummy plot for fst legend p_dummy_fst &lt;- outlier_table %&gt;% filter(row_number() == 1) %&gt;% purrr::pmap(plot_panel_fst) %&gt;% .[[1]] + guides(color = guide_colorbar(barheight = unit(3, &quot;pt&quot;), barwidth = unit(100, &quot;pt&quot;), label.position = &quot;top&quot;, ticks.colour = &quot;black&quot;)) # dummy plot for gxp legend p_dummy_gxp &lt;- outlier_table %&gt;% filter(row_number() == 1) %&gt;% purrr::pmap(plot_panel_gxp, trait = &#39;Bars&#39;) %&gt;% .[[1]] # fst legend p_leg_fst &lt;- (p_dummy_fst+theme(legend.position = &#39;bottom&#39;)) %&gt;% get_legend() # gxp legend p_leg_gxp &lt;- (p_dummy_gxp+theme(legend.position = &#39;bottom&#39;)) %&gt;% get_legend() # create sub-legend 1 p_leg_pomo &lt;- ((midpoint(pomo_data$tree[[1]]) %&gt;% ggtree(layout = &quot;circular&quot;) %&gt;% .$data %&gt;% mutate(support = as.numeric(label) /100, support_class = cut(support, c(0,.5,.7,.9,1)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,0.5]&quot;, &quot;(0.5,0.7]&quot;, &quot;(0.7,0.9]&quot;, &quot;(0.9,1]&quot;)))) %&gt;% conditional_highlight(tree = ., higl_node = 21, highl = FALSE, support_guide = TRUE) + theme(text = element_text(size = plot_text_size), legend.position = &quot;bottom&quot;) ) %&gt;% get_legend() p_leg1 &lt;- cowplot::plot_grid(p_leg_fst, p_leg_gxp, p_leg_pomo, ncol = 1) Then, we construct the topology highlighting color legend. We first define the three highlighting scenarios, the involved species and their base color and then iterate the legend plotting functions over those configurations. The resulting legend elements are then combined to create the right side of the figure legend and the two main legend elements are combined. # create sub-legend 2 (phylogeny schematics) p_leg2 &lt;- tibble(spec1 = c(&#39;indigo&#39;, &#39;indigo&#39;,&#39;unicolor&#39;), spec2 = c(&#39;maya&#39;, &#39;puella&#39;,NA), color = twisst_clr %&gt;% unname() %&gt;% darken(.,factor = .8), mode = c(rep(&#39;pair&#39;,2),&#39;isolation&#39;)) %&gt;% future_pmap(plot_leg) %&gt;% cowplot::plot_grid(plotlist = ., nrow = 1) # create sub-legend 3 p_leg &lt;- cowplot::plot_grid(p_leg1, p_leg2, nrow = 1, rel_widths = c(.6, 1)) After adding the legend to the main part, Figure 5 is complete. # finalize figure p_done &lt;- cowplot::plot_grid(p_single, p_leg, ncol = 1, rel_heights = c(1, .17)) Finally, we can export Figure 5. # export figure hypo_save(plot = p_done, filename = &#39;figures/F5.pdf&#39;, width = f_width, height = f_width * .93, comment = script_name, device = cairo_pdf) "],
["figure-6.html", "22 Figure 6 22.1 Summary 22.2 Details of plot_F6.R", " 22 Figure 6 22.1 Summary This is the accessory documentation of Figure 6. The Figure can be recreated by running the R script plot_F6.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_F6.R \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/geva/ \\ 2_analysis/GxP/bySNP/ 22.2 Details of plot_F6.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen, ggtext, ggpointdensity, scales, grid and prismatic. 22.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_F6.R \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # 2_analysis/geva/ 2_analysis/GxP/bySNP/ # =============================================================== # This script produces Suppl. Figure X of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c( &quot;2_analysis/summaries/fst_outliers_998.tsv&quot;, &quot;2_analysis/geva/&quot;, &quot;2_analysis/GxP/bySNP/&quot; ) # script_name &lt;- &quot;R/fig/plot_F6.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ggtext) library(ggpointdensity) library(scales) library(grid) library(prismatic) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_F6.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 2: 2_analysis/geva/ #&gt; ★ 3: 2_analysis/GxP/bySNP #&gt; ────────────────────────────────────────── /current/working/directory ── The directories allele age and genotype \\(\\times\\) phenotype data, as well as the outlier location file are received and stored in respective variables. # config ----------------------- outlier_file &lt;- as.character(args[1]) geva_path &lt;- as.character(args[2]) gxp_path &lt;- as.character(args[3]) Then, the outlier coordinates are loaded and the allele age and G \\(\\times\\) P data within those regions are imported. outlier_data &lt;- read_tsv(outlier_file) data &lt;- outlier_data[c(2, 13, 14),] %&gt;% pmap_dfr(get_gxp_and_geva) Next, some settings for the plotting are pre-defined. Those include the desire x-range, symbol sizes and the estimated age of the first split within Hypoplectrus. xrange &lt;- c(100, 10^6) color &lt;- rgb(1, 0.5, 0.16) base_length &lt;- 8 base_lwd &lt;- .15 base_line_clr &lt;- &quot;black&quot; splitage &lt;- tibble(intercept = 5000) Also, the outlier labels are formatted and the color scheme of the traits is set (for the G \\(\\times\\) P annotation). gid_label &lt;- c( LG04_1 = &quot;LG04 (A)&quot;, LG12_3 = &quot;LG12 (B)&quot;, LG12_4 = &quot;LG12 (C)&quot; ) gxp_clr &lt;- c(Bars = &quot;#79009f&quot;, Snout = &quot;#E48A00&quot;, Peduncle = &quot;#5B9E2D&quot;) %&gt;% darken(factor = .95) %&gt;% set_names(., nm = c(&quot;Bars&quot;, &quot;Snout&quot;, &quot;Peduncle&quot;)) Than the annotation images are loaded and the traits are re-colored according to the color scheme. annotation_grobs &lt;- tibble(svg = hypo_trait_img$grob_circle[hypo_trait_img$trait %in% c( &#39;Snout&#39;, &#39;Bars&#39;, &#39;Peduncle&#39;)], layer = c(4,3,7), color = gxp_clr[c(1,3,2)]) %&gt;% purrr::pmap(.l = ., .f = hypo_recolor_svg) %&gt;% set_names(nm = c( &quot;LG12_3&quot;,&quot;LG12_4&quot;,&quot;LG04_1&quot;)) annotation_grobs$LG12_3 &lt;- hypo_recolor_svg(annotation_grobs$LG12_3, layer = 7, color = gxp_clr[[1]] %&gt;% clr_desaturate %&gt;% clr_lighten(.25)) For the correct distribution of the annotations across the ggplot facets, the annotations are stored within a tibble that also holds the respective outlier ID \\(\\times\\) trait combinations. annotation_grobs_tib &lt;- tibble(gid = names(annotation_grobs), grob = annotation_grobs) %&gt;% mutate( gid_label = gid_label[gid], trait = factor( c( &quot;Bars&quot;, &quot;Peduncle&quot;, &quot;Snout&quot;), levels = c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;))) Similarly, a tibble for the background highlights is created (compatible with the faceting regime). highlight_rects &lt;- tibble(trait = factor( c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;), levels = c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;)), gid_label = gid_label) At this point, we can create Figure 6. p_done &lt;- data %&gt;% pivot_longer(names_to = &quot;trait&quot;, values_to = &quot;p_wald&quot;, cols = Bars:Snout) %&gt;% mutate(trait = factor(trait, levels = c(&quot;Snout&quot;, &quot;Bars&quot;, &quot;Peduncle&quot;)), gid_label = gid_label[gid]) %&gt;% filter(Clock == &quot;J&quot;, Filtered == 1) %&gt;% ggplot() + geom_rect(data = highlight_rects, aes( xmin = 0, xmax = Inf, ymin = 0, ymax = Inf), color = rgb(.75,.75,.75), size = .4, fill = rgb(.9,.9,.9,.5))+ hypoimg::geom_hypo_grob(inherit.aes = FALSE, data = annotation_grobs_tib, aes(grob = grob), x = .15, y = .78, angle = 0, width = .35, height =.35)+ geom_pointdensity(size = plot_size, aes(x = PostMedian,y = p_wald))+ facet_grid(gid_label ~ trait, scales = &quot;free_y&quot;)+ scale_x_log10(labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)))+ scale_y_continuous(trans = reverselog_trans(10), labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)))+ scale_color_viridis_c(&quot;Density&quot;, option = &quot;B&quot;)+ labs(y = &quot;G x P *p* value &lt;sub&gt;Wald&lt;/sub&gt;&quot;, x = &quot;Derived allele age (generations)&quot;)+ guides(color = guide_colorbar(barwidth = unit(120, &quot;pt&quot;), barheight = unit(3, &quot;pt&quot;)))+ theme_minimal()+ theme(text = element_text(size = plot_text_size), axis.title.y = element_markdown(), legend.position = &quot;bottom&quot;, plot.subtitle = element_markdown(), axis.line = element_line(colour = base_line_clr, size = base_lwd), strip.background = element_blank(), panel.grid.minor = element_blank(), panel.grid.major = element_line(size = plot_lwd) ) Finally, we can export Figure 6. hypo_save(plot = p_done, filename = &quot;figures/F6.pdf&quot;, width = f_width_half, height = f_width_half, comment = plot_comment, device = cairo_pdf, bg = &quot;transparent&quot;) "],
["supplementary-figure-1.html", "23 Supplementary Figure 1 23.1 Summary 23.2 Details of plot_SF1.R", " 23 Supplementary Figure 1 23.1 Summary This is the accessory documentation of Figure S1. The Figure can be recreated by running the R script plot_SF1.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF1.R 2_analysis/pca/ 23.2 Details of plot_SF1.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork 23.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF1.R 2_analysis/pca/ # =============================================================== # This script produces Suppl. Figure 1 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/pca/&quot;) # script_name &lt;- &quot;R/fig/plot_SF1.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(patchwork) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF1.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/pca/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the PCA data is received and stored in a variable. Also the default color scheme is updated and the size of the hamlet ann. # config ----------------------- pca_dir &lt;- as.character(args[1]) clr_alt &lt;- clr clr_alt[[&quot;uni&quot;]] &lt;- &quot;lightgray&quot; Then the layout of the legend elements, labels and patch sizes are pre-configured. fish_tib &lt;- tibble(short = names(clr)[!names(clr) %in% c(&quot;flo&quot;, &quot;tab&quot;, &quot;tor&quot;)], x = c(0.5, 3.5, 7, 9.7, 12.25, 15.25, 18, 21.5)) key_sz &lt;- .75 sp_fam &lt;- rep(c(&quot;H&quot;, &quot;S&quot;, &quot;H&quot;), c(8, 2, 1)) %&gt;% set_names(nm = names(sp_names)) Now, the legend is created. p_leg &lt;- fish_tib %&gt;% ggplot() + coord_equal(xlim = c(-.05, 24), expand = 0) + geom_tile(aes(x = x, y = 0, fill = short, color = after_scale(prismatic::clr_darken(fill, .25))), width = key_sz, height = key_sz, size = .3) + geom_text(aes(x = x + .6, y = 0, label = str_c(sp_fam[short], &quot;. &quot;, sp_names[short])), hjust = 0, fontface = &quot;italic&quot;, size = plot_text_size / ggplot2:::.pt) + pmap(fish_tib, plot_fish_lwd, width = 1, height = 1, y = 0) + scale_fill_manual(values = clr, guide = FALSE) + theme_void() Then, by running the function pca_plot_no_fish() one on each location, the figure can be assembled. p_done &lt;- cowplot::plot_grid((tibble(loc = c(&quot;bel.&quot;, &quot;hon.&quot;, &quot;pan.&quot;), mode = rep(c(&quot;subset_non_diverged&quot;), 3), pc1 = 1, pc2 = 2) %&gt;% pmap(pca_plot_no_fish) %&gt;% wrap_plots(ncol = 3) + plot_annotation(tag_levels = &quot;a&quot;) &amp; theme(plot.background = element_blank())), p_leg, ncol = 1 , rel_heights = c(1,.1)) Finally, we can export Figure S1. hypo_save(p_done, filename = &#39;figures/SF1.pdf&#39;, width = f_width, height = f_width * .38, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["supplementary-figure-2.html", "24 Supplementary Figure 2 24.1 Summary 24.2 Details of plot_SF2.R", " 24 Supplementary Figure 2 24.1 Summary This is the accessory documentation of Figure S2. The Figure can be recreated by running the R script plot_SF2.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF2.R 24.2 Details of plot_SF2.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen and patchwork. 24.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF2.R \\ # 2_analysis/dxy/50k/ 2_analysis/fst/50k/ 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Suppl. Figure 2 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/dxy/50k/&#39;, &#39;2_analysis/fst/50k/&#39;, &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_SF2.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF2.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ★ 2: 2_analysis/fst/50k/ #&gt; ★ 3: 2_analysis/summaries/fst_globals.txt #&gt; ────────────────────────────────────────── /current/working/directory ── The directories containing the \\(F_{ST}\\) and \\(d_{XY}\\) data, as well as the file containing the genome wide average \\(F_{ST}\\) values are received and stored in respective variables. Also, we set a few parameters for the plot layout: # config ----------------------- dxy_dir &lt;- as.character(args[1]) fst_dir &lt;- as.character(args[2]) fst_globals &lt;- as.character(args[3]) wdh &lt;- .3 # The width of the boxplots scaler &lt;- 20 # the ratio of the Fst and the dxy axis clr_sec &lt;- &#39;gray&#39; # the color of the secondary axis (dxy) Then we start with the data import for the \\(F_{ST}\\) data. First all file names are collected, then all files are imporeted and combined into a single tibble. # start script ------------------- # import Fst fst_files &lt;- dir(fst_dir, pattern = &#39;.50k.windowed.weir.fst.gz&#39;) fst_data &lt;- str_c(fst_dir,fst_files) %&gt;% purrr::map(summarize_fst) %&gt;% bind_rows() The same data import approach is used for the \\(d_{XY}\\) data. # lookup dxy files dxy_files &lt;- dir(dxy_dir) # import dxy dxy_data &lt;- str_c(dxy_dir,dxy_files) %&gt;% purrr::map(summarize_dxy) %&gt;% bind_rows() The rank of the pair-wise species comparisons according to their \\(F_{ST}\\) determined. # determine fst ranking fst_order &lt;- fst_data %&gt;% select(run, `mean_weighted-fst`) %&gt;% mutate(run = fct_reorder(run, `mean_weighted-fst`)) Then the \\(F_{ST}\\) and \\(d_{XY}\\) data are merged (this is due to the legacy of this script on by now makes little sense since the \\(F_{ST}\\) is not plotted anymore). # merge fst and dxy cc_data # (large parts of this code are now unnecessary after the separation of dxy and # fst plots into separate panels b &amp; c) data &lt;- left_join(fst_data, dxy_data) %&gt;% select(c(8,1:7,9:15)) %&gt;% # reformat table to enable parallel plotting (with secondary axis) gather(key = &#39;stat&#39;, value = &#39;val&#39;, 2:15) %&gt;% # sumstat contains the values needed to plot the boxplots (quartiles, etc) separate(stat, into = c(&#39;sumstat&#39;, &#39;popstat&#39;), sep = &#39;_&#39;) %&gt;% # duplicate dxy values scaled to fst range mutate(val_scaled = ifelse(popstat == &#39;dxy&#39;, val * scaler , val)) %&gt;% unite(temp, val, val_scaled) %&gt;% # separate th eoriginal values from the scales ons (scaled = secondary axis) spread(.,key = &#39;sumstat&#39;,value = &#39;temp&#39;) %&gt;% separate(mean, into = c(&#39;mean&#39;,&#39;mean_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(median, into = c(&#39;median&#39;,&#39;median_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(sd, into = c(&#39;sd&#39;,&#39;sd_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(lower, into = c(&#39;lower&#39;,&#39;lower_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(upper, into = c(&#39;upper&#39;,&#39;upper_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(lowpoint, into = c(&#39;lowpoint&#39;,&#39;lowpoint_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(highpoint, into = c(&#39;highpoint&#39;,&#39;highpoint_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% # include &quot;dodge&quot;-positions for side-by-side plotting (secondary axis) mutate(loc = str_sub(run,4,6), run = factor(run, levels = levels(fst_order$run)), x = as.numeric(run) , x_dodge = ifelse(popstat == &#39;dxy&#39;, x + .25, x - .25), x_start_dodge = x_dodge - wdh/2, x_end_dodge = x_dodge + wdh/2, popstat_loc = str_c(popstat,&#39;[&#39;,loc,&#39;]&#39;)) For the network plots we need a long-format version of the \\(F_{ST}\\) data though (another legacy piece). # sort run by average genome wide Fst fst_data_gather &lt;- data %&gt;% filter(popstat == &quot;weighted-fst&quot;) %&gt;% gather(key = &#39;stat&#39;, value = &#39;val&#39;, -run) %&gt;% # sumstat contains the values needed to plot the boxplots (quartiles, etc) separate(stat, into = c(&#39;sumstat&#39;, &#39;popstat&#39;), sep = &#39;_&#39;) %&gt;% # duplicate dxy values scaled to fst range mutate(val_scaled = ifelse(popstat == &#39;dxy&#39;, val * scaler , val)) %&gt;% unite(temp, val, val_scaled) %&gt;% # separate th eoriginal values from the scales ons (scaled = secondary axis) spread(.,key = &#39;sumstat&#39;,value = &#39;temp&#39;) %&gt;% separate(mean, into = c(&#39;mean&#39;,&#39;mean_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(median, into = c(&#39;median&#39;,&#39;median_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(sd, into = c(&#39;sd&#39;,&#39;sd_scaled&#39;),sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(lower, into = c(&#39;lower&#39;,&#39;lower_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(upper, into = c(&#39;upper&#39;,&#39;upper_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(lowpoint, into = c(&#39;lowpoint&#39;,&#39;lowpoint_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% separate(highpoint, into = c(&#39;highpoint&#39;,&#39;highpoint_scaled&#39;), sep = &#39;_&#39;, convert = TRUE) %&gt;% # include &quot;dodge&quot;-positions for side-by-side plotting (secondary axis) mutate(loc = str_sub(run,4,6), run = factor(run, levels = levels(fst_order$run)), x = as.numeric(run) , x_dodge = ifelse(popstat == &#39;dxy&#39;, x + .25, x - .25), x_start_dodge = x_dodge - wdh/2, x_end_dodge = x_dodge + wdh/2, popstat_loc = str_c(popstat,&#39;[&#39;,loc,&#39;]&#39;)) run_ord &lt;- tibble(run = levels(data$run), run_ord = 1:length(levels(data$run))) At this point we can prepare the plotting of the network panel. This starts by creating a tibble with the configurations of each individualnetwork. # assemble panel a networx &lt;- tibble( loc = c(&#39;bel&#39;,&#39;hon&#39;, &#39;pan&#39;), n = c(5, 6, 3), label = list(str_c(c(&#39;ind&#39;,&#39;may&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;),&#39;bel&#39;), str_c(c(&#39;abe&#39;,&#39;gum&#39;,&#39;nig&#39;,&#39;pue&#39;,&#39;ran&#39;,&#39;uni&#39;),&#39;hon&#39;), str_c(c(&#39;nig&#39;,&#39;pue&#39;,&#39;uni&#39;),&#39;pan&#39;)), weight = c(1,1.45,1)) %&gt;% purrr::pmap_dfr(network_layout) %&gt;% mutate(edges = map(edges, function(x){x %&gt;% left_join(fst_data_gather %&gt;% filter(popstat == &quot;weighted-fst&quot;) %&gt;% select(run, median, mean)) })) Then we can plot the networks by running plot_network() and combine them using cowplot::plot_grid(). plot_list &lt;- networx %&gt;% purrr::pmap(plot_network, node_lab_shift = .2) p_net &lt;- cowplot::plot_grid( grid::textGrob(&#39;Belize&#39;, gp = gpar(fontsize = plot_text_size, col = clr_loc[[&quot;bel&quot;]])), grid::textGrob(&#39;Honduras&#39;, gp = gpar(fontsize = plot_text_size, col = clr_loc[[&quot;hon&quot;]])), grid::textGrob(&#39;Panama&#39;, gp = gpar(fontsize = plot_text_size, col = clr_loc[[&quot;pan&quot;]])), plot_list[[1]] + theme(legend.position = &quot;none&quot;), plot_list[[2]] + theme(legend.position = &quot;none&quot;), plot_list[[3]] + theme(legend.position = &quot;none&quot;), ncol = 3, rel_heights = c(.1,1) ) %&gt;% cowplot::as_grob() We can now also plot the second panel. # assemble panel b p &lt;- data %&gt;% filter(popstat == &quot;dxy&quot;) %&gt;% ggplot(aes(color = loc)) + geom_segment(aes(x = x, xend = x, y = lowpoint, yend = highpoint), lwd = plot_lwd)+ geom_rect(aes(xmin = x - wdh, xmax = x + wdh, ymin = lower, ymax = upper), fill = &#39;white&#39;, size = plot_lwd)+ geom_segment(aes(x = x - wdh, xend = x + wdh, y = median, yend = median), lwd = plot_lwd)+ geom_point(aes(x = x, y = mean), shape = 21, size = .7, fill = &#39;white&#39;)+ scale_x_continuous(breaks = 1:28) + scale_y_continuous( expression(italic(d[XY])), breaks = c(0,.0025,.005,.0075,.01), limits = c(0,.01))+ scale_color_manual(values = c(make_faint_clr(&#39;bel&#39;), make_faint_clr(&#39;hon&#39;), make_faint_clr(&#39;pan&#39;))[c(2,4,6)])+ coord_cartesian(xlim = c(0,29), expand = c(0,0))+ theme_minimal()+ theme(text = element_text(size = plot_text_size), axis.title.x = element_blank(), legend.position = &#39;none&#39;, strip.placement = &#39;outside&#39;, strip.text = element_text(size = 12), axis.text.y.right = element_text(color = clr_sec), axis.title.y.right = element_text(color = clr_sec)) To create the final Figure S2. # merge panel a &amp; b p_done &lt;- plot_grid(p_net, p, ncol = 1, rel_heights = c(.4, 1), labels = letters[1:2], label_fontface = &quot;plain&quot;, label_size = plot_text_size) Finally, we can export Figure S2. # export figure 1 hypo_save(p_done, filename = &#39;figures/SF2.pdf&#39;, width = f_width * .5, height = f_width * .5, comment = plot_comment) "],
["supplementary-figure-3.html", "25 Supplementary Figure 3 25.1 Summary 25.2 Details of plot_SF3.R", " 25 Supplementary Figure 3 25.1 Summary This is the accessory documentation of Figure S3. The Figure can be recreated by running the R script plot_SF3.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF3.R 2_analysis/dxy/50k/ 25.2 Details of plot_SF3.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, paletteer, patchwork and prismatic. 25.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF3.R 2_analysis/dxy/50k/ # =============================================================== # This script produces Suppl. Figure 3 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/dxy/50k/&quot;) # script_name &lt;- &quot;R/fig/plot_SF3.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypogen) library(hypoimg) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF3.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the\\(d_{XY}\\) data is received and stored in a variable. # config ----------------------- dxy_path &lt;- as.character(args[1]) Then, all file names of all files within the data folder are gathered… # locate dxy data files files &lt;- dir(dxy_path) … and all \\(d_{XY}\\) data is loaded. # load dxy data data &lt;- str_c(dxy_path, files) %&gt;% purrr::map(get_dxy) %&gt;% bind_rows() %&gt;% purrr::set_names(., nm = c(&#39;scaffold&#39;, &#39;start&#39;, &#39;end&#39;, &#39;mid&#39;, &#39;sites&#39;, &#39;pi_pop1&#39;, &#39;pi_pop2&#39;, &#39;dxy&#39;, &#39;fst&#39;, &#39;GSTART&#39;, &#39;gpos&#39;, &#39;run&#39;)) Next, the genome wide average \\(d_{XY}\\) is computed for all species pairs. (This is done to be able to later order the sub-plots with decreasing average genome wide \\(d_{XY}\\).) genoe_wide_avg &lt;- data %&gt;% group_by(run) %&gt;% summarise(avg_dxy = mean(dxy)) %&gt;% ungroup() %&gt;% arrange(avg_dxy) Next, a simple linear model is run for all species pairs. model_data &lt;- data %&gt;% pivot_longer(cols = starts_with(&quot;pi_pop&quot;), names_to = &quot;pi_pop&quot;, values_to= &quot;pi&quot;) %&gt;% mutate(pop = str_remove(pi_pop,&quot;pi_pop&quot;) %&gt;% str_c(&quot;Pop. &quot;,.)) %&gt;% # filter fst data to &quot;non-overlapping&quot; windows filter(start %% 50000 == 1 ) %&gt;% group_by(run, pop) %&gt;% nest() %&gt;% mutate(mod = map(data, function(data){lm(pi ~ dxy, data = data)})) %&gt;% bind_cols(., summarise_model(.)) Then, a wrapper function is created that will allow to plot a subset of all the species pairs in one go. The function first subsets the \\(d_{XY}\\) data set and then directly plots them. dxy_subplot &lt;- function(select_idx){ run_select &lt;- genoe_wide_avg$run[select_idx] plt_data &lt;- data %&gt;% filter(run %in% run_select) %&gt;% pivot_longer(cols = starts_with(&quot;pi_pop&quot;), names_to = &quot;pi_pop&quot;, values_to= &quot;pi&quot;) %&gt;% mutate(pop = str_remove(pi_pop,&quot;pi_pop&quot;) %&gt;% str_c(&quot;Pop. &quot;,.)) base_lwd &lt;- .15 base_line_clr &lt;- &quot;black&quot; p &lt;- plt_data %&gt;% ggplot(aes(x = dxy, y = pi))+ #coord_equal()+ facet_grid(pop ~ run,switch = &quot;y&quot;)+ geom_hex(bins = 30, color = rgb(0,0,0,.3), aes(fill=log10(..count..)))+ # add regression line geom_abline(data = model_data %&gt;% filter(run %in% run_select), color = rgb(1,1,1,.8), linetype = 2, aes(intercept = intercept, slope = slope)) + # add R^2 label geom_text(data = model_data%&gt;% filter(run %in% run_select), x = 0, y = .022, parse = TRUE, hjust = 0, vjust = 1, size = 3, aes(label = str_c(&#39;italic(R)^2:~&#39;,round(r.squared, 3)))) + scale_y_continuous(&quot;\\U03C0&quot;, breaks = c(0,.01,.02),labels = c(&quot;0&quot;, &quot;0.01&quot;, &quot;0.02&quot;))+ scale_x_continuous(expression(italic(d[XY])), breaks = c(0,.01,.02),labels = c(&quot;0&quot;, &quot;0.01&quot;, &quot;0.02&quot;))+ scico::scale_fill_scico(palette = &#39;berlin&#39;, limits = c(0,4.2))+ guides(fill = guide_colorbar(direction = &#39;horizontal&#39;, title.position = &#39;top&#39;, barheight = unit(7,&#39;pt&#39;), barwidth = unit(130,&#39;pt&#39;)))+ # general plot layout theme_minimal()+ theme(legend.position = &quot;bottom&quot;, axis.title.y = element_text(face = &quot;italic&quot;), strip.placement = &quot;outside&quot;, strip.background.x = element_rect(fill = rgb(.95,.95,.95), colour = base_line_clr,size = base_lwd), panel.border = element_rect(size = base_lwd, color = base_line_clr %&gt;% clr_lighten(factor = .8), fill = rgb(1,1,1,0)) ) p } Now, the individual sub-plot are created. ps &lt;- list(1:7, 8:14, 15:21, 22:28) %&gt;% map(dxy_subplot) Form the sub-plots, the final figure is assembled. Finally, we can export Figure S3. # export final figure scl &lt;- 1.2 hypo_save(filename = &#39;figures/SF3.pdf&#39;, plot = p_done, width = f_width * scl, height = f_width * 1.15 * scl, device = cairo_pdf, comment = plot_comment) "],
["supplementary-figure-4.html", "26 Supplementary Figure 4 26.1 Summary 26.2 Details of plot_SF4.R", " 26 Supplementary Figure 4 26.1 Summary This is the accessory documentation of Figure S4. The Figure can be recreated by running the R script plot_SF4.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF4.R 2_analysis/newhyb/nh_input/NH.Results/ 26.2 Details of plot_SF4.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages ggtext, hypoimg, hypogen, paletteer, patchwork and prismatic. 26.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF4.R 2_analysis/newhyb/nh_input/NH.Results/ # =============================================================== # This script produces Suppl. Figure 4 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/newhyb/nh_input/NH.Results/&quot;) # script_name &lt;- &quot;R/fig/plot_SF4.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(prismatic) library(paletteer) library(patchwork) library(ggtext) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_SF4.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/newhyb/nh_input/NH.Results/ #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the hybridization data is received and stored in a variable. # config ----------------------- base_dir &lt;- as.character(args[1]) All the hybridization-subfolders are located (there is one per pair wise newhybrids run). # locate hybridization data files folders &lt;- dir(base_dir) Then we run the high-level function GenomicOriginsScripts::plot_loc() which reads in all the hybridization data of a given sampling location and creates the respective figure panel. # load data and create plots by location p_loc &lt;- c(&quot;bel&quot;, &quot;hon&quot;, &quot;pan&quot;) %&gt;% map(plot_loc) As an example we can have a look at the result for plot_loc(\"pan\"): All three panels are collected and the final Figure is composed using the package patchwork: # compose figure from the individual panels p_done &lt;- (p_loc[[1]] + guides(fill = guide_legend(title = &quot;Hybrid Class&quot;)) + theme_hyb(legend.position = c(1,1)) ) + (p_loc[[2]] + theme_hyb() ) + (p_loc[[3]] + theme_hyb() ) + plot_layout(ncol = 1, heights = c(10,15,3) %&gt;% label_spacer())+ plot_annotation(tag_levels = &#39;a&#39;) Finally, we can export Figure S4. # export the final figure hypo_save(filename = &quot;figures/SF4.pdf&quot;, plot = p_done, height = 16, width = 10, device = cairo_pdf, comment = plot_comment) "],
["supplementary-figure-5.html", "27 Supplementary Figure 5 27.1 Summary 27.2 Details of plot_SF5.R", " 27 Supplementary Figure 5 27.1 Summary This is the accessory documentation of Figure S5. The Figure can be recreated by running the R script plot_SF5.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF5.R 2_analysis/fst/50k/ \\ 2_analysis/summaries/fst_outliers_998.tsv \\ 2_analysis/summaries/fst_globals.txt 27.2 Details of plot_SF5.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages ggtext, hypoimg, hypogen and vroom. 27.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF5.R 2_analysis/fst/50k/ \\ # 2_analysis/summaries/fst_outliers_998.tsv \\ # 2_analysis/summaries/fst_globals.txt # =============================================================== # This script produces Suppl. Figure 5 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/fst/50k/&#39;, # &#39;2_analysis/summaries/fst_outliers_998.tsv&#39;, # &#39;2_analysis/summaries/fst_globals.txt&#39;) # script_name &lt;- &quot;R/fig/plot_SF5.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(vroom) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF5.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/fst/50k/ #&gt; ★ 2: 2_analysis/summaries/fst_outliers_998.tsv #&gt; ★ 3: 2_analysis/summaries/fst_globals.txt #&gt; ────────────────────────────────────────── /current/working/directory ── The directory containing the \\(F_{ST}\\) data, and the files containing the locations of the \\(F_{ST}\\) outlier regions and the genome wide \\(F_{ST}\\) averages are received and stored in a variable. # config ----------------------- data_path &lt;- as.character(args[1]) outlier_file &lt;- as.character(args[2]) globals_file &lt;- as.character(args[3]) The files containing the windowed \\(F_{ST}\\) data are located. # load data ------------------- # locate fst data files files &lt;- dir(data_path, pattern = &#39;.50k.windowed.weir.fst.gz&#39;) Based on these data files, the names of the pair wise species comparisons are created. # extract run names from data file names run_files &lt;- files %&gt;% str_sub(.,1,11) %&gt;% str_replace(.,pattern = &#39;([a-z]{3})-([a-z]{3})-([a-z]{3})&#39;, &#39;\\\\2\\\\1-\\\\3\\\\1&#39;) Then, the genome wide average \\(F_{ST}\\) values are loaded. # load genome wide average fst values for each run globals &lt;- vroom::vroom(globals_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run&#39;,&#39;mean&#39;,&#39;weighted&#39;)) %&gt;% separate(run, into = c(&#39;pop1&#39;,&#39;pop2&#39;)) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;,pop2,loc), run = fct_reorder(run,weighted)) Next, the windowed \\(F_{ST}\\) data are imported. # load all windowed fst data and collapse in to a single data frame data &lt;- purrr::pmap(tibble(file = str_c(data_path,files), run = run_files), hypo_import_windows) %&gt;% bind_rows() %&gt;% purrr::set_names(., nm = c(&#39;CHROM&#39;, &#39;BIN_START&#39;, &#39;BIN_END&#39;, &#39;N_VARIANTS&#39;, &#39;WEIGHTED_FST&#39;, &#39;MEAN_FST&#39;, &#39;GSTART&#39;, &#39;POS&#39;, &#39;GPOS&#39;, &#39;run&#39;)) %&gt;% mutate(pop1 = str_sub(run,1,3), pop2 = str_sub(run,8,10), loc = str_sub(run,4,6), run_label = str_c(&quot;*H. &quot;, sp_names[pop1],&quot;* - *H. &quot;, sp_names[pop2],&quot;*&lt;br&gt;(&quot;,loc_names[loc],&quot;)&quot; )) To be able to indicate the genome wide average \\(F_{ST}\\) in the background of the figure, the \\(F_{ST}\\) values are scaled to the extent of the hamlet reference genome. The rescaled \\(F_{ST}\\) values are compiled into a table for plotting. # create table for the indication of genome wide average fst in the plot background # (rescale covered fst range to the extent of the genome) global_bar &lt;- globals %&gt;% select(weighted,run) %&gt;% mutate(run = as.character(run)) %&gt;% setNames(.,nm = c(&#39;fst&#39;,&#39;run&#39;)) %&gt;% pmap(.,fst_bar_row_run) %&gt;% bind_rows() %&gt;% mutate(pop1 = str_sub(run,1,3), pop2 = str_sub(run,8,10), loc = str_sub(run,4,6), run_label = str_c(&quot;*H. &quot;, sp_names[pop1],&quot;* - *H. &quot;, sp_names[pop2],&quot;*&lt;br&gt;(&quot;,loc_names[loc],&quot;)&quot; ), run_label = fct_reorder(run_label,xmax_org)) To indicate the genome wide average \\(F_{ST}\\) on a secondary x-axis, the secondary x-breakes are pre-computed. # pre-calculate secondary x-axis breaks sc_ax &lt;- scales::cbreaks(c(0,max(globals$weighted)), scales::pretty_breaks(4)) Then, the Supplementary Figure is assembled. # compose final figure p_done &lt;- ggplot()+ # general plot structure separated by run facet_grid( run_label ~ ., as.table = TRUE) + # add genome wide average fst in the background geom_rect(data = global_bar %&gt;% mutate(xmax = xmax * hypo_karyotype$GEND[24]), aes(xmin = 0, xmax = xmax, ymin = -Inf, ymax = Inf), color = rgb(1,1,1,0), fill = clr_below) + # add LG borders geom_vline(data = hypogen::hypo_karyotype, aes(xintercept = GEND), color = hypo_clr_lg) + # add fst data points geom_point(data = data %&gt;% mutate(run_label = factor(run_label, levels = levels(global_bar$run_label))), aes(x = GPOS, y = WEIGHTED_FST), size=.2,color = plot_clr) + # axis layout scale_x_hypo_LG(sec.axis = sec_axis(~ ./hypo_karyotype$GEND[24], breaks = (sc_ax$breaks/max(globals$weighted)), labels = sprintf(&quot;%.2f&quot;, sc_ax$breaks), name = expression(Genomic~position/~Genome~wide~weighted~italic(F[ST])))) + scale_y_continuous(name = expression(italic(&#39;F&#39;[ST])), limits = c(-.1,1), breaks = c(0,.5,1)) + # general plot layout theme_hypo() + theme(strip.text.y = element_markdown(angle = 0), strip.background = element_blank(), legend.position = &#39;none&#39;, axis.title.x = element_text(), axis.text.x.bottom = element_text(colour = &#39;darkgray&#39;)) Finally, we can export Figure S5. # export final figure hypo_save(filename = &#39;figures/SF5.png&#39;, plot = p_done, width = 8, height = 12, type = &quot;cairo&quot;, comment = plot_comment) "],
["supplementary-figure-6.html", "28 Supplementary Figure 6 28.1 Summary 28.2 Details of plot_SF6.R", " 28 Supplementary Figure 6 28.1 Summary This is the accessory documentation of Supplementary Figure 6. The Figure can be recreated by running the R script plot_SF6.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF6.R \\ 2_analysis/summaries/fst_globals.txt \\ 2_analysis/fst/50k/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz 28.2 Details of plot_SF6.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts as well as the packages hypoimg, hypogen, furrr, ggtext and vroom. 28.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF6.R \\ # 2_analysis/summaries/fst_globals.txt \\ # 2_analysis/fst/50k/ \\ # 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz # =============================================================== # This script produces Suppl. Figure 6 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c( &#39;2_analysis/summaries/fst_globals.txt&#39;, # &#39;2_analysis/fst/50k/&#39;, # &#39;2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz&#39;) # script_name &lt;- &quot;R/fig/plot_SF6.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The R package GenomicOriginsScripts is loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(vroom) library(furrr) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_SF6.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/summaries/fst_globals.txt #&gt; ★ 2: 2_analysis/fst/50k/ #&gt; ★ 3: 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz #&gt; ────────────────────────────────────────── /current/working/directory ── The directories containing the \\(F_{ST}\\) and \\(\\rho\\) data, and the file containing the genome wide \\(F_{ST}\\) averages are received and stored in a variable. # config ----------------------- global_fst_file &lt;- as.character(args[1]) fst_dir &lt;- as.character(args[2]) rho_dir &lt;- as.character(args[3]) Initially, the genome wide average \\(F_{ST}\\) values are loaded. # load genome wide average fst data fst_globals &lt;- vroom::vroom(global_fst_file, delim = &#39;\\t&#39;, col_names = c(&#39;loc&#39;,&#39;run_prep&#39;,&#39;mean_fst&#39;,&#39;weighted_fst&#39;)) %&gt;% separate(run_prep,into = c(&#39;pop1&#39;,&#39;pop2&#39;),sep = &#39;-&#39;) %&gt;% mutate(run = str_c(pop1,loc,&#39;-&#39;,pop2,loc), run = fct_reorder(run,weighted_fst)) Then, the files containing the windowed \\(F_{ST}\\) data are located. # locate sliding window fst data files fst_files &lt;- dir(fst_dir, pattern = &#39;.50k.windowed.weir.fst.gz&#39;) Next, these files are loaded. # load sliding window fst data fst_data &lt;- str_c(fst_dir,fst_files) %&gt;% furrr::future_map_dfr(get_fst) %&gt;% mutate(run = factor(run, levels = levels(fst_globals$run))) After this, also the recombination data is loaded. # load recombination rate data rho_data &lt;- vroom::vroom(rho_dir, delim = &#39;\\t&#39;) %&gt;% select(-BIN_END) The \\(F_{ST}\\) and \\(\\rho\\) data frames are merged based on the genomic position and subset to only contain non-overlapping windows. # merge fst and recombination data combined_data &lt;- fst_data %&gt;% # filter fst data to &quot;non-overlapping&quot; windows filter(BIN_START %% 50000 == 1 ) %&gt;% # merge with recombination data left_join(rho_data, by = c(CHROM = &#39;CHROM&#39;, BIN_START = &#39;BIN_START&#39;)) %&gt;% # merge with genome wide average fst data left_join(.,fst_globals %&gt;% select(run, weighted_fst)) %&gt;% # add label column mutate(pop1 = str_sub(run,1,3), pop2 = str_sub(run,8,10), loc = str_sub(run,4,6), run_label = str_c(&quot;*H. &quot;, sp_names[pop1],&quot;* - *H. &quot;, sp_names[pop2],&quot;*&lt;br&gt;(&quot;,loc_names[loc],&quot;)&quot; ), run_label = fct_reorder(run_label,weighted_fst)) Then, the combined data frame is nested based on the pair wise species comparison and for each comparison, \\(F_{ST}\\) is regressed on \\(\\rho\\). The summary statistics of these regressions are added as separate columns of the nested data frame. # nest data to run linear regression on all runs in one go model_data &lt;- combined_data %&gt;% group_by(run) %&gt;% nest() %&gt;% left_join(., fst_globals) %&gt;% mutate(mod = map(data, function(data){lm(WEIGHTED_FST ~ RHO, data = data)}), pop1 = str_sub(run,1,3), pop2 = str_sub(run,8,10), loc = str_sub(run,4,6), run_label = str_c(&quot;*H. &quot;, sp_names[pop1],&quot;* - *H. &quot;, sp_names[pop2],&quot;*&lt;br&gt;(&quot;,loc_names[loc],&quot;)&quot; )) %&gt;% bind_cols(., summarise_model(.)) %&gt;% mutate(run_label = factor(run_label, levels = levels(combined_data$run_label))) Now, we can create the individual panels of the figure. We start with panel a: # create subplot a (hex-bins) p1 &lt;- combined_data %&gt;% ggplot()+ # add hex-bin desity layer geom_hex(bins = 30, color = rgb(0,0,0,.3), aes(fill=log10(..count..), x = RHO, y = WEIGHTED_FST))+ # add regression line geom_abline(data = model_data, color = rgb(1,1,1,.8), linetype = 2, aes(intercept = intercept, slope = slope)) + # add R^2 label geom_text(data = model_data, x = 0, y = .975, parse = TRUE, hjust = 0, vjust = 1, aes(label = str_c(&#39;italic(R)^2:~&#39;,round(r.squared,2)))) + # general plot structure (separated by run) facet_wrap(run_label ~., ncol = 5)+ # set axis layout and color scheme scale_x_continuous(name = expression(rho))+ scale_y_continuous(name = expression(italic(F[ST])),limits = c(-.05,1))+ scico::scale_fill_scico(palette = &#39;berlin&#39;) + # customize legend guides(fill = guide_colorbar(direction = &#39;horizontal&#39;, title.position = &#39;top&#39;, barheight = unit(7,&#39;pt&#39;), barwidth = unit(130,&#39;pt&#39;)))+ # general plot layout theme_minimal()+ theme(legend.position = c(.8,.08), strip.text = element_markdown()) Then we plot the slopes of the regression within panel b. # create subplot b (slopes) p2 &lt;- model_data %&gt;% ggplot()+ geom_point(color = plot_clr, aes(x = weighted_fst, y = slope))+ labs(x = expression(genome~wide~weighted~mean~italic(F[ST])), y = expression(slope~(f(italic(F[ST]))==a~rho+b)))+ theme_minimal() And finally the \\(R^2\\) values in panel c. # create subplot c (R^2s) p3 &lt;- model_data %&gt;% ggplot()+ geom_point(color = plot_clr, aes(x = weighted_fst, y = r.squared))+ labs(x = expression(genome~wide~weighted~mean~italic(F[ST])), y = expression(italic(R^2)))+ theme_minimal() The final figure is created by combining the three sub panels. # compose final figure p_done &lt;- plot_grid(p1, plot_grid(p2,p3, nrow = 1, labels = letters[2:3] %&gt;% project_case()), ncol = 1, rel_heights = c(1,.3),labels = project_case(c(&quot;a&quot;))) Finally, we can export Figure SF6. # export final figure hypo_save(filename = &#39;figures/SF6.pdf&#39;, plot = p_done, width = 10, height = 16, comment = plot_comment) "],
["supplementary-figure-7.html", "29 Supplementary Figure 7 29.1 Summary 29.2 Details of plot_SF7.R", " 29 Supplementary Figure 7 29.1 Summary This is the accessory documentation of Figure S7. The Figure can be recreated by running the R script plot_SF7.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF7.R 2_analysis/dxy/50k/ 29.2 Details of plot_SF7.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts and on the packages hypoimg, hypogen and ggtext. 29.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF7.R 2_analysis/dxy/50k/ # =============================================================== # This script produces Suppl. Figure 7 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/dxy/50k/&#39;) # script_name &lt;- &quot;R/fig/plot_SF7.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF7.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/dxy/50k/ #&gt; ────────────────────────────────────────── /current/working/directory ── The path containing the \\(d_{XY}\\) data is received and stored inside a more descriptive variable. # config ----------------------- dxy_path &lt;- as.character(args[1]) This data path is then screened for input files. # locate dxy data files files &lt;- dir(dxy_path) In the next step, all \\(d_{XY}\\) files are read and the data is compiled into a single table. # load dxy data data &lt;- str_c(dxy_path,files) %&gt;% purrr::map(get_dxy) %&gt;% bind_rows() %&gt;% purrr::set_names(., nm = c(&#39;scaffold&#39;, &#39;start&#39;, &#39;end&#39;, &#39;mid&#39;, &#39;sites&#39;, &#39;pi_pop1&#39;, &#39;pi_pop2&#39;, &#39;dxy&#39;, &#39;fst&#39;, &#39;GSTART&#39;, &#39;gpos&#39;, &#39;run&#39;)) To be able to indicate the genome wide average \\(d_{XY}\\) in the background of the plot, we summarize the \\(d_{XY}\\) data for each pair wise comparison and store the summary in a new table. # create table for the indication of genome wide average dxy in the plot background # (rescale covered dxy range to the extent of the genome) global_bar &lt;- data %&gt;% # filter to non-overlaping windows only filter( start %% 50000 == 1) %&gt;% select(sites, dxy, run) %&gt;% group_by(run) %&gt;% summarise(genome_wide_dxy = sum(sites*dxy)/sum(sites)) %&gt;% arrange(genome_wide_dxy) %&gt;% ungroup() %&gt;% mutate(run = fct_reorder(.f = run, .x = genome_wide_dxy), scaled_dxy = genome_wide_dxy/max(genome_wide_dxy)) For the hamlet illustrations, we also create a table in which we assign each illustration the matching comparison. # prepare plot annotaton images grob_tibble &lt;- global_bar %&gt;% mutate(loc = str_sub(run,4,6), right = str_sub(run,1,3), left = str_sub(run,8,10)) %&gt;% select(1,4:6) %&gt;% pmap(.,plot_pair_run) %&gt;% bind_rows() The genome wide average \\(d_{XY}\\) indicated in the plot background uses a secondary x-axis. As a preparation for this, we create the desired breaks and labels for the secondary axis. # prepare plotting elements -------- # pre-define secondary x-axis breaks sc_ax &lt;- scales::cbreaks(c(0,max(global_bar$genome_wide_dxy)), scales::pretty_breaks(4)) # pre-define secondary x-axis labels labels &lt;- str_c(c(&quot;&quot;, sc_ax$breaks[2:5]*1000), c(&quot;0&quot;, rep(&quot;\\u00B710^-3&quot;,4))) Instead of ordering the species pairs by alphabet, we want the comparisons to be sorted by genome wide average \\(d_{XY}\\). For this we turn the comparisons into a factor. # sort pair-wise population comparisons by average genome wide dxy data &lt;- data %&gt;% mutate(run = factor(run, levels = levels(global_bar$run))) Then we can put together the final plot. # compose final figure p_done &lt;- ggplot()+ # general plot structure separated by run facet_wrap( .~run, as.table = TRUE, ncol = 1, dir = &#39;v&#39;)+ # add genome wide average dxy in the background geom_rect(data = global_bar %&gt;% mutate(xmax = scaled_dxy * hypo_karyotype$GEND[24]), aes(xmin = 0, xmax = xmax, ymin = -Inf, ymax = Inf), color = rgb(1,1,1,0),fill = clr_below)+ # add LG borders geom_vline(data = hypogen::hypo_karyotype, aes(xintercept = GEND), color = hypo_clr_lg)+ # add dxy data points geom_point(data = data, aes(x = gpos, y = dxy), size=.2,color = plot_clr) + # add fish images geom_hypo_grob2(data = grob_tibble, aes(grob = grob, rel_x = .945, rel_y = .5), angle = 0, height = .9, width = .13)+ # axis layout scale_x_hypo_LG(sec.axis = sec_axis(~ ./hypo_karyotype$GEND[24], breaks = (sc_ax$breaks/max(global_bar$genome_wide_dxy))[1:5], labels = labels, name = expression(Genomic~position/~Genome~wide~italic(d[XY]))))+ scale_y_continuous(name = expression(italic(d[XY])), breaks = c(0,.01, .02))+ # set plot extent coord_cartesian(xlim = c(0, hypo_karyotype$GEND[24]*1.135))+ # general plot layout theme_hypo()+ theme(strip.text = element_blank(), legend.position = &#39;none&#39;, axis.title.x = element_text(), axis.text.x.bottom = element_markdown(colour = &#39;darkgray&#39;)) Finally, we can export Figure S7. # export final figure hypo_save(filename = &#39;figures/SF7.png&#39;, plot = p_done, width = 8, height = 12, type = &quot;cairo&quot;, comment = plot_comment) "],
["supplementary-figure-8.html", "30 Supplementary Figure 8 30.1 Summary 30.2 Details of plot_SF8.R", " 30 Supplementary Figure 8 30.1 Summary This is the accessory documentation of Figure S8. The Figure can be recreated by running the R script plot_SF8.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF8.R \\ 2_analysis/pi/50k/ \\ 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz 30.2 Details of plot_SF8.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts as well as the packages hypoimg, hypogen and vroom. 30.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF8.R 2_analysis/pi/50k/ \\ # 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz # =============================================================== # This script produces Suppl. Figure 8 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/pi/50k/&#39;, # &#39;2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz&#39;) # script_name &lt;- &quot;R/fig/plot_SF8.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(vroom) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_SF8.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/pi/50k/ #&gt; ★ 2: 2_analysis/fasteprr/step4/fasteprr.all.rho.txt.gz #&gt; ────────────────────────────────────────── /current/working/directory ── The paths containing the \\(\\pi\\) and \\(\\rho\\) data are received and stored inside more descriptive variables. # config ----------------------- pi_path &lt;- as.character(args[1]) rho_path &lt;- as.character(args[2]) The \\(\\pi\\)-path is screened for data files in the desired resolution (50 kb windows). # locate pi data files files &lt;- dir(pi_path, pattern = &#39;^[a-z]{6}.50k&#39;) Then, the \\(\\pi\\) data is loaded and compiled into a single table containing all populations. # load pi data data &lt;- str_c(pi_path, files) %&gt;% purrr::map(get_pi) %&gt;% bind_rows() To be able to order the subplots of the final figure by the genome wide average \\(\\pi\\) of the populations, we create a second data table containing the summary of the \\(\\pi\\) data for all populations. # compute genome wide average pi for the subplot order global_bar &lt;- data %&gt;% filter( BIN_START %% 50000 == 1) %&gt;% select(N_VARIANTS, PI, spec) %&gt;% group_by(spec) %&gt;% summarise(genome_wide_pi = sum(N_VARIANTS*PI)/sum(N_VARIANTS)) %&gt;% arrange(genome_wide_pi) %&gt;% ungroup() %&gt;% mutate(spec = fct_reorder(.f = spec, .x = genome_wide_pi), scaled_pi = genome_wide_pi/max(genome_wide_pi)) Then, we load the \\(\\rho\\) data. # load recombination data rho_data &lt;- vroom(rho_path, delim = &#39;\\t&#39;) %&gt;% select(-BIN_END) Next, the two data sets are merged based on the window position on the hamlet reference genome. # merge pi and recombination data combined_data &lt;- data %&gt;% # filter pi data to &quot;non-overlapping&quot; windows filter(BIN_START %% 50000 == 1 ) %&gt;% # reorder populations by genome wide average pi mutate(spec = factor(spec, levels = levels(global_bar$spec))) %&gt;% # merge with recombination data left_join(rho_data, by = c(CHROM = &#39;CHROM&#39;, BIN_START = &#39;BIN_START&#39;)) To indicate the hamlet population on the sub-plots, hamlet illustrations are loaded. # create table with fish annotations grob_tibble2 &lt;- global_bar$spec %&gt;% purrr::map(fish_plot2) %&gt;% bind_rows() Then, the final figure is created. # compose final figure p_done &lt;- combined_data %&gt;% ggplot()+ # add fish annotations geom_hypo_grob2(data = grob_tibble2, aes(grob = grob, rel_x = .25,rel_y = .75), angle = 0, height = .5,width = .5)+ # add hex-bin desity layer geom_hex(bins = 30,color = rgb(0,0,0,.3), aes(fill=log10(..count..), x = RHO, y = PI))+ # general plot structure (separated by run) facet_wrap(spec ~., ncol = 3)+ # set axis layout and color scheme scale_x_continuous(name = expression(rho))+ scale_y_continuous(name = expression(pi))+ scico::scale_fill_scico(palette = &#39;berlin&#39;) + # customize legend guides(fill = guide_colorbar(direction = &#39;horizontal&#39;, title.position = &#39;top&#39;, barheight = unit(7,&#39;pt&#39;), barwidth = unit(130,&#39;pt&#39;)))+ # general plot layout theme_minimal()+ theme(legend.position = c(.84,.01), strip.text = element_blank()) Finally, we can export Figure S8. # export final figure hypo_save(filename = &#39;figures/SF8.pdf&#39;, plot = p_done, width = 8, height = 10, comment = plot_comment) "],
["supplementary-figure-9.html", "31 Supplementary Figure 9 31.1 Summary 31.2 Details of plot_SF9.R", " 31 Supplementary Figure 9 31.1 Summary This is the accessory documentation of Figure S9. The Figure can be recreated by running the R script plot_SF9.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF9.R 2_analysis/raxml/hyp155_n_0.33_mac4_5kb.raxml.support 31.2 Details of plot_SF9.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen, ape and ggtree. 31.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF9.R 2_analysis/raxml/hyp155_n_0.33_mac4_5kb.raxml.support # =============================================================== # This script produces Suppl. Figure 9 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/raxml/hyp155_n_0.33_mac4_5kb.raxml.support&quot;) # script_name &lt;- &quot;R/fig/plot_SF9.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ape) library(ggtree) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF9.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/raxml/hyp155_n_0.33_mac4_5kb.raxml.support #&gt; ────────────────────────────────────────── /current/working/directory ── The path of the phylogentic tree file is received and stored in a variable. # config ----------------------- tree_hypo_file &lt;- as.character(args[1]) Then, the tree file is read and the tree is rooted with the H. floriedae sample as outgroup. raxml_tree &lt;- read.tree(tree_hypo_file) raxml_tree_rooted &lt;- root(phy = raxml_tree, outgroup = &quot;PL17_160floflo&quot;) Also, the default tree layout is defined and the default tree color is set. clr_neutral &lt;- rgb(.6, .6, .6) lyout &lt;- &#39;circular&#39; Based on key nodes of the tree, clades of the tree are grouped when they only contain a single hamlet species. raxml_tree_rooted_grouped &lt;- groupClade(raxml_tree_rooted, .node = c(298, 302, 187, 179, 171, 159, 193, 204, 201, 222, 219, 209, 284, 278, 268, 230, 242), group_name = &quot;clade&quot;) Then, the labels for the previously grouped clades are set. clade2spec &lt;- c( `0` = &quot;none&quot;, `1` = &quot;ran&quot;, `2` = &quot;uni&quot;, `3` = &quot;ran&quot;, `4` = &quot;may&quot;, `5` = &quot;pue&quot;, `6` = &quot;ind&quot;, `7` = &quot;nig&quot;, `8` = &quot;nig&quot;, `9` = &quot;ran&quot;, `10` = &quot;abe&quot;, `11` = &quot;abe&quot;, `12` = &quot;gum&quot;, `13` = &quot;uni&quot;, `14` = &quot;pue&quot;, `15` = &quot;uni&quot;, `16` = &quot;pue&quot;, `17` = &quot;nig&quot;) Now, the support values of the tree are transformed into discrete support classes. raxml_data &lt;- ggtree(raxml_tree_rooted_grouped, layout = lyout) %&gt;% .$data %&gt;% mutate(spec = ifelse(isTip, str_sub(label, -6, -4), &quot;ungrouped&quot;), support = as.numeric(label), support_class = cut(support, c(0,50,70,90,100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;)) ) At this point, the basic pylogenetic tree can be drawn. p_tree &lt;- (open_tree( ggtree(raxml_data, layout = lyout, aes(color = ifelse(clade == 0, lab2spec(label), clade2spec[as.character(clade)]))) %&gt;% ggtree::rotate(200), 180)) + geom_tippoint(size = .4) + geom_tiplab2(aes(color = lab2spec(label), label = str_sub( label, -6, -1)), size = 3, hjust = -.1)+ ggtree::geom_treescale(width = .002, x = -.0007, y = 155, offset = -3,fontsize = 3, color = clr_neutral) + xlim(c(-.0007,.0092)) + ggtree::geom_nodepoint(aes(fill = support_class, size = support_class), shape = 21) + scale_color_manual(values = c(ungrouped = clr_neutral, GenomicOriginsScripts::clr2), guide = FALSE) + scale_fill_manual(values = c(`(0,50]` = &quot;transparent&quot;, `(50,70]` = &quot;white&quot;, `(70,90]` = &quot;gray&quot;, `(90,100]` = &quot;black&quot;), drop = FALSE) + scale_size_manual(values = c(`(0,50]` = 0, `(50,70]` = 1.5, `(70,90]` = 1.5, `(90,100]` = 1.5), na.value = 0, drop = FALSE)+ guides(fill = guide_legend(title = &quot;Node Support Class&quot;, title.position = &quot;top&quot;, ncol = 2), size = guide_legend(title = &quot;Node Support Class&quot;,title.position = &quot;top&quot;, ncol = 2)) + theme_void() Unfortunately, the polar coordinate system underlying the circular tree layout introduces a lot of empty space for a open tree that spans 180 degrees. In order to crop that empty space, the basic tree is converted into grid object and used as an annotation in a different ggplot. This uses cartesian coordinates and is easily cropped to create the final figure. y_sep &lt;- .05 x_shift &lt;- -.03 p_done &lt;- ggplot() + coord_equal(xlim = c(0, .93), ylim = c(-.01, .54), expand = 0) + annotation_custom(grob = ggplotGrob(p_tree + theme(legend.position = &quot;none&quot;)), ymin = -.6 + (.5 * y_sep), ymax = .6 + (.5 * y_sep), xmin = -.1, xmax = 1.1) + annotation_custom(grob = cowplot::get_legend(p_tree), ymin = .35, ymax = .54, xmin = 0, xmax = .2) + theme_void() Finally, we can export Figure S9. scl &lt;- 1.5 hypo_save(plot = p_done, filename = &quot;figures/SF9.pdf&quot;, width = 7.5 * scl, height = 4 * scl, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["supplementary-figure-10.html", "32 Supplementary Figure 10 32.1 Summary 32.2 Details of plot_SF10.R", " 32 Supplementary Figure 10 32.1 Summary This is the accessory documentation of Figure S10. The Figure can be recreated by running the R script plot_SF10.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF10.R 2_analysis/pi/50k/ 32.2 Details of plot_SF10.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts and on the packages hypoimg, hypogen and ggtext. 32.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF10.R 2_analysis/pi/50k/ # =============================================================== # This script produces Suppl. Figure 10 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/pi/50k/&#39;) # script_name &lt;- &quot;R/fig/plot_SF10.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF10.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/pi/50k/ #&gt; ────────────────────────────────────────── /current/working/directory ─── The path containing the \\(\\pi\\) data is received and stored inside a more descriptive variable. # config ----------------------- pi_path &lt;- as.character(args[1]) This path is then screened for input files. # locate pi data files files &lt;- dir(pi_path, pattern = &#39;^[a-z]{6}.50k&#39;) All \\(\\pi\\) data files are loaded and compiled into a single data frame. # load pi data data &lt;- str_c(pi_path, files) %&gt;% purrr::map(get_pi) %&gt;% bind_rows() To be able to indicate the genome wide average \\(\\pi\\) in the background of the plot, we summarize the data for each population and store the summary in a new table. # create table for the indication of genome wide average pi in the plot background # (rescale covered pi range to the extent of the genome) global_bar &lt;- data %&gt;% # filter to non-overlaping windows only filter( BIN_START %% 50000 == 1) %&gt;% select(N_VARIANTS, PI, spec) %&gt;% group_by(spec) %&gt;% summarise(genome_wide_pi = sum(N_VARIANTS*PI)/sum(N_VARIANTS)) %&gt;% arrange(genome_wide_pi) %&gt;% ungroup() %&gt;% mutate(spec = fct_reorder(.f = spec, .x = genome_wide_pi), scaled_pi = genome_wide_pi/max(genome_wide_pi)) For the hamlet illustrations, we also create a table in which we assign each illustration to the matching population. # prepare plot annotaton images grob_tibble &lt;- global_bar$spec %&gt;% purrr::map(fish_plot) %&gt;% bind_rows() The genome wide average \\(\\pi\\) indicated in the plot background uses a secondary x-axis. As a preparation for this, we create the desired breaks and labels for the secondary axis. # prepare plotting elements -------- # pre-define secondary x-axis breaks sc_ax &lt;- scales::cbreaks(c(0,max(global_bar$genome_wide_pi)), scales::pretty_breaks(4)) # pre-define secondary x-axis labels labels &lt;- str_c(c(&quot;&quot;, sc_ax$breaks[2:6]*1000), c(&quot;0&quot;, rep(&quot;\\u00B710^-3&quot;,5))) Instead of ordering the populations by alphabet, we want them to be sorted by genome wide average \\(\\pi\\). For this we turn the populations into a factor. # sort pair-wise population comparisons by average genome wide pi data &lt;- data %&gt;% mutate(spec = factor(spec, levels = levels(global_bar$spec))) Then we can put together the final plot. # compose final figure p_done &lt;- ggplot()+ # general plot structure separated by run facet_wrap( .~spec, as.table = TRUE, ncol = 1, dir = &#39;v&#39;)+ # add genome wide average pi in the background geom_rect(data = global_bar %&gt;% mutate(xmax = scaled_pi * hypo_karyotype$GEND[24]), aes(xmin = 0, xmax = xmax, ymin = -Inf, ymax = Inf), color = rgb(1,1,1,0), fill = clr_below)+ # add LG borders geom_vline(data = hypogen::hypo_karyotype,aes(xintercept = GEND),color = hypo_clr_lg)+ # add pi data points geom_point(data = data, aes(x = gpos, y = PI), size=.2, color = plot_clr) + # add fish images geom_hypo_grob2(data = grob_tibble, aes(grob = grob, rel_x = .975, rel_y = .5), angle = 0, height = .8, width = .12)+ # set axis layout scale_x_hypo_LG(sec.axis = sec_axis(~ ./hypo_karyotype$GEND[24], breaks = (sc_ax$breaks/max(global_bar$genome_wide_pi)), labels = labels, name = &quot;Genomic position/ Genome wide *\\u03C0*&quot;))+ scale_y_continuous(name = &quot;*\\u03C0*&quot;, breaks = c(0,.006,.012))+ # set plot extent coord_cartesian(xlim = c(0, hypo_karyotype$GEND[24]*1.06))+ # general plot layout theme_hypo()+ theme(strip.text = element_blank(), legend.position = &#39;none&#39;, axis.title.x = element_markdown(), axis.title.y = element_markdown(), axis.text.x.bottom = element_markdown(colour = &#39;darkgray&#39;)) Finally, we can export Figure S10. # export final figure hypo_save(filename = &#39;figures/SF10.png&#39;, plot = p_done, width = 8, height = 8, type = &quot;cairo&quot;, comment = plot_comment) "],
["supplementary-figure-11.html", "33 Supplementary Figure 11 33.1 Summary 33.2 Details of plot_SF11.R", " 33 Supplementary Figure 11 33.1 Summary This is the accessory documentation of Figure S11. The Figure can be recreated by running the R script plot_SF11.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF11.R \\ 2_analysis/raxml/lg04.1_155N.raxml.support \\ 2_analysis/raxml/lg12.3_155N.raxml.support \\ 2_analysis/raxml/lg12.4_155N.raxml.support 33.2 Details of plot_SF11.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts and on the packages hypoimg, hypogen, ape, ggtree and patchwork. 33.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF11.R 2_analysis/raxml/lg04.1_155N.raxml.support \\ # 2_analysis/raxml/lg12.3_155N.raxml.support \\ # 2_analysis/raxml/lg12.4_155N.raxml.support # =============================================================== # This script produces Suppl. Figure 11 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/raxml/lg04.1_155N.raxml.support&quot;, # &quot;2_analysis/raxml/lg12.3_155N.raxml.support&quot;, # &quot;2_analysis/raxml/lg12.4_155N.raxml.support&quot;) # script_name &lt;- &quot;R/fig/plot_SF11.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ape) library(ggtree) library(patchwork) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF11.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/raxml/lg04.1_155N.raxml.support #&gt; ★ 2: 2_analysis/raxml/lg12.3_155N.raxml.support #&gt; ★ 3: 2_analysis/raxml/lg12.4_155N.raxml.support #&gt; ────────────────────────────────────────── /current/working/directory ─── The paths the phyplogenic trees are received and stored inside more descriptive variable. # config ----------------------- tree_file_lg04_1 &lt;- as.character(args[1]) tree_file_lg12_3 &lt;- as.character(args[2]) tree_file_lg12_4 &lt;- as.character(args[3]) Then, the trees are being read in and rooted using the H. floridae sample as outgroup. trees &lt;- c(tree_file_lg04_1, tree_file_lg12_3, tree_file_lg12_4) %&gt;% map(.f = function(file){ read.tree(file) %&gt;% root(phy = ., outgroup = &quot;PL17_160floflo&quot;)} ) Next, the default tree layout is defined and the default tree color is set. clr_neutral &lt;- rgb(.6, .6, .6) lyout &lt;- &#39;circular&#39; Now, the support values of the tree are transformed into discrete support classes. tree_data &lt;- trees %&gt;% map(.f = function(tree_in){ open_tree(ggtree(tree_in, layout = lyout), 180) %&gt;% .$data %&gt;% mutate(spec = ifelse(isTip, str_sub(label, -6, -4), &quot;ungrouped&quot;), support = as.numeric(label), support_class = cut(support, c(0,50,70,90,100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;)) )} ) After this, the individual trees are being drawn one by one. p1 &lt;- plot_outl_tree(tree_data[[1]]) p2 &lt;- plot_outl_tree(tree_data[[2]], show_legend = FALSE) p3 &lt;- plot_outl_tree(tree_data[[3]], show_legend = FALSE) All three trees are combined to create the final figure. p_done &lt;- p1 + p2 + p3 + plot_annotation(tag_levels = &#39;a&#39;) + plot_layout(ncol = 1) Finally, we can export Figure S11. scl &lt;- 2 hypo_save(plot = p_done, filename = &quot;figures/SF11.pdf&quot;, width = f_width_half * scl, height = f_width_half * 1.5 * scl, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["supplementary-figure-12.html", "34 Supplementary Figure 12 34.1 Summary 34.2 Details of plot_SF12.R", " 34 Supplementary Figure 12 34.1 Summary This is the accessory documentation of Figure S12. The Figure can be recreated by running the R script plot_SF12.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF12.R \\ 2_analysis/raxml/lg04.1_hySN.raxml.support \\ 2_analysis/raxml/lg12.3_hySN.raxml.support \\ 2_analysis/raxml/lg12.4_hySN.raxml.support 34.2 Details of plot_SF12.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts and on the packages hypoimg, hypogen, ape, ggtree, patchwork and phangorn. 34.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF12.R 2_analysis/raxml/lg04.1_hySN.raxml.support \\ # 2_analysis/raxml/lg12.3_hySN.raxml.support \\ # 2_analysis/raxml/lg12.4_hySN.raxml.support # =============================================================== # This script produces Suppl. Figure 12 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/raxml/lg04.1_hySN.raxml.support&quot;, # &quot;2_analysis/raxml/lg12.3_hySN.raxml.support&quot;, # &quot;2_analysis/raxml/lg12.4_hySN.raxml.support&quot;) # script_name &lt;- &quot;R/fig/plot_SF12.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ape) library(ggtree) library(patchwork) library(phangorn) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF12.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/raxml/lg04.1_hySN.raxml.support #&gt; ★ 2: 2_analysis/raxml/lg12.3_hySN.raxml.support #&gt; ★ 3: 2_analysis/raxml/lg12.4_hySN.raxml.support #&gt; ────────────────────────────────────────── /current/working/directory ─── The paths the phyplogenic trees are received and stored inside more descriptive variable. # config ----------------------- tree_file_lg04_1 &lt;- as.character(args[1]) tree_file_lg12_3 &lt;- as.character(args[2]) tree_file_lg12_4 &lt;- as.character(args[3]) Then, the trees are being read in and rooted using the Serranus samples as outgroup. trees &lt;- c(tree_file_lg04_1, tree_file_lg12_3, tree_file_lg12_4) %&gt;% map(.f = function(file){ read.tree(file) %&gt;% root(phy = ., outgroup = c(&quot;28393torpan&quot;, &quot;s_tort_3torpan&quot;, &quot;20478tabhon&quot; )) %&gt;% midpoint()} ) Next, the default tree layout is defined and the default tree color is set. clr_neutral &lt;- rgb(.6, .6, .6) lyout &lt;- &#39;circular&#39; Now, the support values of the tree are transformed into discrete support classes. tree_data &lt;- trees %&gt;% map(.f = function(tree_in){ open_tree(ggtree(tree_in, layout = lyout), 180) %&gt;% .$data %&gt;% mutate(spec = ifelse(isTip, str_sub(label, -6, -4), &quot;ungrouped&quot;), support = as.numeric(label), support_class = cut(support, c(0,50,70,90,100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;)) )} ) After this, the individual trees are being drawn one by one. p1 &lt;- plot_outl_tree_s(tree_data[[1]]) p2 &lt;- plot_outl_tree_s(tree_data[[2]], show_legend = FALSE) p3 &lt;- plot_outl_tree_s(tree_data[[3]], show_legend = FALSE) All three trees are combined to create the final figure. p_done &lt;- p1 + p2 + p3 + plot_annotation(tag_levels = &#39;a&#39;) + plot_layout(ncol = 1) Finally, we can export Figure S12. scl &lt;- 2 hypo_save(plot = p_done, filename = &quot;figures/SF12.pdf&quot;, width = f_width_half * scl, height = f_width_half * 1.5 * scl, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) "],
["supplementary-figure-13.html", "35 Supplementary Figure 13 35.1 Summary 35.2 Details of plot_SF13.R", " 35 Supplementary Figure 13 35.1 Summary This is the accessory documentation of Figure S13. The Figure can be recreated by running the R script plot_SF13.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF13.R\\ 2_analysis/admixture/ \\ metadata/phenotypes.sc 35.2 Details of plot_SF13.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen, ggtext, patchwork and paletteer. 35.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF13.R 2_analysis/admixture/ metadata/phenotypes.sc # =============================================================== # This script produces Suppl. Figure 13 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c( &quot;2_analysis/admixture/&quot;, &quot;metadata/phenotypes.sc&quot;) # script_name &lt;- &quot;R/fig/plot_SF13.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly=FALSE) # setup ----------------------- library(paletteer) library(patchwork) library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) cli::rule( left = str_c(crayon::bold(&#39;Script: &#39;),crayon::red(script_name))) args = args[7:length(args)] cat(&#39; &#39;) cat(str_c(crayon::green(cli::symbol$star),&#39; &#39;, 1:length(args),&#39;: &#39;,crayon::green(args),&#39;\\n&#39;)) cli::rule(right = getwd()) #&gt; ── Script: R/fig/plot_SF13.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/admixture/ #&gt; ★ 2: metadata/phenotypes.sc #&gt; ─────────────────────────────────────────── /current/working/directory ── The path containing the admixture results and the phenotype data file are received and stored inside more descriptive variables. # config ----------------------- admx_path &lt;- as.character(args[1]) pheno_file &lt;- as.character(args[2]) Then, we extract the outlier region IDs from the admixture data file names for later use. # load outlier window IDs (crop from admixture result file names) gids &lt;- dir(admx_path, pattern = &quot;pop.*15.txt&quot;) %&gt;% str_remove(&quot;pop.&quot;) %&gt;% str_remove(&quot;.15.txt&quot;) We start the data import by loading the phenotype data. # load phenotype data pheno_data &lt;- read_sc(pheno_file) %&gt;% select(id, Bars, Peduncle, Snout) %&gt;% filter(!is.na(Bars)) Then we load all the admixture results that used two clusters (k = 2). # load admixture data data &lt;- gids %&gt;% map_dfr(data_amdx, admx_path = admx_path, k = 2) We want to indicate the phenotype of each trait under the outlier region that is expected to be most relevant for this trait. Therefore we assign each trait to a specific outlier ID. # associate phenotypic trait with outlier region pheno_facet &lt;- tibble( trait = c(&quot;Snout&quot;,&quot;Bars&quot;, &quot;Peduncle&quot;), gid = c(&quot;LG04_1&quot;, &quot;LG12_3&quot;, &quot;LG12_4&quot;)) %&gt;% mutate(facet_label = str_c(gid, &quot; / &quot;, trait)) Then we format the outlier ID labels for the final plot. # set outlier region labels gid_labels &lt;- c(LG04_1 = &quot;LG04 (A)&quot;, LG12_3 = &quot;LG12 (B)&quot;, LG12_4 = &quot;LG12 (C)&quot;) To due the historic development of this script, there is a redundant assignment of outlier ID and trait (both are needed within functions of GenomicOriginsScripts 🤷). # set outlier region phenotypic traits gid_traits &lt;- c(LG04_1 = &quot;Snout&quot;, LG12_3 = &quot;Bars&quot;, LG12_4 = &quot;Peduncle&quot;) Using the markdown functionality of ggtext, we store each trait image in the axis labels using html syntax. # set path to trait images trait_icons &lt;- c(LG04_1 = &quot;&lt;img src=&#39;ressources/img/snout_c.png&#39; width=&#39;60&#39; /&gt; &quot;, LG12_3 = &quot;&lt;img src=&#39;ressources/img/bars_c.png&#39; width=&#39;60&#39; /&gt; &quot;, LG12_4 = &quot;&lt;img src=&#39;ressources/img/peduncle_c.png&#39; width=&#39;60&#39; /&gt; &quot;) Then, we clean and format the phenotype data for plotting. # format phenotype data pheno_plot_data &lt;- data %&gt;% filter(!duplicated(id)) %&gt;% select(id:id_order) %&gt;% left_join(pheno_data,by = c( id_nr = &quot;id&quot;)) %&gt;% arrange(spec, Bars, Peduncle, Snout, id) %&gt;% mutate(ord_nr = row_number()) %&gt;% pivot_longer(names_to = &quot;trait&quot;, values_to = &quot;phenotype&quot;, cols = Bars:Snout) %&gt;% left_join(pheno_facet) The separate panels of the figure are actually individual plots. To assure a consistent sample order, this is defined externally and stored within sample_order. # helper for consistent sample order across all panels sample_order &lt;- pheno_plot_data %&gt;% filter(!duplicated(id)) %&gt;% select(id, ord_nr) Then, the admixture panels (a - c) are created as three sepearted plots and stored within a list. # create plot panels a-c p_ad &lt;- c(&quot;LG04_1&quot;, &quot;LG12_3&quot;, &quot;LG12_4&quot;) %&gt;% purrr::map(adm_plot, data = data) (Just as an example - here is the plot for panel c) p_ad[[3]] # not part of the actual script Next, we create a dummy plot to extract the legend for the phenotypes. # create dummy plot for the phenotype legend p_phno &lt;- pheno_plot_data %&gt;% ggplot(aes(x = ord_nr))+ geom_point(aes(y = trait, fill = factor(phenotype)),shape = 21)+ scale_fill_manual(&quot;Phenotype&lt;br&gt;&lt;img src=&#39;ressources/img/all_traits_c.png&#39; width=&#39;110&#39; /&gt;&quot;, values = c(`0` = &quot;white&quot;, `1` = &quot;black&quot;), na.value = &quot;gray&quot;, labels = c(&quot;absent&quot;, &quot;present&quot;, &quot;not scored&quot;))+ guides(fill = guide_legend(ncol = 1))+ theme_minimal()+ theme(legend.title = element_markdown(hjust = .5), legend.position = &quot;bottom&quot;) Below the panels a - c the species of the samples are indicated. To do this, we prepare a small table from the species affiliation of each sample. # prepare table with fish annotations for the species indication tib_drawing &lt;- pheno_plot_data %&gt;% group_by(spec) %&gt;% summarise(pos = (min(ord_nr)+max(ord_nr))*.5) %&gt;% ungroup() This table is then used to create a small plot. # create sub-plot for species indication p_spec &lt;- pheno_plot_data %&gt;% group_by(spec) %&gt;% summarise(start = min(ord_nr)-1, end = max(ord_nr)) %&gt;% ggplot(aes(xmin = start, xmax = end, ymin = -Inf, ymax = Inf))+ # add colored backgroud boxes geom_rect(aes(fill = spec), color = &quot;black&quot;)+ # add fish images (tib_drawing %&gt;% pmap(add_spec_drawing))+ # set axis layout scale_y_continuous(breaks = .5, labels = c( &quot;Species&quot;), limits = c(0,1))+ scale_x_discrete(breaks = sample_order$ord_nr, labels = sample_order$id, expand = c(0,0)) + # set species color scheme scale_fill_manual(&quot;Species&quot;, values = clr, labels = sp_labs)+ # set general plot layout theme_minimal()+ theme(plot.title = element_text(size = 9), legend.position = &quot;bottom&quot;, legend.text.align = 0, axis.title = element_blank(), axis.ticks = element_blank(), axis.text.x = element_blank()) Similarly to the species affiliation, we create a second plot to indicate the sampling location. # create sub-plot for samplong location indication p_loc &lt;- pheno_plot_data %&gt;% ggplot(aes(x = factor(ord_nr)))+ # add colored boxes geom_raster(aes(y = 0, fill = loc))+ # set axis layout scale_y_continuous(breaks = c(0),labels = c(&quot;Location&quot;))+ scale_x_discrete(breaks = sample_order$ord_nr, labels = sample_order$id) + # set location color scheme scale_fill_manual(&quot;Location&quot;, values = clr_loc, loc_names)+ # set general plot layout theme_minimal()+ theme(plot.title = element_text(size = 9), legend.position = &quot;bottom&quot;, axis.title = element_blank(), axis.ticks = element_blank(), axis.text.x = element_blank()) Then, we compose the figure legend from individual sub-legends. # compose legend from individual legend parts p_l &lt;- (get_legend(p_phno) %&gt;% ggdraw()) + (get_legend(p_spec) %&gt;% ggdraw()) + (get_legend(p_loc) %&gt;% ggdraw()) + plot_layout(nrow = 1) At this point, we can assemble the final figure. Yet, due to the positioning of the trait images with ggtext we need to crop the left margin a little (hence we need p_done). # finalize figure p_prep &lt;- p_ad[[1]] + p_ad[[2]] + p_ad[[3]]+ p_spec + p_loc + p_l + plot_layout(ncol = 1, heights = c(.4,.4,.4,.08,.02,.1)) &amp; theme(legend.position = &quot;none&quot;, axis.text = element_text(size = 12)) # crop final figure (remove whitespace on left margin) p_done &lt;- ggdraw(p_prep, xlim = c(.023,1)) Finally, we can export Figure S13. # export final figure scl &lt;- .9 ggsave(&quot;figures/SF13.pdf&quot;, plot = p_done, width = 16*scl, height = 10*scl, device = cairo_pdf) "],
["supplementary-figure-14.html", "36 Supplementary Figure 14 36.1 Summary 36.2 Details of plot_SF14.R", " 36 Supplementary Figure 14 36.1 Summary This is the accessory documentation of Figure S14. The Figure can be recreated by running the R script plot_SF14.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF14.R 2_analysis/GxP/50000/ 36.2 Details of plot_SF14.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts and on the packages hypoimg and hypogen. 36.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF14.R 2_analysis/GxP/50000/ # =============================================================== # This script produces Suppl. Figure 14 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&#39;2_analysis/GxP/50000/&#39;) # script_name &lt;- &quot;R/fig/plot_SF14.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(.,&#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;,getwd(),&#39;/&#39;,.) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF14.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/GxP/50000/ #&gt; ─────────────────────────────────────────── /current/working/directory ── The path containing the \\(G \\times P\\) data is received and stored inside a more descriptive variable. # config ----------------------- gxp_path &lt;- as.character(args[1]) We then set the specific traits for which we want to compare the \\(G \\times P\\) association under a linear model (lm) versus a linear mixed model (lmm). # configure which gxp data to load trait_tib &lt;- tibble(file = dir(gxp_path) %&gt;% .[str_detect(.,&quot;Bars|Peduncle|Snout&quot;)]) %&gt;% mutate(prep = file) %&gt;% separate(prep , into = c(&quot;trait&quot;, &quot;model_type&quot;, &quot;win&quot;, &quot;step&quot;, &quot;filetype&quot;, &quot;zip&quot;), sep = &quot;\\\\.&quot;) %&gt;% select(file, trait, model_type) %&gt;% mutate(path = gxp_path) The we load the \\(G \\times P\\) data. # load gxp data data &lt;- pmap_dfr(trait_tib, get_gxp_both_models) After the data import, this table is already suited for plotting. # compose final figure p_done &lt;- data %&gt;% ggplot(aes(x = gpos, y = AVG_p_wald))+ # add gray/white LGs background geom_hypo_LG()+ # add gxp data points geom_point(color = plot_clr, size = .3)+ # set axis layout scale_x_hypo_LG()+ scale_fill_hypo_LG_bg()+ # set axis titles labs(y = expression(G~x~P~(average~italic(p)[wald])))+ # general plot structure separated by model type and trait facet_grid(trait+model_type ~ ., scales = &quot;free_y&quot;)+ # general plot layout theme_hypo() Finally, we can export Figure S14. # export final figure hypo_save(filename = &quot;figures/SF14.png&quot;, plot = p_done, width = 11, height = 7, dpi = 600, type = &quot;cairo&quot;, comment = plot_comment) "],
["supplementary-figure-15.html", "37 Supplementary Figure 15 37.1 Summary 37.2 Details of plot_SF15.R", " 37 Supplementary Figure 15 37.1 Summary This is the accessory documentation of Figure S15. The Figure can be recreated by running the R script plot_SF15.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF15.R 37.2 Details of plot_SF15.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages ggtext, hypoimg and hypogen. 37.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF15.R 2_analysis/fst_signif/random/ # =============================================================== # This script produces Suppl. Figure 15 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/fst_signif/random/&quot;) # script_name &lt;- &quot;R/fig/plot_SF15.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypogen) library(hypoimg) library(ggtext) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF15.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/fst_signif/random/ #&gt; ─────────────────────────────────────────── /current/working/directory ── The \\(F_{ST}\\) permutation data the hybridization data is received and stored in a variable. After that, all files within the data folder are collected. # config ----------------------- rand_path &lt;- as.character(args[1]) rand_files &lt;- dir(path = rand_path, pattern = &quot;_random_fst.tsv.gz&quot;) Next, the data is read in. data &lt;- str_c(rand_path, rand_files) %&gt;% map_dfr(.f = get_random_fst) %&gt;% mutate(sign = c(subset_non_diverged = -1, whg = 1)[subset_type]) Then the data is nested with respect to permutation group and summary statistics are computed. data_grouped &lt;- data %&gt;% group_by(group, run, subset_type, sign) %&gt;% nest() %&gt;% ungroup() %&gt;% mutate(real_pop = map_dbl(data, function(data){data$weighted_fst[data$type == &quot;real_pop&quot;]}), percentile = map_chr(data, get_percentile), above = map_dbl(data, get_n_above), total = map_dbl(data, get_n_total), label = str_c(sprintf(&quot;%.2f&quot;, as.numeric(percentile)), &quot;&lt;br&gt;(&quot;,above, &quot;/10&lt;sup&gt;&quot;,log10(total),&quot;&lt;/sup&gt;)&quot;)) %&gt;% arrange(-sign, -as.numeric(percentile), -real_pop) Now, the species pairs are ordered according to their original genome wide average \\(F_{ST}\\). run_lvls &lt;- data_grouped %&gt;% filter(sign == 1) %&gt;% mutate(rank = row_number(), run = fct_reorder(run, rank)) %&gt;% .$run %&gt;% levels() The species pair order is also updated for the nested version of the data. data_grouped_ordered &lt;- data_grouped %&gt;% mutate(run = factor(run, levels = run_lvls)) At this point, the figure can be plotted. p_done &lt;- data %&gt;% mutate(run = factor(run, levels = run_lvls)) %&gt;% filter(type == &quot;random&quot;) %&gt;% ggplot(aes(group = group)) + geom_segment(data = data %&gt;% mutate(run = factor(run, levels = run_lvls)) %&gt;% filter(type == &quot;real_pop&quot;), aes(x = weighted_fst, xend = weighted_fst, y = 0, yend = Inf * sign, color = subset_type)) + geom_density(data = data %&gt;% filter(sign == 1), aes( x = weighted_fst, y = ..density.., color = subset_type, fill = after_scale(prismatic::clr_alpha(color,alpha = .3)))) + geom_density(data = data %&gt;% filter(sign == -1), aes( x = weighted_fst, y = ..density.. * -1, color = subset_type, fill = after_scale(prismatic::clr_alpha(color,alpha = .3)))) + geom_richtext(data = data_grouped_ordered, aes(x = .08, y = 300 * sign, label = label), hjust = .5, vjust = .5, size = 2.5, label.padding = unit(3,&quot;pt&quot;), label.size = 0, label.color = &quot;transparent&quot;, fill = &quot;transparent&quot;) + scale_x_continuous(breaks = c(0, .05, .1)) + scale_color_manual(values = c(whg = &quot;#4D4D4D&quot;, subset_non_diverged = &quot;#A2A2A2&quot;), guide = FALSE) + labs( y = &quot;Density (&lt;span style=&#39;color:#4D4D4D&#39;&gt;whg&lt;/span&gt; / &lt;span style=&#39;color:#A2A2A2&#39;&gt;differentiated regions excluded&lt;/span&gt;)&quot;, x = &quot;Weighted Average &lt;i&gt;F&lt;sub&gt;ST&lt;/sub&gt;&lt;/i&gt;&quot;) + facet_wrap(run ~ ., ncol = 4, dir = &quot;v&quot;) + theme_minimal() + theme(axis.text.y = element_blank(), axis.ticks.y = element_blank(), axis.title.y = element_markdown(), axis.title.x = element_markdown()) Finally, we can export Figure S15. scl &lt;- 1.4 hypo_save(p_done, filename = &#39;figures/SF15.pdf&#39;, width = f_width * scl, height = f_width * .65 * scl, device = cairo_pdf, comment = plot_comment, bg = &quot;transparent&quot;) After this, a subset of the data is created… # export significance table data_export &lt;- data_grouped %&gt;% select(run, real_pop, subset_type, percentile) %&gt;% mutate(pre = run, p_perm = 1 - as.numeric(percentile)) %&gt;% separate(pre, into = c(&quot;p2&quot;, &quot;p1&quot;)) …and exported for use for the plotting of Figure 1 (plot_F1.R). write_tsv(x = data_export, file = &quot;2_analysis/summaries/fst_permutation_summary.tsv&quot;) Finally, this subset is also used to compute the empirical p values and apply the false discovery rate correction for significance: # fdr correction (only for visual check) data_export %&gt;% mutate(loc = str_sub(run, -3, -1)) %&gt;% group_by(loc) %&gt;% mutate(loc_n = 28, fdr_correction_factor = sum(1 / 1:loc_n[[1]]), fdr_alpha = .05 / fdr_correction_factor, is_sig = p_perm &gt; fdr_alpha) %&gt;% ungroup() %&gt;% select(run,real_pop, p_perm, loc_n, subset_type, fdr_correction_factor, fdr_alpha) %&gt;% mutate(`050` = .05/ fdr_correction_factor, `010` = .01/ fdr_correction_factor, `001` = .001/ fdr_correction_factor, sig = ifelse(p_perm &lt; `001`, &quot;***&quot;, ifelse(p_perm &lt; `010`, &quot;**&quot;, ifelse(p_perm &lt; `050`, &quot;*&quot;, &quot;-&quot;)))) %&gt;% arrange(as.character(run)) %&gt;% select(run, subset_type, sig,real_pop, p_perm) %&gt;% mutate(real_pop = sprintf(&quot;%.4f&quot;,real_pop), p_perm = sprintf(&quot;%.2f&quot;,p_perm * 100)) "],
["supplementary-figure-16.html", "38 Supplementary Figure 16 38.1 Summary 38.2 Details of plot_SF16.R", " 38 Supplementary Figure 16 38.1 Summary This is the accessory documentation of Figure S16. The Figure can be recreated by running the R script plot_SF16.R: cd $BASE_DIR Rscript --vanilla R/fig/plot_SF16.R 2_analysis/raxml/hyS_n_0.33_mac4_5kb.raxml.support 38.2 Details of plot_SF16.R In the following, the individual steps of the R script are documented. It is an executable R script that depends on the accessory R package GenomicOriginsScripts, as well as on the packages hypoimg, hypogen, ape and ggtree. 38.2.1 Config The scripts start with a header that contains copy &amp; paste templates to execute or debug the script: #!/usr/bin/env Rscript # run from terminal: # Rscript --vanilla R/fig/plot_SF16.R 2_analysis/raxml/hyS_n_0.33_mac4_5kb.raxml.support # =============================================================== # This script produces Suppl. Figure 16 of the study &quot;Ancestral variation, # hybridization and modularity fuel a marine radiation&quot; # by Hench, Helmkampf, McMillan and Puebla # --------------------------------------------------------------- # =============================================================== # args &lt;- c(&quot;2_analysis/raxml/hyS_n_0.33_mac4_5kb.raxml.support&quot;) # script_name &lt;- &quot;R/fig/plot_SF16.R&quot; The next section processes the input from the command line. It stores the arguments in the vector args. The needed R packages are loaded and the script name and the current working directory are stored inside variables (script_name, plot_comment). This information will later be written into the meta data of the figure to help us tracing back the scripts that created the figures in the future. Then we drop all the imported information besides the arguments following the script name and print the information to the terminal. args &lt;- commandArgs(trailingOnly = FALSE) # setup ----------------------- library(GenomicOriginsScripts) library(hypoimg) library(hypogen) library(ape) library(ggtree) cat(&#39;\\n&#39;) script_name &lt;- args[5] %&gt;% str_remove(., &#39;--file=&#39;) plot_comment &lt;- script_name %&gt;% str_c(&#39;mother-script = &#39;, getwd(), &#39;/&#39;, .) args &lt;- process_input(script_name, args) #&gt; ── Script: R/fig/plot_SF16.R ──────────────────────────────────────────── #&gt; Parameters read: #&gt; ★ 1: 2_analysis/raxml/hyS_n_0.33_mac4_5kb.raxml.support #&gt; ────────────────────────────────────────── /current/working/directory ─── The path of the tree file is received and stored inside a more descriptive variable. # config ----------------------- tree_serr_file &lt;- as.character(args[1]) Then, the default tree layout is defined and the default tree color is set. clr_neutral &lt;- rgb(.6, .6, .6) lyout &lt;- &#39;circular&#39; Then, the tree file is read and the tree is rooted once with the Serranus samples as outgroup and once midpoint-rooted. tree_s &lt;- read.tree(tree_serr_file) tree_s_rooted &lt;- root(tree_s, outgroup = c(&quot;28393torpan&quot;, &quot;s_tort_3torpan&quot;, &quot;20478tabhon&quot; )) tree_s_mid &lt;- phangorn::midpoint(tree_s_rooted) Now, the support values of the tree are transformed into discrete support classes. tree_s_data &lt;- open_tree(ggtree(tree_s_mid, layout = &quot;circular&quot;), 180) %&gt;% .$data %&gt;% mutate(spec = ifelse(isTip, str_sub(label, -6, -4), &quot;ungrouped&quot;), support = as.numeric(label), support_class = cut(support, c(0,50,70,90,100)) %&gt;% as.character() %&gt;% factor(levels = c(&quot;(0,50]&quot;, &quot;(50,70]&quot;, &quot;(70,90]&quot;, &quot;(90,100]&quot;))) At this point, the basic pylogenetic tree can be drawn. p_s_tree &lt;- rotate_tree(open_tree(ggtree(tree_s_data, aes(color = spec), layout = &quot;circular&quot;), 180), 0) + geom_tiplab2(aes(label = str_sub(label, -6, -1)), size = 3, offset = .001) + geom_nodepoint(aes(fill = support_class, size = support_class), shape = 21) + ggtree::geom_treescale(width = .05, x = -.02, y = 158, offset = -15, fontsize = 3, color = clr_neutral) + scale_color_manual(values = c(ungrouped = clr_neutral, GenomicOriginsScripts::clr2), guide = FALSE) + scale_fill_manual(values = c(`(0,50]` = &quot;transparent&quot;, `(50,70]` = &quot;white&quot;, `(70,90]` = &quot;gray&quot;, `(90,100]` = &quot;black&quot;), drop = FALSE) + scale_size_manual(values = c(`(0,50]` = 0, `(50,70]` = 1.5, `(70,90]` = 1.5, `(90,100]` = 1.5), na.value = 0, drop = FALSE) + guides(fill = guide_legend(title = &quot;Node Support Class&quot;, title.position = &quot;top&quot;, ncol = 2), size = guide_legend(title = &quot;Node Support Class&quot;, title.position = &quot;top&quot;, ncol = 2)) + theme_void() Unfortunately, the polar coordinate system underlying the circular tree layout introduces a lot of empty space for a open tree that spans 180 degrees. In order to crop that empty space, the basic tree is converted into grid object and used as an annotation in a different ggplot. This uses cartesian coordinates and is easily cropped to create the final figure. y_sep &lt;- .05 x_shift &lt;- -.03 p_done &lt;- ggplot() + coord_equal(xlim = c(0, 1), ylim = c(-.01, .54), expand = 0) + annotation_custom(grob = ggplotGrob(p_s_tree + theme(legend.position = &quot;none&quot;)), ymin = -.6 + (.5 * y_sep), ymax = .6 + (.5 * y_sep), xmin = -.1, xmax = 1.1) + annotation_custom(grob = cowplot::get_legend(p_s_tree), ymin = .05, ymax = .15, xmin = .4, xmax = .6) + theme_void() Finally, we can export Figure S16. scl &lt;- 1.5 hypo_save(plot = p_done, filename = &quot;figures/SF16.pdf&quot;, width = 7.5 * scl, height = 4 * scl, device = cairo_pdf, bg = &quot;transparent&quot;, comment = plot_comment) Rabosky, Daniel L, Jonathan Chang, Pascal O Title, Peter F Cowman, Lauren Sallan, Matt Friedman, Kristin Kaschner, et al. 2019. “Data from: An inverse latitudinal gradient in speciation rate for marine fishes.” Dryad. https://doi.org/10.5061/DRYAD.FC71CP4. "]
]
